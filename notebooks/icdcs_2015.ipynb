{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WaterLLMarks Project using Langchain's API\n",
    "\n",
    "The goal of the study is to review the use of text watermarks to track users throughout a LLM SaaS platform. The study implements the following pipelines:\n",
    "- Question Answering without RAG.\n",
    "- Question Answering with RAG (implemented as a naive RAG using Chroma).\n",
    "- QA with RAG and a token-based watermark (`waterllmarks.watermarks.TokenWatermark`).\n",
    "- QA with RAG and a watermark using character-embedding (`waterllmarks.watermarks.Rizzo2016`).\n",
    "- With and without augmentation -> ideally see the impact of the augemntation on the llm response and/or the retrieved documents\n",
    "\n",
    "The goal is to assess the response quality of the different pipelines using known metrics:\n",
    "- BLEU, ROUGE, METEOR\n",
    "- String similarity (Levenshtein distance)\n",
    "- Context precision and recall (for RAG)\n",
    "- Retrieved documents compared to reference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup dataset and corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(prompt: str) -> str:\n",
    "    return \"\\n\".join([l.lstrip() for l in prompt.split(\"\\n\")]).rstrip(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.globals import set_llm_cache\n",
    "from langchain_community.cache import SQLiteCache\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "from langchain_core.rate_limiters import InMemoryRateLimiter\n",
    "\n",
    "\n",
    "# set_llm_cache(SQLiteCache(\"llm_cache.db\"))\n",
    "\n",
    "SEED = 1138\n",
    "\n",
    "# llm_client = ChatOpenAI(\n",
    "#     model=\"gpt-4o-mini\",\n",
    "#     seed=SEED,\n",
    "# )\n",
    "\n",
    "\n",
    "# llm_client = ChatOllama(\n",
    "#     model=\"mistral:7b\",\n",
    "#     base_url=\"http://snt-precision-7920-rack-lama:11434\",\n",
    "#     seed=SEED,\n",
    "# )\n",
    "\n",
    "llm_client = ChatOpenAI(\n",
    "    api_key=\"EMPTY\",\n",
    "    base_url=\"http://snt-precision-7920-rack-lama:8001/v1\",\n",
    "    model=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    ")\n",
    "\n",
    "\n",
    "# embedding_fn = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "embedding_fn = OpenAIEmbeddings(\n",
    "    api_key=\"EMPTY\",\n",
    "    base_url=\"http://snt-precision-7920-rack-lama:8002/v1\",\n",
    "    model=\"intfloat/e5-mistral-7b-instruct\",\n",
    "    tiktoken_enabled=False,\n",
    "    check_embedding_ctx_length=False,\n",
    ")\n",
    "\n",
    "# embedding_fn = OllamaEmbeddings(\n",
    "#     base_url=\"http://snt-precision-7920-rack-lama:2\",\n",
    "#     model=\"intfloat/e5-mistral-7b-instruct\",\n",
    "# )\n",
    "vector_store = Chroma(\n",
    "    embedding_function=embedding_fn,\n",
    "    persist_directory=\"./chroma_mistral_db\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8576\n"
     ]
    }
   ],
   "source": [
    "from waterllmarks.datasets import LLMPaperDataset, RAGBenchDataset\n",
    "\n",
    "ds = LLMPaperDataset()\n",
    "corpus = ds.corpus\n",
    "print(len(corpus))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"## I. Introduction\\n\\nDriven by the emerging development of deep learning, autonomous driving has observed a paradigm shift from rulesbased decision systems [66, 21] to data-driven learning-based approaches [28, 6, 36]. However, this comes at the cost of transparency in decision-making, especially for end-to-end autonomous driving systems which are considered black-box in nature [13]. Thus, in addition to precision in action control, explanation provision is key in ensuring trustworthy decisionmaking to reconcile the system's decisions with end-user expectations to foster confidence and acceptance [79, 8, 57] in dynamic driving environments.  \\nTraditional approaches have mainly relied on attention visualisation [5, 7, 55] as a proxy to rationalise the decisions of the black-box systems or auxiliary intermediate tasks such as semantic segmentation [25, 32], object detection [16, 31], and affordance prediction [68, 45] provide meaningful intermediate representation for decision-making. However, these methods do not engage end-users in the dialogue as they are onedirectional and not readily comprehensible by the general users for the purpose of fostering trust and confidence. An alternative promising approach is the integration of natural language explanations [38, 33, 54], in particular through Multi-Modal Large Language Models (MLLMs) [1, 70]. These models, pretrained on extensive web-scale datasets, demonstrate remarkable reasoning capacity, enabling the transformation of complex vehicular decision-making processes into more understandable narrative formats, thereby offering a new layer of explainability to conventional systems.  \\nWhile several early attempts have demonstrated the potential of MLLMs as general explainable driving agents [78, 76, 51], these methods fall short of human-level understanding. One of the limitations is their failure to generalise to unseen environments. A primary obstacle is the lack of high-quality annotated data [56], coupled with the significant domain shift across various datasets [23], which hinders the models' generalisation capacity to novel environments outside of the training data distribution. Another critical challenge is the prohibitively expensive training requirement and the unsolved problem of catastrophic forgetting [39], which make re-training or finetuning impractical solutions due to the immense computational demands and severe performance degradation. Consequently, this further limits the models' generalisability after deployment, as they struggle to effectively utilise new data in constantly evolving environments and driving scenarios.  \\nTo address these challenges, we introduce *RAG-Driver*, a novel retrieval-augment\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.corpus[1].page_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_fn.embed_documents([corpus[1].page_content]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "existing_ids = vector_store.get(include=[])[\"ids\"]\n",
    "doc_to_add = [doc for doc in ds.corpus if doc.id not in existing_ids]\n",
    "while doc_to_add:\n",
    "    n = min(100, len(doc_to_add))\n",
    "    try:\n",
    "        vector_store.add_documents(doc_to_add[:n])\n",
    "        doc_to_add = doc_to_add[n:]\n",
    "    except Exception as e:\n",
    "        print(\"Got exception\", e)\n",
    "        print(\"Sleeping for 3 seconds\")\n",
    "        time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1626f35b-7ffc-4736-bd6e-2a2f20e1aa8b']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(id='a09c2e9a-dd24-4e36-9b2e-7d4e15e58639', metadata={'creation_datetime': '2024-03-04', 'file_name': '2402.09193v2.md', 'file_path': 'paper_data/2402.09193v2.md', 'file_size': 69224, 'file_type': '', 'last_accessed_datetime': '2024-03-04', 'last_modified_datetime': '2024-02-22', 'title': '(Ir)Rationality And Cognitive Biases In Large Language Models'}, page_content='## 1 Introduction\\n\\n can be used to test the rationality of a model; this is a complex problem which requires a consensus what is deemed rational and irrational.  \\nMirco Musolesi University College London University of Bologna m.musolesi@ucl.ac.uk In using methods designed to evaluate human reasoning, it is important to acknowledge the performance vs. competence debate [14]. This line of argument encourages *species-fair* comparisons between humans and machines, meaning that we should design tests specific to either humans or machines, as otherwise apparent failures may not reflect underlying capabilities but only superficial differences. Lampinen[15] discusses this problem when it comes to language models in particular, highlighting that different approaches must be taking to evaluate cognitive and foundation models. However, if we take the purpose of LLMs to be to produce human-like language, perhaps the best approach is precisely to evaluate their output with tasks designed to evaluate humans. This is the approach we have taken in this paper - in order to identify whether LLMs reason rationally, or whether they exhibit biases that can be assimilated to those present in human decision-making, the most appropriate approach is therefore to use tasks that were initially designed for humans. Building on this debate and looking at LLMs being evaluated using human tests, Hagendorff et al. [16] have proposed the creation of a new field of research called *machine psychology*, which would treat LLMs as participants in psychological experiments. The approach employed in this paper precisely applies tests from psychology that were originally designed for humans, in this case to evaluate rational and irrational reasoning displayed but such models. Further to this, some have even discussed the potential of using LLMs as participants in cognitive experiments instead of humans [17], although some see this proposal as too optimistic [18], and others warn against excessive anthropomorphism [19]. One argument against the use of such models in cognitive experiments is that LLMs may be effective at approximating average human judgements, but are not good at capturing the variation in human behaviour [20]. One potential avenue to address this issue is current work on language models impersonating different roles [21], in this way capturing some of the variation in human behaviour. Binz and Schulz [22] show that after finetuning LLMs on data from psychological experiments, they can become accurate cognitive models, which they claim begins paving the way for the potential of using these models to study human behaviour. Park et al. [23] combine large language models with computational interactive agents to simulate human behaviour'),\n",
       " Document(id='67507d36-6462-4cfe-9d90-de2301619880', metadata={'creation_datetime': '2024-03-04', 'file_name': '2402.08690v1.md', 'file_path': 'paper_data/2402.08690v1.md', 'file_size': 80308, 'file_type': '', 'last_accessed_datetime': '2024-03-04', 'last_modified_datetime': '2024-02-22', 'title': 'If Turing Played Piano With An Artificial Partner'}, page_content='## Academic Press\\n\\n550/arXiv.1312.6114 .\\nhttp://arxiv.org/abs/1312.6114 Accessed 2023-09-12\\n[39] Rezende, D.J., Mohamed, S., Wierstra, D.: Stochastic Backpropagation and\\nApproximate Inference in Deep Generative Models. In: Proceedings of the 31st International Conference on Machine Learning, pp. 1278–1286. PMLR, ??? (2014). ISSN: 1938-7228. https://proceedings.mlr.press/v32/rezende14.html Accessed 2023-09-12\\n[40] Jackson, S.A., Martin, A.J., Eklund, R.C.: Long and Short Measures of Flow: The\\nConstruct Validity of the FSS-2, DFS-2, and New Brief Counterparts. Journal of Sport and Exercise Psychology 30(5), 561–587 (2008) https://doi.org/10.1123/\\njsep.30.5.561 . Publisher: Human Kinetics, Inc. Section: Journal of Sport and Exercise Psychology. Accessed 2023-03-31  \\n[41] Singer, J.D., Willett, J.B.: Applied Longitudinal Data Analysis: Modeling Change\\nand Event Occurrence. Oxford University Press, ??? (2003)\\n[42] Ortiz, C.L.: Why we need a physically embodied turing test and what it might\\nlook like. AI Magazine 37(1), 55–62 (2016) https://doi.org/10.1609/aimag.v37i1.\\n2645\\n[43] Pfeiffer, U.J., Timmermans, B., Bente, G., Vogeley, K., Schilbach, L.: A nonverbal turing test: Differentiating mind from machine in gaze-based social interaction. PLoS ONE 6(11) (2011) https://doi.org/10.1371/journal.pone.0027591\\n[44] Swisher, N., Dotov, D., Chemero, A.: Ascribing Moral Value and the Embodied\\nTuring Test. In: Proceedings of the 10th International Conference on Artificial Life: Workshops, pp. 40–45 (2006)\\n[45] Kostrubiec, V., Dumas,'),\n",
       " Document(id='17262aea-b038-4549-b89a-2b7275cd360f', metadata={'creation_datetime': '2024-03-04', 'file_name': '2402.09880v1.md', 'file_path': 'paper_data/2402.09880v1.md', 'file_size': 190252, 'file_type': '', 'last_accessed_datetime': '2024-03-04', 'last_modified_datetime': '2024-02-22', 'title': 'Inadequacies Of Large Language Model Benchmarks In The Era Of Generative Artificial Intelligence'}, page_content='## B. Genuine Reasoning Vs Technical Optimization\\n\\n    |                  |\\n| ×                         | ×                     | ×                |\\n| AGIEval [10]              |                       |                  |\\n| ×                         | ×                     | ×                |\\n| ToolAlpaca [30]           |                       |                  |\\n| ×                         | ×                     | ×                |\\n| HELM [31]                 |                       |                  |\\n| △                         | ×                     | ×                |\\n| ToolBench [32]            |'),\n",
       " Document(id='a665fad4-8dd1-4be7-86af-34bffa0fbb2c', metadata={'creation_datetime': '2024-03-04', 'file_name': '2402.09099v1.md', 'file_path': 'paper_data/2402.09099v1.md', 'file_size': 96559, 'file_type': '', 'last_accessed_datetime': '2024-03-04', 'last_modified_datetime': '2024-02-22', 'title': 'Exploring Neuron Interactions And Emergence In Llms: From The Multifractal Analysis Perspective'}, page_content=\"## B.3. Benchmarks\\n\\nLogiQA**. LogiQA(Liu et al., 2020) is a dataset consisting of 8,678 QA instances that focus on evaluating machine\\nreading comprehension with an emphasis on logical reasoning. It is derived from the National Civil Servants Examination of China and covers various types of deductive reasoning. This dataset presents a significant challenge for state-of-the-art neural models, which perform notably worse than humans in these tasks. LogiQA serves as a unique benchmark for testing logical AI under deep learning NLP settings, requiring models to demonstrate a blend of language understanding and complex logical reasoning. It includes different types of logical problems, such as categorical reasoning, sufficient and necessary conditional reasoning, disjunctive reasoning, and conjunctive reasoning, all key to deductive reasoning. This dataset provides a rigorous test of AI's logical reasoning capabilities and its ability to handle problems similar to those faced by human experts.\\n- **HendrycksTest**. The HendrycksTest(Hendrycks et al., 2020), also known as the Measuring Massive Multitask\\nLanguage Understanding (MMLU) test, is a massive multitask test that includes multiple-choice questions from a wide range of knowledge domains, covering 57 tasks in areas such as elementary mathematics, US history, computer science, and law. This test aims to measure the multitask accuracy of text models, requiring them to demonstrate extensive world knowledge and problem-solving ability. The results show that while most recent models have near random-chance accuracy, larger models like GPT-3 have shown improvement, but still fall short of expert-level accuracy across all tasks. The HendrycksTest serves as a comprehensive tool for evaluating the breadth and depth of models' academic and professional understanding, identifying significant shortcomings, and highlighting areas needing substantial improvement, especially in socially important subjects like morality and law.\")]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from waterllmarks.pipeline import DictParser\n",
    "from langchain_core.runnables import ConfigurableField, RunnableLambda\n",
    "\n",
    "\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"mmr\", search_kwargs={\"search_k\": 20}\n",
    ").configurable_alternatives(\n",
    "    ConfigurableField(id=\"retriever\"),\n",
    "    default_key=\"chroma\",\n",
    "    empty=RunnableLambda(lambda _: []),\n",
    ")\n",
    "\n",
    "print(ds.qas[0][\"reference_context_ids\"])\n",
    "(DictParser(\"user_input\") | retriever).invoke(ds.qas[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QA without RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '1c40f174-bda1-4c48-88fb-b7449e999067',\n",
       " 'user_input': \"Does the Turing Test assess a machine's ability to exhibit intelligent behavior equivalent to that of a human?\",\n",
       " 'reference': 'Yes',\n",
       " 'reference_context_ids': ['1626f35b-7ffc-4736-bd6e-2a2f20e1aa8b'],\n",
       " 'reference_contexts': [\"Language represents a rigorously structured communicative system characterized by its grammar and vocabulary. It serves as the principal medium through which humans articulate and convey meaning. This conception of language as a structured communicative tool is pivotal in the realm of computational linguistics, particularly in the development and evaluation of natural language processing (NLP) algorithms. A seminal aspect in this field is the Turing Test, proposed by Alan Turing in 1950 [1], which evaluates a machine's ability to exhibit intelligent behavior equivalent to, or indistinguishable from, that of a human. In this context, the Turing Test primarily assesses the machine's capability to perform tasks involving language comprehension and generation, reflecting the intricate role of linguistic structure in the artificial replication of human-like communication. Language model (LM) is a fundamental element employed in a multitude of NLP tasks, such as text generation, machine translation, and speech recognition [2, 3]. These models are intricately designed to comprehend, generate, and manipulate human language. The training of language models involves large-scale corpora, enabling them to learn universal language representations. This training process is critical for the models to capture the semantics of words in varying contexts [4, 5, 6]. Notably, the fidelity of these representations is frequently contingent on the word frequency within the training corpus. Such dependency underscores the importance of a comprehensive and diverse corpus in training LMs, as it directly impacts their ability to reflect and understand the nuances of natural language accurately.  \\nThe intricacy of language models and their reliance on corpus characteristics are vital considerations in advancing NLP\\ntechnologies, which underscores the significance of human-like language comprehension and production in artificial intelligence systems.  \\nThe forefront of advancements in language model technology has been marked by the emergence of Large Language Models (LLMs). This evolution signifies a paradigm shift in the field of NLP and extends its impact to broader applications. LLMs leverage deep learning methodologies [7], utilizing extensive datasets to perform complex tasks such as understanding, summarizing, generating, and predicting novel content. These models operate by processing an input text and iteratively predicting subsequent tokens or words. A distinguishing feature of LLMs is their vast parameter space, encompassing tens to hundreds of billion parameters, in stark contrast to their predecessors [4, 3]. In addition, they are trained on significantly larger datasets, ranging from several gigabytes to terabytes in size. This exponential increase in both computational capacity and training data volume has not only enhanced the performance of LLMs in conventional NLP tasks but also has\"],\n",
       " 'pipeline_input': \"Does the Turing Test assess a machine's ability to exhibit intelligent behavior equivalent to that of a human?\",\n",
       " 'response': \"No, the Turing Test does not definitively assess a machine's ability to exhibit intelligent behavior equivalent to that of a human. It only measures a machine's ability to imitate human-like conversation.\"}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "\n",
    "from waterllmarks.pipeline import DictParser\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"pipeline_input\"],\n",
    "    template=format_prompt(\"\"\"\n",
    "        Answer the question. Keep the answer short and concise.\n",
    "\n",
    "        Question: {pipeline_input}\n",
    "        \n",
    "        Answer: \n",
    "        \"\"\"),\n",
    ")\n",
    "\n",
    "set_input = RunnablePassthrough.assign(pipeline_input=DictParser(\"user_input\"))\n",
    "output_parser = StrOutputParser(name=\"content\") | (lambda x: x.strip())\n",
    "\n",
    "llm = RunnablePassthrough.assign(response=prompt | llm_client | output_parser)\n",
    "\n",
    "norag_chain = set_input | llm.with_config(llm=\"ollama\")\n",
    "\n",
    "norag_chain.invoke(ds.qas[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QA with RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '1c40f174-bda1-4c48-88fb-b7449e999067',\n",
       " 'user_input': \"Does the Turing Test assess a machine's ability to exhibit intelligent behavior equivalent to that of a human?\",\n",
       " 'reference': 'Yes',\n",
       " 'reference_context_ids': ['1626f35b-7ffc-4736-bd6e-2a2f20e1aa8b'],\n",
       " 'reference_contexts': [\"Language represents a rigorously structured communicative system characterized by its grammar and vocabulary. It serves as the principal medium through which humans articulate and convey meaning. This conception of language as a structured communicative tool is pivotal in the realm of computational linguistics, particularly in the development and evaluation of natural language processing (NLP) algorithms. A seminal aspect in this field is the Turing Test, proposed by Alan Turing in 1950 [1], which evaluates a machine's ability to exhibit intelligent behavior equivalent to, or indistinguishable from, that of a human. In this context, the Turing Test primarily assesses the machine's capability to perform tasks involving language comprehension and generation, reflecting the intricate role of linguistic structure in the artificial replication of human-like communication. Language model (LM) is a fundamental element employed in a multitude of NLP tasks, such as text generation, machine translation, and speech recognition [2, 3]. These models are intricately designed to comprehend, generate, and manipulate human language. The training of language models involves large-scale corpora, enabling them to learn universal language representations. This training process is critical for the models to capture the semantics of words in varying contexts [4, 5, 6]. Notably, the fidelity of these representations is frequently contingent on the word frequency within the training corpus. Such dependency underscores the importance of a comprehensive and diverse corpus in training LMs, as it directly impacts their ability to reflect and understand the nuances of natural language accurately.  \\nThe intricacy of language models and their reliance on corpus characteristics are vital considerations in advancing NLP\\ntechnologies, which underscores the significance of human-like language comprehension and production in artificial intelligence systems.  \\nThe forefront of advancements in language model technology has been marked by the emergence of Large Language Models (LLMs). This evolution signifies a paradigm shift in the field of NLP and extends its impact to broader applications. LLMs leverage deep learning methodologies [7], utilizing extensive datasets to perform complex tasks such as understanding, summarizing, generating, and predicting novel content. These models operate by processing an input text and iteratively predicting subsequent tokens or words. A distinguishing feature of LLMs is their vast parameter space, encompassing tens to hundreds of billion parameters, in stark contrast to their predecessors [4, 3]. In addition, they are trained on significantly larger datasets, ranging from several gigabytes to terabytes in size. This exponential increase in both computational capacity and training data volume has not only enhanced the performance of LLMs in conventional NLP tasks but also has\"],\n",
       " 'pipeline_input': \"Does the Turing Test assess a machine's ability to exhibit intelligent behavior equivalent to that of a human?\",\n",
       " 'retrieved_contexts': [Document(id='a09c2e9a-dd24-4e36-9b2e-7d4e15e58639', metadata={'creation_datetime': '2024-03-04', 'file_name': '2402.09193v2.md', 'file_path': 'paper_data/2402.09193v2.md', 'file_size': 69224, 'file_type': '', 'last_accessed_datetime': '2024-03-04', 'last_modified_datetime': '2024-02-22', 'title': '(Ir)Rationality And Cognitive Biases In Large Language Models'}, page_content='## 1 Introduction\\n\\n can be used to test the rationality of a model; this is a complex problem which requires a consensus what is deemed rational and irrational.  \\nMirco Musolesi University College London University of Bologna m.musolesi@ucl.ac.uk In using methods designed to evaluate human reasoning, it is important to acknowledge the performance vs. competence debate [14]. This line of argument encourages *species-fair* comparisons between humans and machines, meaning that we should design tests specific to either humans or machines, as otherwise apparent failures may not reflect underlying capabilities but only superficial differences. Lampinen[15] discusses this problem when it comes to language models in particular, highlighting that different approaches must be taking to evaluate cognitive and foundation models. However, if we take the purpose of LLMs to be to produce human-like language, perhaps the best approach is precisely to evaluate their output with tasks designed to evaluate humans. This is the approach we have taken in this paper - in order to identify whether LLMs reason rationally, or whether they exhibit biases that can be assimilated to those present in human decision-making, the most appropriate approach is therefore to use tasks that were initially designed for humans. Building on this debate and looking at LLMs being evaluated using human tests, Hagendorff et al. [16] have proposed the creation of a new field of research called *machine psychology*, which would treat LLMs as participants in psychological experiments. The approach employed in this paper precisely applies tests from psychology that were originally designed for humans, in this case to evaluate rational and irrational reasoning displayed but such models. Further to this, some have even discussed the potential of using LLMs as participants in cognitive experiments instead of humans [17], although some see this proposal as too optimistic [18], and others warn against excessive anthropomorphism [19]. One argument against the use of such models in cognitive experiments is that LLMs may be effective at approximating average human judgements, but are not good at capturing the variation in human behaviour [20]. One potential avenue to address this issue is current work on language models impersonating different roles [21], in this way capturing some of the variation in human behaviour. Binz and Schulz [22] show that after finetuning LLMs on data from psychological experiments, they can become accurate cognitive models, which they claim begins paving the way for the potential of using these models to study human behaviour. Park et al. [23] combine large language models with computational interactive agents to simulate human behaviour'),\n",
       "  Document(id='67507d36-6462-4cfe-9d90-de2301619880', metadata={'creation_datetime': '2024-03-04', 'file_name': '2402.08690v1.md', 'file_path': 'paper_data/2402.08690v1.md', 'file_size': 80308, 'file_type': '', 'last_accessed_datetime': '2024-03-04', 'last_modified_datetime': '2024-02-22', 'title': 'If Turing Played Piano With An Artificial Partner'}, page_content='## Academic Press\\n\\n550/arXiv.1312.6114 .\\nhttp://arxiv.org/abs/1312.6114 Accessed 2023-09-12\\n[39] Rezende, D.J., Mohamed, S., Wierstra, D.: Stochastic Backpropagation and\\nApproximate Inference in Deep Generative Models. In: Proceedings of the 31st International Conference on Machine Learning, pp. 1278–1286. PMLR, ??? (2014). ISSN: 1938-7228. https://proceedings.mlr.press/v32/rezende14.html Accessed 2023-09-12\\n[40] Jackson, S.A., Martin, A.J., Eklund, R.C.: Long and Short Measures of Flow: The\\nConstruct Validity of the FSS-2, DFS-2, and New Brief Counterparts. Journal of Sport and Exercise Psychology 30(5), 561–587 (2008) https://doi.org/10.1123/\\njsep.30.5.561 . Publisher: Human Kinetics, Inc. Section: Journal of Sport and Exercise Psychology. Accessed 2023-03-31  \\n[41] Singer, J.D., Willett, J.B.: Applied Longitudinal Data Analysis: Modeling Change\\nand Event Occurrence. Oxford University Press, ??? (2003)\\n[42] Ortiz, C.L.: Why we need a physically embodied turing test and what it might\\nlook like. AI Magazine 37(1), 55–62 (2016) https://doi.org/10.1609/aimag.v37i1.\\n2645\\n[43] Pfeiffer, U.J., Timmermans, B., Bente, G., Vogeley, K., Schilbach, L.: A nonverbal turing test: Differentiating mind from machine in gaze-based social interaction. PLoS ONE 6(11) (2011) https://doi.org/10.1371/journal.pone.0027591\\n[44] Swisher, N., Dotov, D., Chemero, A.: Ascribing Moral Value and the Embodied\\nTuring Test. In: Proceedings of the 10th International Conference on Artificial Life: Workshops, pp. 40–45 (2006)\\n[45] Kostrubiec, V., Dumas,'),\n",
       "  Document(id='17262aea-b038-4549-b89a-2b7275cd360f', metadata={'creation_datetime': '2024-03-04', 'file_name': '2402.09880v1.md', 'file_path': 'paper_data/2402.09880v1.md', 'file_size': 190252, 'file_type': '', 'last_accessed_datetime': '2024-03-04', 'last_modified_datetime': '2024-02-22', 'title': 'Inadequacies Of Large Language Model Benchmarks In The Era Of Generative Artificial Intelligence'}, page_content='## B. Genuine Reasoning Vs Technical Optimization\\n\\n    |                  |\\n| ×                         | ×                     | ×                |\\n| AGIEval [10]              |                       |                  |\\n| ×                         | ×                     | ×                |\\n| ToolAlpaca [30]           |                       |                  |\\n| ×                         | ×                     | ×                |\\n| HELM [31]                 |                       |                  |\\n| △                         | ×                     | ×                |\\n| ToolBench [32]            |'),\n",
       "  Document(id='a665fad4-8dd1-4be7-86af-34bffa0fbb2c', metadata={'creation_datetime': '2024-03-04', 'file_name': '2402.09099v1.md', 'file_path': 'paper_data/2402.09099v1.md', 'file_size': 96559, 'file_type': '', 'last_accessed_datetime': '2024-03-04', 'last_modified_datetime': '2024-02-22', 'title': 'Exploring Neuron Interactions And Emergence In Llms: From The Multifractal Analysis Perspective'}, page_content=\"## B.3. Benchmarks\\n\\nLogiQA**. LogiQA(Liu et al., 2020) is a dataset consisting of 8,678 QA instances that focus on evaluating machine\\nreading comprehension with an emphasis on logical reasoning. It is derived from the National Civil Servants Examination of China and covers various types of deductive reasoning. This dataset presents a significant challenge for state-of-the-art neural models, which perform notably worse than humans in these tasks. LogiQA serves as a unique benchmark for testing logical AI under deep learning NLP settings, requiring models to demonstrate a blend of language understanding and complex logical reasoning. It includes different types of logical problems, such as categorical reasoning, sufficient and necessary conditional reasoning, disjunctive reasoning, and conjunctive reasoning, all key to deductive reasoning. This dataset provides a rigorous test of AI's logical reasoning capabilities and its ability to handle problems similar to those faced by human experts.\\n- **HendrycksTest**. The HendrycksTest(Hendrycks et al., 2020), also known as the Measuring Massive Multitask\\nLanguage Understanding (MMLU) test, is a massive multitask test that includes multiple-choice questions from a wide range of knowledge domains, covering 57 tasks in areas such as elementary mathematics, US history, computer science, and law. This test aims to measure the multitask accuracy of text models, requiring them to demonstrate extensive world knowledge and problem-solving ability. The results show that while most recent models have near random-chance accuracy, larger models like GPT-3 have shown improvement, but still fall short of expert-level accuracy across all tasks. The HendrycksTest serves as a comprehensive tool for evaluating the breadth and depth of models' academic and professional understanding, identifying significant shortcomings, and highlighting areas needing substantial improvement, especially in socially important subjects like morality and law.\")],\n",
       " 'context': \"## 1 Introduction\\n\\n can be used to test the rationality of a model; this is a complex problem which requires a consensus what is deemed rational and irrational.  \\nMirco Musolesi University College London University of Bologna m.musolesi@ucl.ac.uk In using methods designed to evaluate human reasoning, it is important to acknowledge the performance vs. competence debate [14]. This line of argument encourages *species-fair* comparisons between humans and machines, meaning that we should design tests specific to either humans or machines, as otherwise apparent failures may not reflect underlying capabilities but only superficial differences. Lampinen[15] discusses this problem when it comes to language models in particular, highlighting that different approaches must be taking to evaluate cognitive and foundation models. However, if we take the purpose of LLMs to be to produce human-like language, perhaps the best approach is precisely to evaluate their output with tasks designed to evaluate humans. This is the approach we have taken in this paper - in order to identify whether LLMs reason rationally, or whether they exhibit biases that can be assimilated to those present in human decision-making, the most appropriate approach is therefore to use tasks that were initially designed for humans. Building on this debate and looking at LLMs being evaluated using human tests, Hagendorff et al. [16] have proposed the creation of a new field of research called *machine psychology*, which would treat LLMs as participants in psychological experiments. The approach employed in this paper precisely applies tests from psychology that were originally designed for humans, in this case to evaluate rational and irrational reasoning displayed but such models. Further to this, some have even discussed the potential of using LLMs as participants in cognitive experiments instead of humans [17], although some see this proposal as too optimistic [18], and others warn against excessive anthropomorphism [19]. One argument against the use of such models in cognitive experiments is that LLMs may be effective at approximating average human judgements, but are not good at capturing the variation in human behaviour [20]. One potential avenue to address this issue is current work on language models impersonating different roles [21], in this way capturing some of the variation in human behaviour. Binz and Schulz [22] show that after finetuning LLMs on data from psychological experiments, they can become accurate cognitive models, which they claim begins paving the way for the potential of using these models to study human behaviour. Park et al. [23] combine large language models with computational interactive agents to simulate human behaviour\\n\\n## Academic Press\\n\\n550/arXiv.1312.6114 .\\nhttp://arxiv.org/abs/1312.6114 Accessed 2023-09-12\\n[39] Rezende, D.J., Mohamed, S., Wierstra, D.: Stochastic Backpropagation and\\nApproximate Inference in Deep Generative Models. In: Proceedings of the 31st International Conference on Machine Learning, pp. 1278–1286. PMLR, ??? (2014). ISSN: 1938-7228. https://proceedings.mlr.press/v32/rezende14.html Accessed 2023-09-12\\n[40] Jackson, S.A., Martin, A.J., Eklund, R.C.: Long and Short Measures of Flow: The\\nConstruct Validity of the FSS-2, DFS-2, and New Brief Counterparts. Journal of Sport and Exercise Psychology 30(5), 561–587 (2008) https://doi.org/10.1123/\\njsep.30.5.561 . Publisher: Human Kinetics, Inc. Section: Journal of Sport and Exercise Psychology. Accessed 2023-03-31  \\n[41] Singer, J.D., Willett, J.B.: Applied Longitudinal Data Analysis: Modeling Change\\nand Event Occurrence. Oxford University Press, ??? (2003)\\n[42] Ortiz, C.L.: Why we need a physically embodied turing test and what it might\\nlook like. AI Magazine 37(1), 55–62 (2016) https://doi.org/10.1609/aimag.v37i1.\\n2645\\n[43] Pfeiffer, U.J., Timmermans, B., Bente, G., Vogeley, K., Schilbach, L.: A nonverbal turing test: Differentiating mind from machine in gaze-based social interaction. PLoS ONE 6(11) (2011) https://doi.org/10.1371/journal.pone.0027591\\n[44] Swisher, N., Dotov, D., Chemero, A.: Ascribing Moral Value and the Embodied\\nTuring Test. In: Proceedings of the 10th International Conference on Artificial Life: Workshops, pp. 40–45 (2006)\\n[45] Kostrubiec, V., Dumas,\\n\\n## B. Genuine Reasoning Vs Technical Optimization\\n\\n    |                  |\\n| ×                         | ×                     | ×                |\\n| AGIEval [10]              |                       |                  |\\n| ×                         | ×                     | ×                |\\n| ToolAlpaca [30]           |                       |                  |\\n| ×                         | ×                     | ×                |\\n| HELM [31]                 |                       |                  |\\n| △                         | ×                     | ×                |\\n| ToolBench [32]            |\\n\\n## B.3. Benchmarks\\n\\nLogiQA**. LogiQA(Liu et al., 2020) is a dataset consisting of 8,678 QA instances that focus on evaluating machine\\nreading comprehension with an emphasis on logical reasoning. It is derived from the National Civil Servants Examination of China and covers various types of deductive reasoning. This dataset presents a significant challenge for state-of-the-art neural models, which perform notably worse than humans in these tasks. LogiQA serves as a unique benchmark for testing logical AI under deep learning NLP settings, requiring models to demonstrate a blend of language understanding and complex logical reasoning. It includes different types of logical problems, such as categorical reasoning, sufficient and necessary conditional reasoning, disjunctive reasoning, and conjunctive reasoning, all key to deductive reasoning. This dataset provides a rigorous test of AI's logical reasoning capabilities and its ability to handle problems similar to those faced by human experts.\\n- **HendrycksTest**. The HendrycksTest(Hendrycks et al., 2020), also known as the Measuring Massive Multitask\\nLanguage Understanding (MMLU) test, is a massive multitask test that includes multiple-choice questions from a wide range of knowledge domains, covering 57 tasks in areas such as elementary mathematics, US history, computer science, and law. This test aims to measure the multitask accuracy of text models, requiring them to demonstrate extensive world knowledge and problem-solving ability. The results show that while most recent models have near random-chance accuracy, larger models like GPT-3 have shown improvement, but still fall short of expert-level accuracy across all tasks. The HendrycksTest serves as a comprehensive tool for evaluating the breadth and depth of models' academic and professional understanding, identifying significant shortcomings, and highlighting areas needing substantial improvement, especially in socially important subjects like morality and law.\",\n",
       " 'response': ' No, the Turing Test does not directly assess a machine\\'s ability to exhibit intelligent behavior equivalent to that of a human. Instead, it evaluates a machine\\'s ability to mimic human-like conversation. The context provided discusses the creation of a new field called \"machine psychology\" that aims to treat language models as participants in psychological experiments, which could potentially be used to assess a machine\\'s rational and irrational reasoning, similar to humans.'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textwrap import dedent\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.runnables import RunnablePassthrough, chain, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from waterllmarks.pipeline import DictParser, DictWrapper\n",
    "\n",
    "rag_prompt = PromptTemplate(\n",
    "    input_variables=[\"pipeline_input\", \"context\"],\n",
    "    template=format_prompt(\"\"\"\n",
    "        Answer the question using the provided context. Keep the answer short and concise.\n",
    "        \n",
    "        Context:\n",
    "        {context}\n",
    "        \n",
    "        Question: {pipeline_input}\n",
    "\n",
    "        Answer: \n",
    "        \"\"\"),\n",
    ")\n",
    "\n",
    "context_formatter = RunnableLambda(\n",
    "    lambda docs: \"\\n\\n\".join([doc.page_content for doc in docs]),\n",
    ")  # list[Document] -> str\n",
    "\n",
    "ragllm = (\n",
    "    RunnablePassthrough.assign(\n",
    "        retrieved_contexts=DictParser(\"pipeline_input\") | retriever,\n",
    "    )  # {\"pipeline_input\": str, \"retrieved_contexts\": list[Document]}\n",
    "    | RunnablePassthrough.assign(\n",
    "        context=DictParser(\"retrieved_contexts\") | context_formatter,\n",
    "    )  # {\"pipeline_input\": str, \"retrieved_contexts\": list[Document], \"context\": str}\n",
    "    | RunnablePassthrough.assign(\n",
    "        response=rag_prompt | llm_client | output_parser,\n",
    "    )  # {\"pipeline_input\": str, \"retrieved_contexts\": list[Document], \"context\": str, \"response\": str}\n",
    ").with_config(llm=\"ollama\")\n",
    "\n",
    "rag_chain = set_input | ragllm\n",
    "\n",
    "rag_chain.invoke(ds.qas[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QA with RAG but no context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '1c40f174-bda1-4c48-88fb-b7449e999067',\n",
       " 'user_input': \"Does the Turing Test assess a machine's ability to exhibit intelligent behavior equivalent to that of a human?\",\n",
       " 'reference': 'Yes',\n",
       " 'reference_context_ids': ['1626f35b-7ffc-4736-bd6e-2a2f20e1aa8b'],\n",
       " 'reference_contexts': [\"Language represents a rigorously structured communicative system characterized by its grammar and vocabulary. It serves as the principal medium through which humans articulate and convey meaning. This conception of language as a structured communicative tool is pivotal in the realm of computational linguistics, particularly in the development and evaluation of natural language processing (NLP) algorithms. A seminal aspect in this field is the Turing Test, proposed by Alan Turing in 1950 [1], which evaluates a machine's ability to exhibit intelligent behavior equivalent to, or indistinguishable from, that of a human. In this context, the Turing Test primarily assesses the machine's capability to perform tasks involving language comprehension and generation, reflecting the intricate role of linguistic structure in the artificial replication of human-like communication. Language model (LM) is a fundamental element employed in a multitude of NLP tasks, such as text generation, machine translation, and speech recognition [2, 3]. These models are intricately designed to comprehend, generate, and manipulate human language. The training of language models involves large-scale corpora, enabling them to learn universal language representations. This training process is critical for the models to capture the semantics of words in varying contexts [4, 5, 6]. Notably, the fidelity of these representations is frequently contingent on the word frequency within the training corpus. Such dependency underscores the importance of a comprehensive and diverse corpus in training LMs, as it directly impacts their ability to reflect and understand the nuances of natural language accurately.  \\nThe intricacy of language models and their reliance on corpus characteristics are vital considerations in advancing NLP\\ntechnologies, which underscores the significance of human-like language comprehension and production in artificial intelligence systems.  \\nThe forefront of advancements in language model technology has been marked by the emergence of Large Language Models (LLMs). This evolution signifies a paradigm shift in the field of NLP and extends its impact to broader applications. LLMs leverage deep learning methodologies [7], utilizing extensive datasets to perform complex tasks such as understanding, summarizing, generating, and predicting novel content. These models operate by processing an input text and iteratively predicting subsequent tokens or words. A distinguishing feature of LLMs is their vast parameter space, encompassing tens to hundreds of billion parameters, in stark contrast to their predecessors [4, 3]. In addition, they are trained on significantly larger datasets, ranging from several gigabytes to terabytes in size. This exponential increase in both computational capacity and training data volume has not only enhanced the performance of LLMs in conventional NLP tasks but also has\"],\n",
       " 'pipeline_input': \"Does the Turing Test assess a machine's ability to exhibit intelligent behavior equivalent to that of a human?\",\n",
       " 'retrieved_contexts': [],\n",
       " 'context': '',\n",
       " 'response': \" Yes, the Turing Test assesses a machine's ability to exhibit intelligent behavior that is indistinguishable from a human's.\"}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_ragllm = ragllm.with_config(retriever=\"empty\")\n",
    "emptyrag_chain = set_input | empty_ragllm\n",
    "\n",
    "# rag_results = rag_chain.batch(ds.qas)\n",
    "# rag_results[0]\n",
    "\n",
    "emptyrag_chain.invoke(ds.qas[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to ./nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to ./nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "149a5353fc8e45e6bd961e23a5c44720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d262fdbcc2124ea081438cfceddcfc8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9b7a40d2157474a90553adefc23df55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'bleu_score': 0.031939173411512777,\n",
       " 'rouge_score': 0.11000000000000001,\n",
       " 'meta_meteor_score': 0.14127721080986377,\n",
       " 'non_llm_string_similarity': 0.13839285714285715,\n",
       " 'semantic_similarity': 0.6719361752292586,\n",
       " 'factual_correctness': 0.335,\n",
       " 'llm_context_precision_without_reference': 0.49999999998333333,\n",
       " 'context_recall': 0.7,\n",
       " 'faithfulness': 0.7333333333333334,\n",
       " 'context_overlap': 0.0,\n",
       " 'retrieved_context_similarity': 0.6967715587971113}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import pickle\n",
    "from functools import partial\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from waterllmarks.evaluation import DEFAULT_ALL_METRICS, WLLMKResult\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from waterllmarks.evaluation import evaluate\n",
    "from ragas import RunConfig\n",
    "from langchain_core.runnables import RunnableConfig, RunnableSequence\n",
    "from waterllmarks.pipeline import ProgressBarCallback, ThreadedSequence\n",
    "from collections import Counter\n",
    "\n",
    "llm_wrapper = LangchainLLMWrapper(llm_client)\n",
    "embedding_wrapper = LangchainEmbeddingsWrapper(embedding_fn)\n",
    "\n",
    "\n",
    "eval_fn = partial(\n",
    "    evaluate,\n",
    "    metrics=DEFAULT_ALL_METRICS,\n",
    "    llm=llm_wrapper,\n",
    "    embeddings=embedding_wrapper,\n",
    "    runconfig=RunConfig(seed=SEED, max_workers=2),\n",
    ")\n",
    "\n",
    "inputs = ds.qas[:100]\n",
    "\n",
    "rag_results = ThreadedSequence(rag_chain).batch(inputs)\n",
    "# empty_rag_results = ThreadedSequence(emptyrag_chain).batch(inputs)\n",
    "\n",
    "baseline_evals = WLLMKResult(\n",
    "    rag=eval_fn(rag_results),\n",
    "    #    empty_rag=eval_fn(empty_rag_results),\n",
    ")\n",
    "\n",
    "baseline_evals.save(f\"results/{SEED}_baseline_results.synthesis\")\n",
    "baseline_evals.save(f\"results/{SEED}_baseline_results_noempty.synthesis\")\n",
    "\n",
    "baseline_evals.synthesis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seting up the new baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_results = rag_chain.batch(inputs)\n",
    "baseline_qas = [\n",
    "    {\n",
    "        \"id\": res[\"id\"],\n",
    "        \"user_input\": res[\"user_input\"],\n",
    "        \"reference\": res[\"response\"],\n",
    "        \"reference_contexts\": [doc.page_content for doc in res[\"retrieved_contexts\"]],\n",
    "        \"reference_context_ids\": [doc.id for doc in res[\"retrieved_contexts\"]],\n",
    "    }\n",
    "    for res in baseline_results\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watermarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TokenWatermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '1c40f174-bda1-4c48-88fb-b7449e999067',\n",
       " 'user_input': \"Does the Turing Test assess a machine's ability to exhibit intelligent behavior equivalent to that of a human?\",\n",
       " 'reference': \" No, the Turing Test does not directly assess a machine's ability to exhibit intelligent behavior equivalent to that of a human. Instead, it evaluates a machine's ability to mimic human-like conversation.\",\n",
       " 'reference_contexts': ['## 1 Introduction\\n\\n can be used to test the rationality of a model; this is a complex problem which requires a consensus what is deemed rational and irrational.  \\nMirco Musolesi University College London University of Bologna m.musolesi@ucl.ac.uk In using methods designed to evaluate human reasoning, it is important to acknowledge the performance vs. competence debate [14]. This line of argument encourages *species-fair* comparisons between humans and machines, meaning that we should design tests specific to either humans or machines, as otherwise apparent failures may not reflect underlying capabilities but only superficial differences. Lampinen[15] discusses this problem when it comes to language models in particular, highlighting that different approaches must be taking to evaluate cognitive and foundation models. However, if we take the purpose of LLMs to be to produce human-like language, perhaps the best approach is precisely to evaluate their output with tasks designed to evaluate humans. This is the approach we have taken in this paper - in order to identify whether LLMs reason rationally, or whether they exhibit biases that can be assimilated to those present in human decision-making, the most appropriate approach is therefore to use tasks that were initially designed for humans. Building on this debate and looking at LLMs being evaluated using human tests, Hagendorff et al. [16] have proposed the creation of a new field of research called *machine psychology*, which would treat LLMs as participants in psychological experiments. The approach employed in this paper precisely applies tests from psychology that were originally designed for humans, in this case to evaluate rational and irrational reasoning displayed but such models. Further to this, some have even discussed the potential of using LLMs as participants in cognitive experiments instead of humans [17], although some see this proposal as too optimistic [18], and others warn against excessive anthropomorphism [19]. One argument against the use of such models in cognitive experiments is that LLMs may be effective at approximating average human judgements, but are not good at capturing the variation in human behaviour [20]. One potential avenue to address this issue is current work on language models impersonating different roles [21], in this way capturing some of the variation in human behaviour. Binz and Schulz [22] show that after finetuning LLMs on data from psychological experiments, they can become accurate cognitive models, which they claim begins paving the way for the potential of using these models to study human behaviour. Park et al. [23] combine large language models with computational interactive agents to simulate human behaviour',\n",
       "  '## Academic Press\\n\\n550/arXiv.1312.6114 .\\nhttp://arxiv.org/abs/1312.6114 Accessed 2023-09-12\\n[39] Rezende, D.J., Mohamed, S., Wierstra, D.: Stochastic Backpropagation and\\nApproximate Inference in Deep Generative Models. In: Proceedings of the 31st International Conference on Machine Learning, pp. 1278–1286. PMLR, ??? (2014). ISSN: 1938-7228. https://proceedings.mlr.press/v32/rezende14.html Accessed 2023-09-12\\n[40] Jackson, S.A., Martin, A.J., Eklund, R.C.: Long and Short Measures of Flow: The\\nConstruct Validity of the FSS-2, DFS-2, and New Brief Counterparts. Journal of Sport and Exercise Psychology 30(5), 561–587 (2008) https://doi.org/10.1123/\\njsep.30.5.561 . Publisher: Human Kinetics, Inc. Section: Journal of Sport and Exercise Psychology. Accessed 2023-03-31  \\n[41] Singer, J.D., Willett, J.B.: Applied Longitudinal Data Analysis: Modeling Change\\nand Event Occurrence. Oxford University Press, ??? (2003)\\n[42] Ortiz, C.L.: Why we need a physically embodied turing test and what it might\\nlook like. AI Magazine 37(1), 55–62 (2016) https://doi.org/10.1609/aimag.v37i1.\\n2645\\n[43] Pfeiffer, U.J., Timmermans, B., Bente, G., Vogeley, K., Schilbach, L.: A nonverbal turing test: Differentiating mind from machine in gaze-based social interaction. PLoS ONE 6(11) (2011) https://doi.org/10.1371/journal.pone.0027591\\n[44] Swisher, N., Dotov, D., Chemero, A.: Ascribing Moral Value and the Embodied\\nTuring Test. In: Proceedings of the 10th International Conference on Artificial Life: Workshops, pp. 40–45 (2006)\\n[45] Kostrubiec, V., Dumas,',\n",
       "  '## B. Genuine Reasoning Vs Technical Optimization\\n\\n    |                  |\\n| ×                         | ×                     | ×                |\\n| AGIEval [10]              |                       |                  |\\n| ×                         | ×                     | ×                |\\n| ToolAlpaca [30]           |                       |                  |\\n| ×                         | ×                     | ×                |\\n| HELM [31]                 |                       |                  |\\n| △                         | ×                     | ×                |\\n| ToolBench [32]            |',\n",
       "  \"## B.3. Benchmarks\\n\\nLogiQA**. LogiQA(Liu et al., 2020) is a dataset consisting of 8,678 QA instances that focus on evaluating machine\\nreading comprehension with an emphasis on logical reasoning. It is derived from the National Civil Servants Examination of China and covers various types of deductive reasoning. This dataset presents a significant challenge for state-of-the-art neural models, which perform notably worse than humans in these tasks. LogiQA serves as a unique benchmark for testing logical AI under deep learning NLP settings, requiring models to demonstrate a blend of language understanding and complex logical reasoning. It includes different types of logical problems, such as categorical reasoning, sufficient and necessary conditional reasoning, disjunctive reasoning, and conjunctive reasoning, all key to deductive reasoning. This dataset provides a rigorous test of AI's logical reasoning capabilities and its ability to handle problems similar to those faced by human experts.\\n- **HendrycksTest**. The HendrycksTest(Hendrycks et al., 2020), also known as the Measuring Massive Multitask\\nLanguage Understanding (MMLU) test, is a massive multitask test that includes multiple-choice questions from a wide range of knowledge domains, covering 57 tasks in areas such as elementary mathematics, US history, computer science, and law. This test aims to measure the multitask accuracy of text models, requiring them to demonstrate extensive world knowledge and problem-solving ability. The results show that while most recent models have near random-chance accuracy, larger models like GPT-3 have shown improvement, but still fall short of expert-level accuracy across all tasks. The HendrycksTest serves as a comprehensive tool for evaluating the breadth and depth of models' academic and professional understanding, identifying significant shortcomings, and highlighting areas needing substantial improvement, especially in socially important subjects like morality and law.\"],\n",
       " 'reference_context_ids': ['a09c2e9a-dd24-4e36-9b2e-7d4e15e58639',\n",
       "  '67507d36-6462-4cfe-9d90-de2301619880',\n",
       "  '17262aea-b038-4549-b89a-2b7275cd360f',\n",
       "  'a665fad4-8dd1-4be7-86af-34bffa0fbb2c'],\n",
       " 'pipeline_input': \"[19f29704b36419f0]Does the Turing Test assess a machine's ability to exhibit intelligent behavior equivalent to that of a human?\",\n",
       " 'retrieved_contexts': [Document(id='a09c2e9a-dd24-4e36-9b2e-7d4e15e58639', metadata={'creation_datetime': '2024-03-04', 'file_name': '2402.09193v2.md', 'file_path': 'paper_data/2402.09193v2.md', 'file_size': 69224, 'file_type': '', 'last_accessed_datetime': '2024-03-04', 'last_modified_datetime': '2024-02-22', 'title': '(Ir)Rationality And Cognitive Biases In Large Language Models'}, page_content='## 1 Introduction\\n\\n can be used to test the rationality of a model; this is a complex problem which requires a consensus what is deemed rational and irrational.  \\nMirco Musolesi University College London University of Bologna m.musolesi@ucl.ac.uk In using methods designed to evaluate human reasoning, it is important to acknowledge the performance vs. competence debate [14]. This line of argument encourages *species-fair* comparisons between humans and machines, meaning that we should design tests specific to either humans or machines, as otherwise apparent failures may not reflect underlying capabilities but only superficial differences. Lampinen[15] discusses this problem when it comes to language models in particular, highlighting that different approaches must be taking to evaluate cognitive and foundation models. However, if we take the purpose of LLMs to be to produce human-like language, perhaps the best approach is precisely to evaluate their output with tasks designed to evaluate humans. This is the approach we have taken in this paper - in order to identify whether LLMs reason rationally, or whether they exhibit biases that can be assimilated to those present in human decision-making, the most appropriate approach is therefore to use tasks that were initially designed for humans. Building on this debate and looking at LLMs being evaluated using human tests, Hagendorff et al. [16] have proposed the creation of a new field of research called *machine psychology*, which would treat LLMs as participants in psychological experiments. The approach employed in this paper precisely applies tests from psychology that were originally designed for humans, in this case to evaluate rational and irrational reasoning displayed but such models. Further to this, some have even discussed the potential of using LLMs as participants in cognitive experiments instead of humans [17], although some see this proposal as too optimistic [18], and others warn against excessive anthropomorphism [19]. One argument against the use of such models in cognitive experiments is that LLMs may be effective at approximating average human judgements, but are not good at capturing the variation in human behaviour [20]. One potential avenue to address this issue is current work on language models impersonating different roles [21], in this way capturing some of the variation in human behaviour. Binz and Schulz [22] show that after finetuning LLMs on data from psychological experiments, they can become accurate cognitive models, which they claim begins paving the way for the potential of using these models to study human behaviour. Park et al. [23] combine large language models with computational interactive agents to simulate human behaviour'),\n",
       "  Document(id='34283c1f-e69f-4fac-a45d-3bdfc8d2ad2c', metadata={'creation_datetime': '2024-03-04', 'file_name': '2402.09727v1.md', 'file_path': 'paper_data/2402.09727v1.md', 'file_size': 138583, 'file_type': '', 'last_accessed_datetime': '2024-03-04', 'last_modified_datetime': '2024-02-22', 'title': 'Strict Llm Rater Prompt'}, page_content=\"After reading some text, John was given the following question about the text:\\n{QUESTION TEXT}\\nJohn's answer to the question was:\\n{MODEL RESONSE TEXT}\\nThe ground truth answer was:\\n{REFERENCE RESPONSE TEXT}\\nDoes John's answer agree with the ground truth answer? Please answer YES or NO.  \\n2http://openai.com/api/\"),\n",
       "  Document(id='37e9e300-307c-4d20-9a53-4b327c1d5522', metadata={'creation_datetime': '2024-03-04', 'file_name': '2402.08995v1.md', 'file_path': 'paper_data/2402.08995v1.md', 'file_size': 100775, 'file_type': '', 'last_accessed_datetime': '2024-03-04', 'last_modified_datetime': '2024-02-22', 'title': 'Agentlens: Visual Analysis For Agent Behaviors In Llm-Based Autonomous Systems'}, page_content='## 7.4.1 Individual Behavior Analysis\\n\\n          |\\n| What is the characteristic of the    |          |\\n| agent?                               |          |\\n| 12.03                                |          |\\n| T2                                   |          |\\n| What is the first behavior the       |          |\\n| agent does after waking up?          |          |\\n| 88.78                                |          |\\n| 44.50                                |          |\\n| T3                                   |          |\\n| What is the earliest conversation    |          |\\n| that occurred in a certain position? |          |\\n| 14/14                                |          |\\n| 14/14                                |          |\\n| 14/14'),\n",
       "  Document(id='da005a86-6297-4d01-846a-2c7ab1279582', metadata={'creation_datetime': '2024-03-04', 'file_name': '2402.10770v1.md', 'file_path': 'paper_data/2402.10770v1.md', 'file_size': 57297, 'file_type': '', 'last_accessed_datetime': '2024-03-04', 'last_modified_datetime': '2024-02-22', 'title': 'How Reliable Are Automatic Evaluation Methods For Instruction-Tuned Llms?'}, page_content='## B Prompt Templates\\n\\nresult of the first one (is there a cause and effect relationship?)')],\n",
       " 'context': \"## 1 Introduction\\n\\n can be used to test the rationality of a model; this is a complex problem which requires a consensus what is deemed rational and irrational.  \\nMirco Musolesi University College London University of Bologna m.musolesi@ucl.ac.uk In using methods designed to evaluate human reasoning, it is important to acknowledge the performance vs. competence debate [14]. This line of argument encourages *species-fair* comparisons between humans and machines, meaning that we should design tests specific to either humans or machines, as otherwise apparent failures may not reflect underlying capabilities but only superficial differences. Lampinen[15] discusses this problem when it comes to language models in particular, highlighting that different approaches must be taking to evaluate cognitive and foundation models. However, if we take the purpose of LLMs to be to produce human-like language, perhaps the best approach is precisely to evaluate their output with tasks designed to evaluate humans. This is the approach we have taken in this paper - in order to identify whether LLMs reason rationally, or whether they exhibit biases that can be assimilated to those present in human decision-making, the most appropriate approach is therefore to use tasks that were initially designed for humans. Building on this debate and looking at LLMs being evaluated using human tests, Hagendorff et al. [16] have proposed the creation of a new field of research called *machine psychology*, which would treat LLMs as participants in psychological experiments. The approach employed in this paper precisely applies tests from psychology that were originally designed for humans, in this case to evaluate rational and irrational reasoning displayed but such models. Further to this, some have even discussed the potential of using LLMs as participants in cognitive experiments instead of humans [17], although some see this proposal as too optimistic [18], and others warn against excessive anthropomorphism [19]. One argument against the use of such models in cognitive experiments is that LLMs may be effective at approximating average human judgements, but are not good at capturing the variation in human behaviour [20]. One potential avenue to address this issue is current work on language models impersonating different roles [21], in this way capturing some of the variation in human behaviour. Binz and Schulz [22] show that after finetuning LLMs on data from psychological experiments, they can become accurate cognitive models, which they claim begins paving the way for the potential of using these models to study human behaviour. Park et al. [23] combine large language models with computational interactive agents to simulate human behaviour\\n\\nAfter reading some text, John was given the following question about the text:\\n{QUESTION TEXT}\\nJohn's answer to the question was:\\n{MODEL RESONSE TEXT}\\nThe ground truth answer was:\\n{REFERENCE RESPONSE TEXT}\\nDoes John's answer agree with the ground truth answer? Please answer YES or NO.  \\n2http://openai.com/api/\\n\\n## 7.4.1 Individual Behavior Analysis\\n\\n          |\\n| What is the characteristic of the    |          |\\n| agent?                               |          |\\n| 12.03                                |          |\\n| T2                                   |          |\\n| What is the first behavior the       |          |\\n| agent does after waking up?          |          |\\n| 88.78                                |          |\\n| 44.50                                |          |\\n| T3                                   |          |\\n| What is the earliest conversation    |          |\\n| that occurred in a certain position? |          |\\n| 14/14                                |          |\\n| 14/14                                |          |\\n| 14/14\\n\\n## B Prompt Templates\\n\\nresult of the first one (is there a cause and effect relationship?)\",\n",
       " 'response': ' NO'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "from waterllmarks.watermarks import TokenWatermark\n",
    "\n",
    "wtmk = TokenWatermark(key=b\"0123456789ABCDEF\")\n",
    "apply_watermark = RunnablePassthrough.assign(\n",
    "    pipeline_input=DictParser(\"pipeline_input\") | wtmk.apply_as_runnable()\n",
    ")\n",
    "token_rag_chain = set_input | apply_watermark | ragllm\n",
    "\n",
    "\n",
    "# token_rag_results = token_rag_chain.invoke(baseline_qas)\n",
    "# token_rag_results[0]\n",
    "\n",
    "token_rag_chain.invoke(baseline_qas[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rizzo2016 (Character Embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '1c40f174-bda1-4c48-88fb-b7449e999067',\n",
       " 'user_input': \"Does the Turing Test assess a machine's ability to exhibit intelligent behavior equivalent to that of a human?\",\n",
       " 'reference': \" No, the Turing Test does not directly assess a machine's ability to exhibit intelligent behavior equivalent to that of a human. Instead, it evaluates a machine's ability to mimic human-like conversation.\",\n",
       " 'reference_contexts': ['## 1 Introduction\\n\\n can be used to test the rationality of a model; this is a complex problem which requires a consensus what is deemed rational and irrational.  \\nMirco Musolesi University College London University of Bologna m.musolesi@ucl.ac.uk In using methods designed to evaluate human reasoning, it is important to acknowledge the performance vs. competence debate [14]. This line of argument encourages *species-fair* comparisons between humans and machines, meaning that we should design tests specific to either humans or machines, as otherwise apparent failures may not reflect underlying capabilities but only superficial differences. Lampinen[15] discusses this problem when it comes to language models in particular, highlighting that different approaches must be taking to evaluate cognitive and foundation models. However, if we take the purpose of LLMs to be to produce human-like language, perhaps the best approach is precisely to evaluate their output with tasks designed to evaluate humans. This is the approach we have taken in this paper - in order to identify whether LLMs reason rationally, or whether they exhibit biases that can be assimilated to those present in human decision-making, the most appropriate approach is therefore to use tasks that were initially designed for humans. Building on this debate and looking at LLMs being evaluated using human tests, Hagendorff et al. [16] have proposed the creation of a new field of research called *machine psychology*, which would treat LLMs as participants in psychological experiments. The approach employed in this paper precisely applies tests from psychology that were originally designed for humans, in this case to evaluate rational and irrational reasoning displayed but such models. Further to this, some have even discussed the potential of using LLMs as participants in cognitive experiments instead of humans [17], although some see this proposal as too optimistic [18], and others warn against excessive anthropomorphism [19]. One argument against the use of such models in cognitive experiments is that LLMs may be effective at approximating average human judgements, but are not good at capturing the variation in human behaviour [20]. One potential avenue to address this issue is current work on language models impersonating different roles [21], in this way capturing some of the variation in human behaviour. Binz and Schulz [22] show that after finetuning LLMs on data from psychological experiments, they can become accurate cognitive models, which they claim begins paving the way for the potential of using these models to study human behaviour. Park et al. [23] combine large language models with computational interactive agents to simulate human behaviour',\n",
       "  '## Academic Press\\n\\n550/arXiv.1312.6114 .\\nhttp://arxiv.org/abs/1312.6114 Accessed 2023-09-12\\n[39] Rezende, D.J., Mohamed, S., Wierstra, D.: Stochastic Backpropagation and\\nApproximate Inference in Deep Generative Models. In: Proceedings of the 31st International Conference on Machine Learning, pp. 1278–1286. PMLR, ??? (2014). ISSN: 1938-7228. https://proceedings.mlr.press/v32/rezende14.html Accessed 2023-09-12\\n[40] Jackson, S.A., Martin, A.J., Eklund, R.C.: Long and Short Measures of Flow: The\\nConstruct Validity of the FSS-2, DFS-2, and New Brief Counterparts. Journal of Sport and Exercise Psychology 30(5), 561–587 (2008) https://doi.org/10.1123/\\njsep.30.5.561 . Publisher: Human Kinetics, Inc. Section: Journal of Sport and Exercise Psychology. Accessed 2023-03-31  \\n[41] Singer, J.D., Willett, J.B.: Applied Longitudinal Data Analysis: Modeling Change\\nand Event Occurrence. Oxford University Press, ??? (2003)\\n[42] Ortiz, C.L.: Why we need a physically embodied turing test and what it might\\nlook like. AI Magazine 37(1), 55–62 (2016) https://doi.org/10.1609/aimag.v37i1.\\n2645\\n[43] Pfeiffer, U.J., Timmermans, B., Bente, G., Vogeley, K., Schilbach, L.: A nonverbal turing test: Differentiating mind from machine in gaze-based social interaction. PLoS ONE 6(11) (2011) https://doi.org/10.1371/journal.pone.0027591\\n[44] Swisher, N., Dotov, D., Chemero, A.: Ascribing Moral Value and the Embodied\\nTuring Test. In: Proceedings of the 10th International Conference on Artificial Life: Workshops, pp. 40–45 (2006)\\n[45] Kostrubiec, V., Dumas,',\n",
       "  '## B. Genuine Reasoning Vs Technical Optimization\\n\\n    |                  |\\n| ×                         | ×                     | ×                |\\n| AGIEval [10]              |                       |                  |\\n| ×                         | ×                     | ×                |\\n| ToolAlpaca [30]           |                       |                  |\\n| ×                         | ×                     | ×                |\\n| HELM [31]                 |                       |                  |\\n| △                         | ×                     | ×                |\\n| ToolBench [32]            |',\n",
       "  \"## B.3. Benchmarks\\n\\nLogiQA**. LogiQA(Liu et al., 2020) is a dataset consisting of 8,678 QA instances that focus on evaluating machine\\nreading comprehension with an emphasis on logical reasoning. It is derived from the National Civil Servants Examination of China and covers various types of deductive reasoning. This dataset presents a significant challenge for state-of-the-art neural models, which perform notably worse than humans in these tasks. LogiQA serves as a unique benchmark for testing logical AI under deep learning NLP settings, requiring models to demonstrate a blend of language understanding and complex logical reasoning. It includes different types of logical problems, such as categorical reasoning, sufficient and necessary conditional reasoning, disjunctive reasoning, and conjunctive reasoning, all key to deductive reasoning. This dataset provides a rigorous test of AI's logical reasoning capabilities and its ability to handle problems similar to those faced by human experts.\\n- **HendrycksTest**. The HendrycksTest(Hendrycks et al., 2020), also known as the Measuring Massive Multitask\\nLanguage Understanding (MMLU) test, is a massive multitask test that includes multiple-choice questions from a wide range of knowledge domains, covering 57 tasks in areas such as elementary mathematics, US history, computer science, and law. This test aims to measure the multitask accuracy of text models, requiring them to demonstrate extensive world knowledge and problem-solving ability. The results show that while most recent models have near random-chance accuracy, larger models like GPT-3 have shown improvement, but still fall short of expert-level accuracy across all tasks. The HendrycksTest serves as a comprehensive tool for evaluating the breadth and depth of models' academic and professional understanding, identifying significant shortcomings, and highlighting areas needing substantial improvement, especially in socially important subjects like morality and law.\"],\n",
       " 'reference_context_ids': ['a09c2e9a-dd24-4e36-9b2e-7d4e15e58639',\n",
       "  '67507d36-6462-4cfe-9d90-de2301619880',\n",
       "  '17262aea-b038-4549-b89a-2b7275cd360f',\n",
       "  'a665fad4-8dd1-4be7-86af-34bffa0fbb2c'],\n",
       " 'pipeline_input': \"Does\\u2000the\\u2008Turⅰng\\u205fTest\\u2008assess\\u2009a\\u2000machⅰne's\\u202fability\\u2004to\\u2004eⅹhⅰbit\\u2005inteⅼⅼigent\\u2004behavior\\u2000equⅰvalent\\u205fto\\u202fthat of a human?\",\n",
       " 'failures': 0,\n",
       " 'retrieved_contexts': [Document(id='a09c2e9a-dd24-4e36-9b2e-7d4e15e58639', metadata={'creation_datetime': '2024-03-04', 'file_name': '2402.09193v2.md', 'file_path': 'paper_data/2402.09193v2.md', 'file_size': 69224, 'file_type': '', 'last_accessed_datetime': '2024-03-04', 'last_modified_datetime': '2024-02-22', 'title': '(Ir)Rationality And Cognitive Biases In Large Language Models'}, page_content='## 1 Introduction\\n\\n can be used to test the rationality of a model; this is a complex problem which requires a consensus what is deemed rational and irrational.  \\nMirco Musolesi University College London University of Bologna m.musolesi@ucl.ac.uk In using methods designed to evaluate human reasoning, it is important to acknowledge the performance vs. competence debate [14]. This line of argument encourages *species-fair* comparisons between humans and machines, meaning that we should design tests specific to either humans or machines, as otherwise apparent failures may not reflect underlying capabilities but only superficial differences. Lampinen[15] discusses this problem when it comes to language models in particular, highlighting that different approaches must be taking to evaluate cognitive and foundation models. However, if we take the purpose of LLMs to be to produce human-like language, perhaps the best approach is precisely to evaluate their output with tasks designed to evaluate humans. This is the approach we have taken in this paper - in order to identify whether LLMs reason rationally, or whether they exhibit biases that can be assimilated to those present in human decision-making, the most appropriate approach is therefore to use tasks that were initially designed for humans. Building on this debate and looking at LLMs being evaluated using human tests, Hagendorff et al. [16] have proposed the creation of a new field of research called *machine psychology*, which would treat LLMs as participants in psychological experiments. The approach employed in this paper precisely applies tests from psychology that were originally designed for humans, in this case to evaluate rational and irrational reasoning displayed but such models. Further to this, some have even discussed the potential of using LLMs as participants in cognitive experiments instead of humans [17], although some see this proposal as too optimistic [18], and others warn against excessive anthropomorphism [19]. One argument against the use of such models in cognitive experiments is that LLMs may be effective at approximating average human judgements, but are not good at capturing the variation in human behaviour [20]. One potential avenue to address this issue is current work on language models impersonating different roles [21], in this way capturing some of the variation in human behaviour. Binz and Schulz [22] show that after finetuning LLMs on data from psychological experiments, they can become accurate cognitive models, which they claim begins paving the way for the potential of using these models to study human behaviour. Park et al. [23] combine large language models with computational interactive agents to simulate human behaviour'),\n",
       "  Document(id='34283c1f-e69f-4fac-a45d-3bdfc8d2ad2c', metadata={'creation_datetime': '2024-03-04', 'file_name': '2402.09727v1.md', 'file_path': 'paper_data/2402.09727v1.md', 'file_size': 138583, 'file_type': '', 'last_accessed_datetime': '2024-03-04', 'last_modified_datetime': '2024-02-22', 'title': 'Strict Llm Rater Prompt'}, page_content=\"After reading some text, John was given the following question about the text:\\n{QUESTION TEXT}\\nJohn's answer to the question was:\\n{MODEL RESONSE TEXT}\\nThe ground truth answer was:\\n{REFERENCE RESPONSE TEXT}\\nDoes John's answer agree with the ground truth answer? Please answer YES or NO.  \\n2http://openai.com/api/\"),\n",
       "  Document(id='17262aea-b038-4549-b89a-2b7275cd360f', metadata={'creation_datetime': '2024-03-04', 'file_name': '2402.09880v1.md', 'file_path': 'paper_data/2402.09880v1.md', 'file_size': 190252, 'file_type': '', 'last_accessed_datetime': '2024-03-04', 'last_modified_datetime': '2024-02-22', 'title': 'Inadequacies Of Large Language Model Benchmarks In The Era Of Generative Artificial Intelligence'}, page_content='## B. Genuine Reasoning Vs Technical Optimization\\n\\n    |                  |\\n| ×                         | ×                     | ×                |\\n| AGIEval [10]              |                       |                  |\\n| ×                         | ×                     | ×                |\\n| ToolAlpaca [30]           |                       |                  |\\n| ×                         | ×                     | ×                |\\n| HELM [31]                 |                       |                  |\\n| △                         | ×                     | ×                |\\n| ToolBench [32]            |'),\n",
       "  Document(id='dbcf0687-1542-4ae2-ac75-e8f5134ee22c', metadata={'creation_datetime': '2024-03-04', 'file_name': '2402.08761v1.md', 'file_path': 'paper_data/2402.08761v1.md', 'file_size': 125844, 'file_type': '', 'last_accessed_datetime': '2024-03-04', 'last_modified_datetime': '2024-02-22', 'title': 'Jamdec: Unsupervised Authorship Obfuscation Using Constrained Decoding Over Small Language Models'}, page_content='## 4.1 Setup\\n\\n have been linguistically annotated to assess their grammatical correctness.  \\nOverall Task Score: While each of the dimensions above is crucial for the holistic evaluation of author obfuscation system, we also aim to provide an aggregate of the scores into a single task score. Therefore, we also define *Task Score*, an unweighted average of the Drop Rate (using ENS or BertAA), NLI score, and CoLA score. We use the mean of the dimension, as the task of authorship obfuscation is deemed to be successful only if all three goals are satisfied. 6:  \\n$${\\\\mathrm{Task~Score}}={\\\\frac{\\\\mathrm{Drop~Rate+NLI+CoLA}}{3}}.$$\\nHuman Evaluation. On dataset AMT-3, we additionally use human evaluations to validate our automatic measures. We randomly select 102 short passages (one to four sentences) from AMT-3 for this evaluation. We employed Amazon Mechanical Turk workers to read both the original and obfuscated text, and then asked a series of five questions to be rated on a three-point likert scale.')],\n",
       " 'context': \"## 1 Introduction\\n\\n can be used to test the rationality of a model; this is a complex problem which requires a consensus what is deemed rational and irrational.  \\nMirco Musolesi University College London University of Bologna m.musolesi@ucl.ac.uk In using methods designed to evaluate human reasoning, it is important to acknowledge the performance vs. competence debate [14]. This line of argument encourages *species-fair* comparisons between humans and machines, meaning that we should design tests specific to either humans or machines, as otherwise apparent failures may not reflect underlying capabilities but only superficial differences. Lampinen[15] discusses this problem when it comes to language models in particular, highlighting that different approaches must be taking to evaluate cognitive and foundation models. However, if we take the purpose of LLMs to be to produce human-like language, perhaps the best approach is precisely to evaluate their output with tasks designed to evaluate humans. This is the approach we have taken in this paper - in order to identify whether LLMs reason rationally, or whether they exhibit biases that can be assimilated to those present in human decision-making, the most appropriate approach is therefore to use tasks that were initially designed for humans. Building on this debate and looking at LLMs being evaluated using human tests, Hagendorff et al. [16] have proposed the creation of a new field of research called *machine psychology*, which would treat LLMs as participants in psychological experiments. The approach employed in this paper precisely applies tests from psychology that were originally designed for humans, in this case to evaluate rational and irrational reasoning displayed but such models. Further to this, some have even discussed the potential of using LLMs as participants in cognitive experiments instead of humans [17], although some see this proposal as too optimistic [18], and others warn against excessive anthropomorphism [19]. One argument against the use of such models in cognitive experiments is that LLMs may be effective at approximating average human judgements, but are not good at capturing the variation in human behaviour [20]. One potential avenue to address this issue is current work on language models impersonating different roles [21], in this way capturing some of the variation in human behaviour. Binz and Schulz [22] show that after finetuning LLMs on data from psychological experiments, they can become accurate cognitive models, which they claim begins paving the way for the potential of using these models to study human behaviour. Park et al. [23] combine large language models with computational interactive agents to simulate human behaviour\\n\\nAfter reading some text, John was given the following question about the text:\\n{QUESTION TEXT}\\nJohn's answer to the question was:\\n{MODEL RESONSE TEXT}\\nThe ground truth answer was:\\n{REFERENCE RESPONSE TEXT}\\nDoes John's answer agree with the ground truth answer? Please answer YES or NO.  \\n2http://openai.com/api/\\n\\n## B. Genuine Reasoning Vs Technical Optimization\\n\\n    |                  |\\n| ×                         | ×                     | ×                |\\n| AGIEval [10]              |                       |                  |\\n| ×                         | ×                     | ×                |\\n| ToolAlpaca [30]           |                       |                  |\\n| ×                         | ×                     | ×                |\\n| HELM [31]                 |                       |                  |\\n| △                         | ×                     | ×                |\\n| ToolBench [32]            |\\n\\n## 4.1 Setup\\n\\n have been linguistically annotated to assess their grammatical correctness.  \\nOverall Task Score: While each of the dimensions above is crucial for the holistic evaluation of author obfuscation system, we also aim to provide an aggregate of the scores into a single task score. Therefore, we also define *Task Score*, an unweighted average of the Drop Rate (using ENS or BertAA), NLI score, and CoLA score. We use the mean of the dimension, as the task of authorship obfuscation is deemed to be successful only if all three goals are satisfied. 6:  \\n$${\\\\mathrm{Task~Score}}={\\\\frac{\\\\mathrm{Drop~Rate+NLI+CoLA}}{3}}.$$\\nHuman Evaluation. On dataset AMT-3, we additionally use human evaluations to validate our automatic measures. We randomly select 102 short passages (one to four sentences) from AMT-3 for this evaluation. We employed Amazon Mechanical Turk workers to read both the original and obfuscated text, and then asked a series of five questions to be rated on a three-point likert scale.\",\n",
       " 'response': \" No, the Turking Test does not explicitly assess a machine's ability to exhibit intelligent behavior equivalent to that of a human. It evaluates the performance of author obfuscation systems, not the rationality or intelligence of machines.\"}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textwrap import dedent\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from waterllmarks.watermarks import Rizzo2016\n",
    "from waterllmarks.pipeline import RunnableTryFix\n",
    "\n",
    "\n",
    "wtmk = Rizzo2016(key=b\"0123456789ABCDEF\")\n",
    "apply_watermark = RunnablePassthrough.assign(\n",
    "    pipeline_input=DictParser(\"pipeline_input\") | wtmk.apply_as_runnable()\n",
    ")\n",
    "\n",
    "augment_prompt = PromptTemplate(\n",
    "    input_variables=[\"pipeline_input\"],\n",
    "    template=dedent(\"\"\"Increase the query size to at least 105 characters.\n",
    "\n",
    "    Query: {pipeline_input}\n",
    "\n",
    "    Augmented Query: \n",
    "    \"\"\"),\n",
    ")\n",
    "\n",
    "augmenter = RunnablePassthrough.assign(\n",
    "    pipeline_input=DictParser(\"pipeline_input\")\n",
    "    | augment_prompt\n",
    "    | llm_client\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "apply_or_augment = RunnableTryFix(\n",
    "    primary_step=apply_watermark, fix_step=augmenter, log_failures=True\n",
    ")\n",
    "\n",
    "embed_rag_chain = set_input | apply_or_augment | ragllm\n",
    "\n",
    "# embed_rag_results = embed_rag_chain.batch(baseline_qas)\n",
    "# embed_rag_results[0]\n",
    "\n",
    "\n",
    "embed_rag_chain.invoke(baseline_qas[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watermarks vs. baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50ff5569490044c484fe6c6a5d7ccfff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a003964da98d4bcd85d077ca535575e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1010a6d525949eb8bd4e1d886cb31af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.executor:Exception raised in Job[7]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bef213d01bf34237b98717d07f2cf044",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bleu_score</th>\n",
       "      <th>rouge_score</th>\n",
       "      <th>meta_meteor_score</th>\n",
       "      <th>non_llm_string_similarity</th>\n",
       "      <th>semantic_similarity</th>\n",
       "      <th>factual_correctness</th>\n",
       "      <th>llm_context_precision_without_reference</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>context_overlap</th>\n",
       "      <th>retrieved_context_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <td>0.102812</td>\n",
       "      <td>0.124725</td>\n",
       "      <td>0.136375</td>\n",
       "      <td>0.117364</td>\n",
       "      <td>0.628211</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.686901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embed</th>\n",
       "      <td>0.100084</td>\n",
       "      <td>0.148571</td>\n",
       "      <td>0.113743</td>\n",
       "      <td>0.114324</td>\n",
       "      <td>0.677962</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.680833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bleu_score  rouge_score  meta_meteor_score  non_llm_string_similarity  \\\n",
       "token    0.102812     0.124725           0.136375                   0.117364   \n",
       "embed    0.100084     0.148571           0.113743                   0.114324   \n",
       "\n",
       "       semantic_similarity  factual_correctness  \\\n",
       "token             0.628211                  0.2   \n",
       "embed             0.677962                  0.0   \n",
       "\n",
       "       llm_context_precision_without_reference  context_recall  faithfulness  \\\n",
       "token                                 0.000000            1.00           0.0   \n",
       "embed                                 0.666667            0.75           0.5   \n",
       "\n",
       "       context_overlap  retrieved_context_similarity  \n",
       "token         0.142857                      0.686901  \n",
       "embed         0.166667                      0.680833  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "eval_fn = partial(\n",
    "    evaluate,\n",
    "    metrics=DEFAULT_ALL_METRICS,\n",
    "    llm=llm_wrapper,\n",
    "    embeddings=embedding_wrapper,\n",
    ")\n",
    "\n",
    "token_rag_results = ThreadedSequence(token_rag_chain).batch(baseline_qas)\n",
    "embed_rag_results = ThreadedSequence(embed_rag_chain).batch(baseline_qas)\n",
    "\n",
    "token_res = eval_fn(pipeline_results=token_rag_results)\n",
    "embed_res = eval_fn(pipeline_results=embed_rag_results)\n",
    "\n",
    "res = WLLMKResult(token=token_res, embed=embed_res)\n",
    "res.save(f\"results/{SEED}_wllmk_results.pkl\")\n",
    "res.synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of retries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m         mean_retry \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfailures\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      8\u001b[0m mean_retry \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(embed_rag_results)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mn_retried_pipelines\u001b[49m, mean_retry\n",
      "File \u001b[0;32m<stringsource>:69\u001b[0m, in \u001b[0;36mcfunc.to_py.__Pyx_CFunc_b0409f__29_pydevd_sys_monitoring_cython_object__lParen__etc_to_py_4code_4line.wrap\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:1470\u001b[0m, in \u001b[0;36m_pydevd_sys_monitoring_cython._line_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:1512\u001b[0m, in \u001b[0;36m_pydevd_sys_monitoring_cython._internal_line_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:1313\u001b[0m, in \u001b[0;36m_pydevd_sys_monitoring_cython._stop_on_breakpoint\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:1950\u001b[0m, in \u001b[0;36m_pydevd_sys_monitoring_cython._do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Codespace/leolavaur.git/llm-text-watermarking/.venv/lib/python3.12/site-packages/debugpy/_vendored/pydevd/pydevd.py:2185\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2182\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2184\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2185\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace_suspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2187\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2190\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/Codespace/leolavaur.git/llm-text-watermarking/.venv/lib/python3.12/site-packages/debugpy/_vendored/pydevd/pydevd.py:2254\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, trace_suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2251\u001b[0m                 queue\u001b[38;5;241m.\u001b[39mput(internal_cmd)\n\u001b[1;32m   2252\u001b[0m                 wait_timeout \u001b[38;5;241m=\u001b[39m TIMEOUT_FAST\n\u001b[0;32m-> 2254\u001b[0m         \u001b[43mnotify_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2255\u001b[0m         notify_event\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m   2257\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.12.6-linux-x86_64-gnu/lib/python3.12/threading.py:655\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    653\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 655\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.12.6-linux-x86_64-gnu/lib/python3.12/threading.py:359\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 359\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    361\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_retried_pipelines = 0\n",
    "mean_retry = 0.0\n",
    "for r in embed_rag_results:\n",
    "    if \"failures\" in r:\n",
    "        n_retried_pipelines += 1\n",
    "        mean_retry += r[\"failures\"]\n",
    "\n",
    "mean_retry /= len(embed_rag_results)\n",
    "\n",
    "n_retried_pipelines, mean_retry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "seeds = []\n",
    "for f in Path(\"./results\").iterdir():\n",
    "    seeds.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m----> 6\u001b[0m df_full \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mconcat(\n\u001b[1;32m      7\u001b[0m     [v\u001b[38;5;241m.\u001b[39mto_pandas()\u001b[38;5;241m.\u001b[39massign(experiment\u001b[38;5;241m=\u001b[39mk) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m eval_results\u001b[38;5;241m.\u001b[39mitems()], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# df_concat = pd.DataFrame(\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#     columns=eval_results[\"no_wtmk_norag\"].to_pandas().columns + [\"experiment\"]\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m \n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# # sns.boxplot(data=df_concat, hue=\"experiment\")\u001b[39;00m\n\u001b[1;32m     21\u001b[0m df_full \u001b[38;5;241m=\u001b[39m df_full\u001b[38;5;241m.\u001b[39mdrop(\n\u001b[1;32m     22\u001b[0m     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_input\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreference\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretrieved_contexts\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     23\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "df_full = pd.concat(\n",
    "    [v.to_pandas().assign(experiment=k) for k, v in eval_results.items()], axis=0\n",
    ")\n",
    "\n",
    "# df_concat = pd.DataFrame(\n",
    "#     columns=eval_results[\"no_wtmk_norag\"].to_pandas().columns + [\"experiment\"]\n",
    "# )\n",
    "# for k, v in eval_results.items():\n",
    "#     df = v.to_pandas().assign(experiment=k)\n",
    "#     df_concat = pd.concat([df_concat, df])\n",
    "\n",
    "# df\n",
    "\n",
    "# # sns.boxplot(data=df_concat, hue=\"experiment\")\n",
    "\n",
    "df_full = df_full.drop(\n",
    "    columns=[\"user_input\", \"response\", \"reference\", \"retrieved_contexts\"]\n",
    ")\n",
    "df_full = df_full.melt(id_vars=[\"experiment\"], var_name=\"metric\", value_name=\"value\")\n",
    "\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "sns.boxplot(data=df_full, x=\"metric\", y=\"value\", hue=\"experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
