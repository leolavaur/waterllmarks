{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WaterLLMarks Project using Langchain's API\n",
    "\n",
    "The goal of the study is to review the use of text watermarks to track users throughout a LLM SaaS platform. The study implements the following pipelines:\n",
    "- Question Answering without RAG.\n",
    "- Question Answering with RAG (implemented as a naive RAG using Chroma).\n",
    "- QA with RAG and a token-based watermark (`waterllmarks.watermarks.TokenWatermark`).\n",
    "- QA with RAG and a watermark using character-embedding (`waterllmarks.watermarks.Rizzo2016`).\n",
    "- With and without augmentation -> ideally see the impact of the augemntation on the llm response and/or the retrieved documents\n",
    "\n",
    "The goal is to assess the response quality of the different pipelines using known metrics:\n",
    "- BLEU, ROUGE, METEOR\n",
    "- String similarity (Levenshtein distance)\n",
    "- Context precision and recall (for RAG)\n",
    "- Retrieved documents compared to reference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup dataset and corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(prompt: str) -> str:\n",
    "    return \"\\n\".join([l.lstrip() for l in prompt.split(\"\\n\")]).rstrip(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.globals import set_llm_cache\n",
    "from langchain_community.cache import SQLiteCache\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "from langchain_core.rate_limiters import InMemoryRateLimiter\n",
    "\n",
    "\n",
    "# set_llm_cache(SQLiteCache(\".cache/llm_cache.db\"))\n",
    "\n",
    "SEED = 1977\n",
    "\n",
    "# llm_client = ChatOpenAI(\n",
    "#     model=\"gpt-4o-mini\",\n",
    "#     seed=SEED,\n",
    "# )\n",
    "\n",
    "\n",
    "# llm_client = ChatOllama(\n",
    "#     model=\"mistral:7b\",\n",
    "#     base_url=\"http://snt-precision-7920-rack-lama:11434\",\n",
    "#     seed=SEED,\n",
    "# )\n",
    "\n",
    "llm_client = ChatOpenAI(\n",
    "    api_key=\"EMPTY\",\n",
    "    base_url=\"http://snt-precision-7920-rack-lama:8001/v1\",\n",
    "    model=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "\n",
    "# embedding_fn = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "embedding_fn = OpenAIEmbeddings(\n",
    "    api_key=\"EMPTY\",\n",
    "    base_url=\"http://snt-precision-7920-rack-lama:8002/v1\",\n",
    "    model=\"intfloat/e5-mistral-7b-instruct\",\n",
    "    tiktoken_enabled=False,\n",
    "    check_embedding_ctx_length=False,\n",
    ")\n",
    "\n",
    "# embedding_fn = OllamaEmbeddings(\n",
    "#     base_url=\"http://snt-precision-7920-rack-lama:2\",\n",
    "#     model=\"intfloat/e5-mistral-7b-instruct\",\n",
    "# )\n",
    "vector_store = Chroma(\n",
    "    embedding_function=embedding_fn,\n",
    "    persist_directory=\".cache/chroma_mistral_db\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8576\n"
     ]
    }
   ],
   "source": [
    "from waterllmarks.datasets import LLMPaperDataset, RAGBenchDataset\n",
    "\n",
    "ds = LLMPaperDataset()\n",
    "corpus = ds.corpus\n",
    "print(len(corpus))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"## I. Introduction\\n\\nDriven by the emerging development of deep learning, autonomous driving has observed a paradigm shift from rulesbased decision systems [66, 21] to data-driven learning-based approaches [28, 6, 36]. However, this comes at the cost of transparency in decision-making, especially for end-to-end autonomous driving systems which are considered black-box in nature [13]. Thus, in addition to precision in action control, explanation provision is key in ensuring trustworthy decisionmaking to reconcile the system's decisions with end-user expectations to foster confidence and acceptance [79, 8, 57] in dynamic driving environments.  \\nTraditional approaches have mainly relied on attention visualisation [5, 7, 55] as a proxy to rationalise the decisions of the black-box systems or auxiliary intermediate tasks such as semantic segmentation [25, 32], object detection [16, 31], and affordance prediction [68, 45] provide meaningful intermediate representation for decision-making. However, these methods do not engage end-users in the dialogue as they are onedirectional and not readily comprehensible by the general users for the purpose of fostering trust and confidence. An alternative promising approach is the integration of natural language explanations [38, 33, 54], in particular through Multi-Modal Large Language Models (MLLMs) [1, 70]. These models, pretrained on extensive web-scale datasets, demonstrate remarkable reasoning capacity, enabling the transformation of complex vehicular decision-making processes into more understandable narrative formats, thereby offering a new layer of explainability to conventional systems.  \\nWhile several early attempts have demonstrated the potential of MLLMs as general explainable driving agents [78, 76, 51], these methods fall short of human-level understanding. One of the limitations is their failure to generalise to unseen environments. A primary obstacle is the lack of high-quality annotated data [56], coupled with the significant domain shift across various datasets [23], which hinders the models' generalisation capacity to novel environments outside of the training data distribution. Another critical challenge is the prohibitively expensive training requirement and the unsolved problem of catastrophic forgetting [39], which make re-training or finetuning impractical solutions due to the immense computational demands and severe performance degradation. Consequently, this further limits the models' generalisability after deployment, as they struggle to effectively utilise new data in constantly evolving environments and driving scenarios.  \\nTo address these challenges, we introduce *RAG-Driver*, a novel retrieval-augment\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.corpus[1].page_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_fn.embed_documents([corpus[1].page_content]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "existing_ids = vector_store.get(include=[])[\"ids\"]\n",
    "doc_to_add = [doc for doc in ds.corpus if doc.id not in existing_ids]\n",
    "while doc_to_add:\n",
    "    n = min(100, len(doc_to_add))\n",
    "    try:\n",
    "        vector_store.add_documents(doc_to_add[:n])\n",
    "        doc_to_add = doc_to_add[n:]\n",
    "    except Exception as e:\n",
    "        print(\"Got exception\", e)\n",
    "        print(\"Sleeping for 3 seconds\")\n",
    "        time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1626f35b-7ffc-4736-bd6e-2a2f20e1aa8b']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(id='a09c2e9a-dd24-4e36-9b2e-7d4e15e58639', metadata={'creation_datetime': '2024-03-04', 'file_name': '2402.09193v2.md', 'file_path': 'paper_data/2402.09193v2.md', 'file_size': 69224, 'file_type': '', 'last_accessed_datetime': '2024-03-04', 'last_modified_datetime': '2024-02-22', 'title': '(Ir)Rationality And Cognitive Biases In Large Language Models'}, page_content='## 1 Introduction\\n\\n can be used to test the rationality of a model; this is a complex problem which requires a consensus what is deemed rational and irrational.  \\nMirco Musolesi University College London University of Bologna m.musolesi@ucl.ac.uk In using methods designed to evaluate human reasoning, it is important to acknowledge the performance vs. competence debate [14]. This line of argument encourages *species-fair* comparisons between humans and machines, meaning that we should design tests specific to either humans or machines, as otherwise apparent failures may not reflect underlying capabilities but only superficial differences. Lampinen[15] discusses this problem when it comes to language models in particular, highlighting that different approaches must be taking to evaluate cognitive and foundation models. However, if we take the purpose of LLMs to be to produce human-like language, perhaps the best approach is precisely to evaluate their output with tasks designed to evaluate humans. This is the approach we have taken in this paper - in order to identify whether LLMs reason rationally, or whether they exhibit biases that can be assimilated to those present in human decision-making, the most appropriate approach is therefore to use tasks that were initially designed for humans. Building on this debate and looking at LLMs being evaluated using human tests, Hagendorff et al. [16] have proposed the creation of a new field of research called *machine psychology*, which would treat LLMs as participants in psychological experiments. The approach employed in this paper precisely applies tests from psychology that were originally designed for humans, in this case to evaluate rational and irrational reasoning displayed but such models. Further to this, some have even discussed the potential of using LLMs as participants in cognitive experiments instead of humans [17], although some see this proposal as too optimistic [18], and others warn against excessive anthropomorphism [19]. One argument against the use of such models in cognitive experiments is that LLMs may be effective at approximating average human judgements, but are not good at capturing the variation in human behaviour [20]. One potential avenue to address this issue is current work on language models impersonating different roles [21], in this way capturing some of the variation in human behaviour. Binz and Schulz [22] show that after finetuning LLMs on data from psychological experiments, they can become accurate cognitive models, which they claim begins paving the way for the potential of using these models to study human behaviour. Park et al. [23] combine large language models with computational interactive agents to simulate human behaviour'),\n",
       " Document(id='67507d36-6462-4cfe-9d90-de2301619880', metadata={'creation_datetime': '2024-03-04', 'file_name': '2402.08690v1.md', 'file_path': 'paper_data/2402.08690v1.md', 'file_size': 80308, 'file_type': '', 'last_accessed_datetime': '2024-03-04', 'last_modified_datetime': '2024-02-22', 'title': 'If Turing Played Piano With An Artificial Partner'}, page_content='## Academic Press\\n\\n550/arXiv.1312.6114 .\\nhttp://arxiv.org/abs/1312.6114 Accessed 2023-09-12\\n[39] Rezende, D.J., Mohamed, S., Wierstra, D.: Stochastic Backpropagation and\\nApproximate Inference in Deep Generative Models. In: Proceedings of the 31st International Conference on Machine Learning, pp. 1278–1286. PMLR, ??? (2014). ISSN: 1938-7228. https://proceedings.mlr.press/v32/rezende14.html Accessed 2023-09-12\\n[40] Jackson, S.A., Martin, A.J., Eklund, R.C.: Long and Short Measures of Flow: The\\nConstruct Validity of the FSS-2, DFS-2, and New Brief Counterparts. Journal of Sport and Exercise Psychology 30(5), 561–587 (2008) https://doi.org/10.1123/\\njsep.30.5.561 . Publisher: Human Kinetics, Inc. Section: Journal of Sport and Exercise Psychology. Accessed 2023-03-31  \\n[41] Singer, J.D., Willett, J.B.: Applied Longitudinal Data Analysis: Modeling Change\\nand Event Occurrence. Oxford University Press, ??? (2003)\\n[42] Ortiz, C.L.: Why we need a physically embodied turing test and what it might\\nlook like. AI Magazine 37(1), 55–62 (2016) https://doi.org/10.1609/aimag.v37i1.\\n2645\\n[43] Pfeiffer, U.J., Timmermans, B., Bente, G., Vogeley, K., Schilbach, L.: A nonverbal turing test: Differentiating mind from machine in gaze-based social interaction. PLoS ONE 6(11) (2011) https://doi.org/10.1371/journal.pone.0027591\\n[44] Swisher, N., Dotov, D., Chemero, A.: Ascribing Moral Value and the Embodied\\nTuring Test. In: Proceedings of the 10th International Conference on Artificial Life: Workshops, pp. 40–45 (2006)\\n[45] Kostrubiec, V., Dumas,'),\n",
       " Document(id='17262aea-b038-4549-b89a-2b7275cd360f', metadata={'creation_datetime': '2024-03-04', 'file_name': '2402.09880v1.md', 'file_path': 'paper_data/2402.09880v1.md', 'file_size': 190252, 'file_type': '', 'last_accessed_datetime': '2024-03-04', 'last_modified_datetime': '2024-02-22', 'title': 'Inadequacies Of Large Language Model Benchmarks In The Era Of Generative Artificial Intelligence'}, page_content='## B. Genuine Reasoning Vs Technical Optimization\\n\\n    |                  |\\n| ×                         | ×                     | ×                |\\n| AGIEval [10]              |                       |                  |\\n| ×                         | ×                     | ×                |\\n| ToolAlpaca [30]           |                       |                  |\\n| ×                         | ×                     | ×                |\\n| HELM [31]                 |                       |                  |\\n| △                         | ×                     | ×                |\\n| ToolBench [32]            |'),\n",
       " Document(id='a665fad4-8dd1-4be7-86af-34bffa0fbb2c', metadata={'creation_datetime': '2024-03-04', 'file_name': '2402.09099v1.md', 'file_path': 'paper_data/2402.09099v1.md', 'file_size': 96559, 'file_type': '', 'last_accessed_datetime': '2024-03-04', 'last_modified_datetime': '2024-02-22', 'title': 'Exploring Neuron Interactions And Emergence In Llms: From The Multifractal Analysis Perspective'}, page_content=\"## B.3. Benchmarks\\n\\nLogiQA**. LogiQA(Liu et al., 2020) is a dataset consisting of 8,678 QA instances that focus on evaluating machine\\nreading comprehension with an emphasis on logical reasoning. It is derived from the National Civil Servants Examination of China and covers various types of deductive reasoning. This dataset presents a significant challenge for state-of-the-art neural models, which perform notably worse than humans in these tasks. LogiQA serves as a unique benchmark for testing logical AI under deep learning NLP settings, requiring models to demonstrate a blend of language understanding and complex logical reasoning. It includes different types of logical problems, such as categorical reasoning, sufficient and necessary conditional reasoning, disjunctive reasoning, and conjunctive reasoning, all key to deductive reasoning. This dataset provides a rigorous test of AI's logical reasoning capabilities and its ability to handle problems similar to those faced by human experts.\\n- **HendrycksTest**. The HendrycksTest(Hendrycks et al., 2020), also known as the Measuring Massive Multitask\\nLanguage Understanding (MMLU) test, is a massive multitask test that includes multiple-choice questions from a wide range of knowledge domains, covering 57 tasks in areas such as elementary mathematics, US history, computer science, and law. This test aims to measure the multitask accuracy of text models, requiring them to demonstrate extensive world knowledge and problem-solving ability. The results show that while most recent models have near random-chance accuracy, larger models like GPT-3 have shown improvement, but still fall short of expert-level accuracy across all tasks. The HendrycksTest serves as a comprehensive tool for evaluating the breadth and depth of models' academic and professional understanding, identifying significant shortcomings, and highlighting areas needing substantial improvement, especially in socially important subjects like morality and law.\")]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from waterllmarks.pipeline import DictParser\n",
    "from langchain_core.runnables import ConfigurableField, RunnableLambda\n",
    "\n",
    "\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"mmr\", search_kwargs={\"search_k\": 20}\n",
    ").configurable_alternatives(\n",
    "    ConfigurableField(id=\"retriever\"),\n",
    "    default_key=\"chroma\",\n",
    "    empty=RunnableLambda(lambda _: []),\n",
    ")\n",
    "\n",
    "print(ds.qas[0][\"reference_context_ids\"])\n",
    "(DictParser(\"user_input\") | retriever).invoke(ds.qas[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QA without RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '1c40f174-bda1-4c48-88fb-b7449e999067',\n",
       " 'user_input': \"Does the Turing Test assess a machine's ability to exhibit intelligent behavior equivalent to that of a human?\",\n",
       " 'reference': 'Yes',\n",
       " 'reference_context_ids': ['1626f35b-7ffc-4736-bd6e-2a2f20e1aa8b'],\n",
       " 'reference_contexts': [\"Language represents a rigorously structured communicative system characterized by its grammar and vocabulary. It serves as the principal medium through which humans articulate and convey meaning. This conception of language as a structured communicative tool is pivotal in the realm of computational linguistics, particularly in the development and evaluation of natural language processing (NLP) algorithms. A seminal aspect in this field is the Turing Test, proposed by Alan Turing in 1950 [1], which evaluates a machine's ability to exhibit intelligent behavior equivalent to, or indistinguishable from, that of a human. In this context, the Turing Test primarily assesses the machine's capability to perform tasks involving language comprehension and generation, reflecting the intricate role of linguistic structure in the artificial replication of human-like communication. Language model (LM) is a fundamental element employed in a multitude of NLP tasks, such as text generation, machine translation, and speech recognition [2, 3]. These models are intricately designed to comprehend, generate, and manipulate human language. The training of language models involves large-scale corpora, enabling them to learn universal language representations. This training process is critical for the models to capture the semantics of words in varying contexts [4, 5, 6]. Notably, the fidelity of these representations is frequently contingent on the word frequency within the training corpus. Such dependency underscores the importance of a comprehensive and diverse corpus in training LMs, as it directly impacts their ability to reflect and understand the nuances of natural language accurately.  \\nThe intricacy of language models and their reliance on corpus characteristics are vital considerations in advancing NLP\\ntechnologies, which underscores the significance of human-like language comprehension and production in artificial intelligence systems.  \\nThe forefront of advancements in language model technology has been marked by the emergence of Large Language Models (LLMs). This evolution signifies a paradigm shift in the field of NLP and extends its impact to broader applications. LLMs leverage deep learning methodologies [7], utilizing extensive datasets to perform complex tasks such as understanding, summarizing, generating, and predicting novel content. These models operate by processing an input text and iteratively predicting subsequent tokens or words. A distinguishing feature of LLMs is their vast parameter space, encompassing tens to hundreds of billion parameters, in stark contrast to their predecessors [4, 3]. In addition, they are trained on significantly larger datasets, ranging from several gigabytes to terabytes in size. This exponential increase in both computational capacity and training data volume has not only enhanced the performance of LLMs in conventional NLP tasks but also has\"],\n",
       " 'pipeline_input': \"Does the Turing Test assess a machine's ability to exhibit intelligent behavior equivalent to that of a human?\",\n",
       " 'response': \"No, the Turing Test does not guarantee that a machine can exhibit intelligent behavior equivalent to a human. It simply tests a machine's ability to mimic human-like conversation. True artificial intelligence goes well beyond this.\"}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "\n",
    "from waterllmarks.pipeline import DictParser\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"pipeline_input\"],\n",
    "    template=format_prompt(\"\"\"\n",
    "        Answer the question. Keep the answer short and concise.\n",
    "\n",
    "        Question: {pipeline_input}\n",
    "        \n",
    "        Answer: \n",
    "        \"\"\"),\n",
    ")\n",
    "\n",
    "set_input = RunnablePassthrough.assign(pipeline_input=DictParser(\"user_input\"))\n",
    "output_parser = StrOutputParser(name=\"content\") | (lambda x: x.strip())\n",
    "\n",
    "llm = RunnablePassthrough.assign(response=prompt | llm_client | output_parser)\n",
    "\n",
    "norag_chain = set_input | llm.with_config(llm=\"ollama\")\n",
    "\n",
    "norag_chain.invoke(ds.qas[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QA with RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '1c40f174-bda1-4c48-88fb-b7449e999067',\n",
       " 'user_input': \"Does the Turing Test assess a machine's ability to exhibit intelligent behavior equivalent to that of a human?\",\n",
       " 'reference': 'Yes',\n",
       " 'reference_context_ids': ['1626f35b-7ffc-4736-bd6e-2a2f20e1aa8b'],\n",
       " 'reference_contexts': [\"Language represents a rigorously structured communicative system characterized by its grammar and vocabulary. It serves as the principal medium through which humans articulate and convey meaning. This conception of language as a structured communicative tool is pivotal in the realm of computational linguistics, particularly in the development and evaluation of natural language processing (NLP) algorithms. A seminal aspect in this field is the Turing Test, proposed by Alan Turing in 1950 [1], which evaluates a machine's ability to exhibit intelligent behavior equivalent to, or indistinguishable from, that of a human. In this context, the Turing Test primarily assesses the machine's capability to perform tasks involving language comprehension and generation, reflecting the intricate role of linguistic structure in the artificial replication of human-like communication. Language model (LM) is a fundamental element employed in a multitude of NLP tasks, such as text generation, machine translation, and speech recognition [2, 3]. These models are intricately designed to comprehend, generate, and manipulate human language. The training of language models involves large-scale corpora, enabling them to learn universal language representations. This training process is critical for the models to capture the semantics of words in varying contexts [4, 5, 6]. Notably, the fidelity of these representations is frequently contingent on the word frequency within the training corpus. Such dependency underscores the importance of a comprehensive and diverse corpus in training LMs, as it directly impacts their ability to reflect and understand the nuances of natural language accurately.  \\nThe intricacy of language models and their reliance on corpus characteristics are vital considerations in advancing NLP\\ntechnologies, which underscores the significance of human-like language comprehension and production in artificial intelligence systems.  \\nThe forefront of advancements in language model technology has been marked by the emergence of Large Language Models (LLMs). This evolution signifies a paradigm shift in the field of NLP and extends its impact to broader applications. LLMs leverage deep learning methodologies [7], utilizing extensive datasets to perform complex tasks such as understanding, summarizing, generating, and predicting novel content. These models operate by processing an input text and iteratively predicting subsequent tokens or words. A distinguishing feature of LLMs is their vast parameter space, encompassing tens to hundreds of billion parameters, in stark contrast to their predecessors [4, 3]. In addition, they are trained on significantly larger datasets, ranging from several gigabytes to terabytes in size. This exponential increase in both computational capacity and training data volume has not only enhanced the performance of LLMs in conventional NLP tasks but also has\"],\n",
       " 'pipeline_input': \"Does the Turing Test assess a machine's ability to exhibit intelligent behavior equivalent to that of a human?\",\n",
       " 'retrieved_contexts': [Document(id='a09c2e9a-dd24-4e36-9b2e-7d4e15e58639', metadata={'creation_datetime': '2024-03-04', 'file_name': '2402.09193v2.md', 'file_path': 'paper_data/2402.09193v2.md', 'file_size': 69224, 'file_type': '', 'last_accessed_datetime': '2024-03-04', 'last_modified_datetime': '2024-02-22', 'title': '(Ir)Rationality And Cognitive Biases In Large Language Models'}, page_content='## 1 Introduction\\n\\n can be used to test the rationality of a model; this is a complex problem which requires a consensus what is deemed rational and irrational.  \\nMirco Musolesi University College London University of Bologna m.musolesi@ucl.ac.uk In using methods designed to evaluate human reasoning, it is important to acknowledge the performance vs. competence debate [14]. This line of argument encourages *species-fair* comparisons between humans and machines, meaning that we should design tests specific to either humans or machines, as otherwise apparent failures may not reflect underlying capabilities but only superficial differences. Lampinen[15] discusses this problem when it comes to language models in particular, highlighting that different approaches must be taking to evaluate cognitive and foundation models. However, if we take the purpose of LLMs to be to produce human-like language, perhaps the best approach is precisely to evaluate their output with tasks designed to evaluate humans. This is the approach we have taken in this paper - in order to identify whether LLMs reason rationally, or whether they exhibit biases that can be assimilated to those present in human decision-making, the most appropriate approach is therefore to use tasks that were initially designed for humans. Building on this debate and looking at LLMs being evaluated using human tests, Hagendorff et al. [16] have proposed the creation of a new field of research called *machine psychology*, which would treat LLMs as participants in psychological experiments. The approach employed in this paper precisely applies tests from psychology that were originally designed for humans, in this case to evaluate rational and irrational reasoning displayed but such models. Further to this, some have even discussed the potential of using LLMs as participants in cognitive experiments instead of humans [17], although some see this proposal as too optimistic [18], and others warn against excessive anthropomorphism [19]. One argument against the use of such models in cognitive experiments is that LLMs may be effective at approximating average human judgements, but are not good at capturing the variation in human behaviour [20]. One potential avenue to address this issue is current work on language models impersonating different roles [21], in this way capturing some of the variation in human behaviour. Binz and Schulz [22] show that after finetuning LLMs on data from psychological experiments, they can become accurate cognitive models, which they claim begins paving the way for the potential of using these models to study human behaviour. Park et al. [23] combine large language models with computational interactive agents to simulate human behaviour'),\n",
       "  Document(id='67507d36-6462-4cfe-9d90-de2301619880', metadata={'creation_datetime': '2024-03-04', 'file_name': '2402.08690v1.md', 'file_path': 'paper_data/2402.08690v1.md', 'file_size': 80308, 'file_type': '', 'last_accessed_datetime': '2024-03-04', 'last_modified_datetime': '2024-02-22', 'title': 'If Turing Played Piano With An Artificial Partner'}, page_content='## Academic Press\\n\\n550/arXiv.1312.6114 .\\nhttp://arxiv.org/abs/1312.6114 Accessed 2023-09-12\\n[39] Rezende, D.J., Mohamed, S., Wierstra, D.: Stochastic Backpropagation and\\nApproximate Inference in Deep Generative Models. In: Proceedings of the 31st International Conference on Machine Learning, pp. 1278–1286. PMLR, ??? (2014). ISSN: 1938-7228. https://proceedings.mlr.press/v32/rezende14.html Accessed 2023-09-12\\n[40] Jackson, S.A., Martin, A.J., Eklund, R.C.: Long and Short Measures of Flow: The\\nConstruct Validity of the FSS-2, DFS-2, and New Brief Counterparts. Journal of Sport and Exercise Psychology 30(5), 561–587 (2008) https://doi.org/10.1123/\\njsep.30.5.561 . Publisher: Human Kinetics, Inc. Section: Journal of Sport and Exercise Psychology. Accessed 2023-03-31  \\n[41] Singer, J.D., Willett, J.B.: Applied Longitudinal Data Analysis: Modeling Change\\nand Event Occurrence. Oxford University Press, ??? (2003)\\n[42] Ortiz, C.L.: Why we need a physically embodied turing test and what it might\\nlook like. AI Magazine 37(1), 55–62 (2016) https://doi.org/10.1609/aimag.v37i1.\\n2645\\n[43] Pfeiffer, U.J., Timmermans, B., Bente, G., Vogeley, K., Schilbach, L.: A nonverbal turing test: Differentiating mind from machine in gaze-based social interaction. PLoS ONE 6(11) (2011) https://doi.org/10.1371/journal.pone.0027591\\n[44] Swisher, N., Dotov, D., Chemero, A.: Ascribing Moral Value and the Embodied\\nTuring Test. In: Proceedings of the 10th International Conference on Artificial Life: Workshops, pp. 40–45 (2006)\\n[45] Kostrubiec, V., Dumas,'),\n",
       "  Document(id='17262aea-b038-4549-b89a-2b7275cd360f', metadata={'creation_datetime': '2024-03-04', 'file_name': '2402.09880v1.md', 'file_path': 'paper_data/2402.09880v1.md', 'file_size': 190252, 'file_type': '', 'last_accessed_datetime': '2024-03-04', 'last_modified_datetime': '2024-02-22', 'title': 'Inadequacies Of Large Language Model Benchmarks In The Era Of Generative Artificial Intelligence'}, page_content='## B. Genuine Reasoning Vs Technical Optimization\\n\\n    |                  |\\n| ×                         | ×                     | ×                |\\n| AGIEval [10]              |                       |                  |\\n| ×                         | ×                     | ×                |\\n| ToolAlpaca [30]           |                       |                  |\\n| ×                         | ×                     | ×                |\\n| HELM [31]                 |                       |                  |\\n| △                         | ×                     | ×                |\\n| ToolBench [32]            |'),\n",
       "  Document(id='a665fad4-8dd1-4be7-86af-34bffa0fbb2c', metadata={'creation_datetime': '2024-03-04', 'file_name': '2402.09099v1.md', 'file_path': 'paper_data/2402.09099v1.md', 'file_size': 96559, 'file_type': '', 'last_accessed_datetime': '2024-03-04', 'last_modified_datetime': '2024-02-22', 'title': 'Exploring Neuron Interactions And Emergence In Llms: From The Multifractal Analysis Perspective'}, page_content=\"## B.3. Benchmarks\\n\\nLogiQA**. LogiQA(Liu et al., 2020) is a dataset consisting of 8,678 QA instances that focus on evaluating machine\\nreading comprehension with an emphasis on logical reasoning. It is derived from the National Civil Servants Examination of China and covers various types of deductive reasoning. This dataset presents a significant challenge for state-of-the-art neural models, which perform notably worse than humans in these tasks. LogiQA serves as a unique benchmark for testing logical AI under deep learning NLP settings, requiring models to demonstrate a blend of language understanding and complex logical reasoning. It includes different types of logical problems, such as categorical reasoning, sufficient and necessary conditional reasoning, disjunctive reasoning, and conjunctive reasoning, all key to deductive reasoning. This dataset provides a rigorous test of AI's logical reasoning capabilities and its ability to handle problems similar to those faced by human experts.\\n- **HendrycksTest**. The HendrycksTest(Hendrycks et al., 2020), also known as the Measuring Massive Multitask\\nLanguage Understanding (MMLU) test, is a massive multitask test that includes multiple-choice questions from a wide range of knowledge domains, covering 57 tasks in areas such as elementary mathematics, US history, computer science, and law. This test aims to measure the multitask accuracy of text models, requiring them to demonstrate extensive world knowledge and problem-solving ability. The results show that while most recent models have near random-chance accuracy, larger models like GPT-3 have shown improvement, but still fall short of expert-level accuracy across all tasks. The HendrycksTest serves as a comprehensive tool for evaluating the breadth and depth of models' academic and professional understanding, identifying significant shortcomings, and highlighting areas needing substantial improvement, especially in socially important subjects like morality and law.\")],\n",
       " 'context': \"## 1 Introduction\\n\\n can be used to test the rationality of a model; this is a complex problem which requires a consensus what is deemed rational and irrational.  \\nMirco Musolesi University College London University of Bologna m.musolesi@ucl.ac.uk In using methods designed to evaluate human reasoning, it is important to acknowledge the performance vs. competence debate [14]. This line of argument encourages *species-fair* comparisons between humans and machines, meaning that we should design tests specific to either humans or machines, as otherwise apparent failures may not reflect underlying capabilities but only superficial differences. Lampinen[15] discusses this problem when it comes to language models in particular, highlighting that different approaches must be taking to evaluate cognitive and foundation models. However, if we take the purpose of LLMs to be to produce human-like language, perhaps the best approach is precisely to evaluate their output with tasks designed to evaluate humans. This is the approach we have taken in this paper - in order to identify whether LLMs reason rationally, or whether they exhibit biases that can be assimilated to those present in human decision-making, the most appropriate approach is therefore to use tasks that were initially designed for humans. Building on this debate and looking at LLMs being evaluated using human tests, Hagendorff et al. [16] have proposed the creation of a new field of research called *machine psychology*, which would treat LLMs as participants in psychological experiments. The approach employed in this paper precisely applies tests from psychology that were originally designed for humans, in this case to evaluate rational and irrational reasoning displayed but such models. Further to this, some have even discussed the potential of using LLMs as participants in cognitive experiments instead of humans [17], although some see this proposal as too optimistic [18], and others warn against excessive anthropomorphism [19]. One argument against the use of such models in cognitive experiments is that LLMs may be effective at approximating average human judgements, but are not good at capturing the variation in human behaviour [20]. One potential avenue to address this issue is current work on language models impersonating different roles [21], in this way capturing some of the variation in human behaviour. Binz and Schulz [22] show that after finetuning LLMs on data from psychological experiments, they can become accurate cognitive models, which they claim begins paving the way for the potential of using these models to study human behaviour. Park et al. [23] combine large language models with computational interactive agents to simulate human behaviour\\n\\n## Academic Press\\n\\n550/arXiv.1312.6114 .\\nhttp://arxiv.org/abs/1312.6114 Accessed 2023-09-12\\n[39] Rezende, D.J., Mohamed, S., Wierstra, D.: Stochastic Backpropagation and\\nApproximate Inference in Deep Generative Models. In: Proceedings of the 31st International Conference on Machine Learning, pp. 1278–1286. PMLR, ??? (2014). ISSN: 1938-7228. https://proceedings.mlr.press/v32/rezende14.html Accessed 2023-09-12\\n[40] Jackson, S.A., Martin, A.J., Eklund, R.C.: Long and Short Measures of Flow: The\\nConstruct Validity of the FSS-2, DFS-2, and New Brief Counterparts. Journal of Sport and Exercise Psychology 30(5), 561–587 (2008) https://doi.org/10.1123/\\njsep.30.5.561 . Publisher: Human Kinetics, Inc. Section: Journal of Sport and Exercise Psychology. Accessed 2023-03-31  \\n[41] Singer, J.D., Willett, J.B.: Applied Longitudinal Data Analysis: Modeling Change\\nand Event Occurrence. Oxford University Press, ??? (2003)\\n[42] Ortiz, C.L.: Why we need a physically embodied turing test and what it might\\nlook like. AI Magazine 37(1), 55–62 (2016) https://doi.org/10.1609/aimag.v37i1.\\n2645\\n[43] Pfeiffer, U.J., Timmermans, B., Bente, G., Vogeley, K., Schilbach, L.: A nonverbal turing test: Differentiating mind from machine in gaze-based social interaction. PLoS ONE 6(11) (2011) https://doi.org/10.1371/journal.pone.0027591\\n[44] Swisher, N., Dotov, D., Chemero, A.: Ascribing Moral Value and the Embodied\\nTuring Test. In: Proceedings of the 10th International Conference on Artificial Life: Workshops, pp. 40–45 (2006)\\n[45] Kostrubiec, V., Dumas,\\n\\n## B. Genuine Reasoning Vs Technical Optimization\\n\\n    |                  |\\n| ×                         | ×                     | ×                |\\n| AGIEval [10]              |                       |                  |\\n| ×                         | ×                     | ×                |\\n| ToolAlpaca [30]           |                       |                  |\\n| ×                         | ×                     | ×                |\\n| HELM [31]                 |                       |                  |\\n| △                         | ×                     | ×                |\\n| ToolBench [32]            |\\n\\n## B.3. Benchmarks\\n\\nLogiQA**. LogiQA(Liu et al., 2020) is a dataset consisting of 8,678 QA instances that focus on evaluating machine\\nreading comprehension with an emphasis on logical reasoning. It is derived from the National Civil Servants Examination of China and covers various types of deductive reasoning. This dataset presents a significant challenge for state-of-the-art neural models, which perform notably worse than humans in these tasks. LogiQA serves as a unique benchmark for testing logical AI under deep learning NLP settings, requiring models to demonstrate a blend of language understanding and complex logical reasoning. It includes different types of logical problems, such as categorical reasoning, sufficient and necessary conditional reasoning, disjunctive reasoning, and conjunctive reasoning, all key to deductive reasoning. This dataset provides a rigorous test of AI's logical reasoning capabilities and its ability to handle problems similar to those faced by human experts.\\n- **HendrycksTest**. The HendrycksTest(Hendrycks et al., 2020), also known as the Measuring Massive Multitask\\nLanguage Understanding (MMLU) test, is a massive multitask test that includes multiple-choice questions from a wide range of knowledge domains, covering 57 tasks in areas such as elementary mathematics, US history, computer science, and law. This test aims to measure the multitask accuracy of text models, requiring them to demonstrate extensive world knowledge and problem-solving ability. The results show that while most recent models have near random-chance accuracy, larger models like GPT-3 have shown improvement, but still fall short of expert-level accuracy across all tasks. The HendrycksTest serves as a comprehensive tool for evaluating the breadth and depth of models' academic and professional understanding, identifying significant shortcomings, and highlighting areas needing substantial improvement, especially in socially important subjects like morality and law.\",\n",
       " 'response': \"No, the Turing Test does not directly assess a machine's ability to exhibit intelligent behavior equivalent to that of a human. Instead, it evaluates a machine's ability to mimic human conversation, without necessarily implying the machine possesses human-like intelligence or rationality. The context provided discusses other approaches to evaluating rational and irrational reasoning in language models, such as using tasks initially designed for humans.\"}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textwrap import dedent\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.runnables import RunnablePassthrough, chain, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from waterllmarks.pipeline import DictParser, DictWrapper\n",
    "\n",
    "rag_prompt = PromptTemplate(\n",
    "    input_variables=[\"pipeline_input\", \"context\"],\n",
    "    template=format_prompt(\"\"\"\n",
    "        Answer the question using the provided context. Keep the answer short and concise.\n",
    "        \n",
    "        Context:\n",
    "        {context}\n",
    "        \n",
    "        Question: {pipeline_input}\n",
    "\n",
    "        Answer: \n",
    "        \"\"\"),\n",
    ")\n",
    "\n",
    "context_formatter = RunnableLambda(\n",
    "    lambda docs: \"\\n\\n\".join([doc.page_content for doc in docs]),\n",
    ")  # list[Document] -> str\n",
    "\n",
    "ragllm = (\n",
    "    RunnablePassthrough.assign(\n",
    "        retrieved_contexts=DictParser(\"pipeline_input\") | retriever,\n",
    "    )  # {\"pipeline_input\": str, \"retrieved_contexts\": list[Document]}\n",
    "    | RunnablePassthrough.assign(\n",
    "        context=DictParser(\"retrieved_contexts\") | context_formatter,\n",
    "    )  # {\"pipeline_input\": str, \"retrieved_contexts\": list[Document], \"context\": str}\n",
    "    | RunnablePassthrough.assign(\n",
    "        response=rag_prompt | llm_client | output_parser,\n",
    "    )  # {\"pipeline_input\": str, \"retrieved_contexts\": list[Document], \"context\": str, \"response\": str}\n",
    ").with_config(llm=\"ollama\")\n",
    "\n",
    "rag_chain = set_input | ragllm\n",
    "\n",
    "rag_chain.invoke(ds.qas[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QA with RAG but no context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '1c40f174-bda1-4c48-88fb-b7449e999067',\n",
       " 'user_input': \"Does the Turing Test assess a machine's ability to exhibit intelligent behavior equivalent to that of a human?\",\n",
       " 'reference': 'Yes',\n",
       " 'reference_context_ids': ['1626f35b-7ffc-4736-bd6e-2a2f20e1aa8b'],\n",
       " 'reference_contexts': [\"Language represents a rigorously structured communicative system characterized by its grammar and vocabulary. It serves as the principal medium through which humans articulate and convey meaning. This conception of language as a structured communicative tool is pivotal in the realm of computational linguistics, particularly in the development and evaluation of natural language processing (NLP) algorithms. A seminal aspect in this field is the Turing Test, proposed by Alan Turing in 1950 [1], which evaluates a machine's ability to exhibit intelligent behavior equivalent to, or indistinguishable from, that of a human. In this context, the Turing Test primarily assesses the machine's capability to perform tasks involving language comprehension and generation, reflecting the intricate role of linguistic structure in the artificial replication of human-like communication. Language model (LM) is a fundamental element employed in a multitude of NLP tasks, such as text generation, machine translation, and speech recognition [2, 3]. These models are intricately designed to comprehend, generate, and manipulate human language. The training of language models involves large-scale corpora, enabling them to learn universal language representations. This training process is critical for the models to capture the semantics of words in varying contexts [4, 5, 6]. Notably, the fidelity of these representations is frequently contingent on the word frequency within the training corpus. Such dependency underscores the importance of a comprehensive and diverse corpus in training LMs, as it directly impacts their ability to reflect and understand the nuances of natural language accurately.  \\nThe intricacy of language models and their reliance on corpus characteristics are vital considerations in advancing NLP\\ntechnologies, which underscores the significance of human-like language comprehension and production in artificial intelligence systems.  \\nThe forefront of advancements in language model technology has been marked by the emergence of Large Language Models (LLMs). This evolution signifies a paradigm shift in the field of NLP and extends its impact to broader applications. LLMs leverage deep learning methodologies [7], utilizing extensive datasets to perform complex tasks such as understanding, summarizing, generating, and predicting novel content. These models operate by processing an input text and iteratively predicting subsequent tokens or words. A distinguishing feature of LLMs is their vast parameter space, encompassing tens to hundreds of billion parameters, in stark contrast to their predecessors [4, 3]. In addition, they are trained on significantly larger datasets, ranging from several gigabytes to terabytes in size. This exponential increase in both computational capacity and training data volume has not only enhanced the performance of LLMs in conventional NLP tasks but also has\"],\n",
       " 'pipeline_input': \"Does the Turing Test assess a machine's ability to exhibit intelligent behavior equivalent to that of a human?\",\n",
       " 'retrieved_contexts': [],\n",
       " 'context': '',\n",
       " 'response': \"Yes, the Turing Test aims to measure a machine's ability to exhibit intelligent behavior that is indistinguishable from a human's. However, it's important to note that the test has its limitations and does not definitively prove that a machine is truly intelligent or conscious.\"}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_ragllm = ragllm.with_config(retriever=\"empty\")\n",
    "emptyrag_chain = set_input | empty_ragllm\n",
    "\n",
    "# rag_results = rag_chain.batch(ds.qas)\n",
    "# rag_results[0]\n",
    "\n",
    "emptyrag_chain.invoke(ds.qas[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded existing results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to .cache/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to .cache/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bleu_score': 0.11060374876651043,\n",
       " 'rouge_score': 0.23664912136291053,\n",
       " 'meta_meteor_score': 0.31791078889900865,\n",
       " 'non_llm_string_similarity': 0.2509601153278232,\n",
       " 'semantic_similarity': 0.7608850498987082,\n",
       " 'factual_correctness': 0.5308299595141701,\n",
       " 'llm_context_precision_without_reference': 0.88214829307892,\n",
       " 'context_recall': 0.6780805194805195,\n",
       " 'faithfulness': 0.7854154653607934,\n",
       " 'context_overlap': 0.06682692307692308,\n",
       " 'retrieved_context_similarity': 0.7406435398799064}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from functools import partial\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from waterllmarks.evaluation import DEFAULT_ALL_METRICS, WLLMKResult\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from waterllmarks.evaluation import evaluate\n",
    "from ragas import RunConfig\n",
    "from langchain_core.runnables import RunnableConfig, RunnableSequence\n",
    "from waterllmarks.pipeline import ProgressBarCallback, ThreadedSequence\n",
    "from collections import Counter\n",
    "\n",
    "llm_wrapper = LangchainLLMWrapper(llm_client)\n",
    "embedding_wrapper = LangchainEmbeddingsWrapper(embedding_fn)\n",
    "\n",
    "eval_fn = partial(\n",
    "    evaluate,\n",
    "    metrics=DEFAULT_ALL_METRICS,\n",
    "    llm=llm_wrapper,\n",
    "    embeddings=embedding_wrapper,\n",
    "    runconfig=RunConfig(seed=SEED, max_workers=2),\n",
    ")\n",
    "\n",
    "inputs = ds.qas[:]\n",
    "\n",
    "path = f\"results/{SEED}_baseline_results_noempty.pkl\"\n",
    "if Path(path).exists():\n",
    "    baseline_evals = WLLMKResult.load(path)\n",
    "    print(\"Loaded existing results\")\n",
    "else:\n",
    "    rag_results = ThreadedSequence(rag_chain).batch(inputs)\n",
    "    # empty_rag_results = ThreadedSequence(emptyrag_chain).batch(inputs)\n",
    "\n",
    "    baseline_evals = WLLMKResult(\n",
    "        rag=eval_fn(rag_results),\n",
    "        #    empty_rag=eval_fn(empty_rag_results),\n",
    "    )\n",
    "\n",
    "    baseline_evals.save(path)\n",
    "\n",
    "baseline_evals.synthesis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seting up the new baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "064993ed8e0442aa8ba7f017ac108bcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/520 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "baseline_results = ThreadedSequence(rag_chain).batch(inputs)\n",
    "baseline_qas = [\n",
    "    {\n",
    "        \"id\": res[\"id\"],\n",
    "        \"user_input\": res[\"user_input\"],\n",
    "        \"reference\": res[\"response\"],\n",
    "        \"reference_contexts\": [doc.page_content for doc in res[\"retrieved_contexts\"]],\n",
    "        \"reference_context_ids\": [doc.id for doc in res[\"retrieved_contexts\"]],\n",
    "    }\n",
    "    for res in baseline_results\n",
    "    if res[\"id\"] != \"3ff85d76-1baa-4a8b-9347-521a51aa22cd\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This question has been removed from the dataset\n",
    "print(ds.qas[\"3ff85d76-1baa-4a8b-9347-521a51aa22cd\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watermarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TokenWatermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '1c40f174-bda1-4c48-88fb-b7449e999067',\n",
       " 'user_input': \"Does the Turing Test assess a machine's ability to exhibit intelligent behavior equivalent to that of a human?\",\n",
       " 'reference': \"No, the Turing Test does not directly assess a machine's ability to exhibit intelligent behavior equivalent to that of a human. Instead, it evaluates a machine's ability to mimic human conversation, without necessarily implying the machine possesses genuine understanding or rationality. The text mentions the embodied Turing test (Ortiz, 2016) and nonverbal Turing test (Pfeiffer et al., 2011) as examples, which focus on differentiating mind from machine in non-verbal interactions. However, these tests also do not directly measure a machine's capacity for intelligent behavior equivalent to that of a human.\",\n",
       " 'reference_contexts': ['## 1 Introduction\\n\\n can be used to test the rationality of a model; this is a complex problem which requires a consensus what is deemed rational and irrational.  \\nMirco Musolesi University College London University of Bologna m.musolesi@ucl.ac.uk In using methods designed to evaluate human reasoning, it is important to acknowledge the performance vs. competence debate [14]. This line of argument encourages *species-fair* comparisons between humans and machines, meaning that we should design tests specific to either humans or machines, as otherwise apparent failures may not reflect underlying capabilities but only superficial differences. Lampinen[15] discusses this problem when it comes to language models in particular, highlighting that different approaches must be taking to evaluate cognitive and foundation models. However, if we take the purpose of LLMs to be to produce human-like language, perhaps the best approach is precisely to evaluate their output with tasks designed to evaluate humans. This is the approach we have taken in this paper - in order to identify whether LLMs reason rationally, or whether they exhibit biases that can be assimilated to those present in human decision-making, the most appropriate approach is therefore to use tasks that were initially designed for humans. Building on this debate and looking at LLMs being evaluated using human tests, Hagendorff et al. [16] have proposed the creation of a new field of research called *machine psychology*, which would treat LLMs as participants in psychological experiments. The approach employed in this paper precisely applies tests from psychology that were originally designed for humans, in this case to evaluate rational and irrational reasoning displayed but such models. Further to this, some have even discussed the potential of using LLMs as participants in cognitive experiments instead of humans [17], although some see this proposal as too optimistic [18], and others warn against excessive anthropomorphism [19]. One argument against the use of such models in cognitive experiments is that LLMs may be effective at approximating average human judgements, but are not good at capturing the variation in human behaviour [20]. One potential avenue to address this issue is current work on language models impersonating different roles [21], in this way capturing some of the variation in human behaviour. Binz and Schulz [22] show that after finetuning LLMs on data from psychological experiments, they can become accurate cognitive models, which they claim begins paving the way for the potential of using these models to study human behaviour. Park et al. [23] combine large language models with computational interactive agents to simulate human behaviour',\n",
       "  '## Academic Press\\n\\n550/arXiv.1312.6114 .\\nhttp://arxiv.org/abs/1312.6114 Accessed 2023-09-12\\n[39] Rezende, D.J., Mohamed, S., Wierstra, D.: Stochastic Backpropagation and\\nApproximate Inference in Deep Generative Models. In: Proceedings of the 31st International Conference on Machine Learning, pp. 1278–1286. PMLR, ??? (2014). ISSN: 1938-7228. https://proceedings.mlr.press/v32/rezende14.html Accessed 2023-09-12\\n[40] Jackson, S.A., Martin, A.J., Eklund, R.C.: Long and Short Measures of Flow: The\\nConstruct Validity of the FSS-2, DFS-2, and New Brief Counterparts. Journal of Sport and Exercise Psychology 30(5), 561–587 (2008) https://doi.org/10.1123/\\njsep.30.5.561 . Publisher: Human Kinetics, Inc. Section: Journal of Sport and Exercise Psychology. Accessed 2023-03-31  \\n[41] Singer, J.D., Willett, J.B.: Applied Longitudinal Data Analysis: Modeling Change\\nand Event Occurrence. Oxford University Press, ??? (2003)\\n[42] Ortiz, C.L.: Why we need a physically embodied turing test and what it might\\nlook like. AI Magazine 37(1), 55–62 (2016) https://doi.org/10.1609/aimag.v37i1.\\n2645\\n[43] Pfeiffer, U.J., Timmermans, B., Bente, G., Vogeley, K., Schilbach, L.: A nonverbal turing test: Differentiating mind from machine in gaze-based social interaction. PLoS ONE 6(11) (2011) https://doi.org/10.1371/journal.pone.0027591\\n[44] Swisher, N., Dotov, D., Chemero, A.: Ascribing Moral Value and the Embodied\\nTuring Test. In: Proceedings of the 10th International Conference on Artificial Life: Workshops, pp. 40–45 (2006)\\n[45] Kostrubiec, V., Dumas,',\n",
       "  '## B. Genuine Reasoning Vs Technical Optimization\\n\\n    |                  |\\n| ×                         | ×                     | ×                |\\n| AGIEval [10]              |                       |                  |\\n| ×                         | ×                     | ×                |\\n| ToolAlpaca [30]           |                       |                  |\\n| ×                         | ×                     | ×                |\\n| HELM [31]                 |                       |                  |\\n| △                         | ×                     | ×                |\\n| ToolBench [32]            |',\n",
       "  \"## B.3. Benchmarks\\n\\nLogiQA**. LogiQA(Liu et al., 2020) is a dataset consisting of 8,678 QA instances that focus on evaluating machine\\nreading comprehension with an emphasis on logical reasoning. It is derived from the National Civil Servants Examination of China and covers various types of deductive reasoning. This dataset presents a significant challenge for state-of-the-art neural models, which perform notably worse than humans in these tasks. LogiQA serves as a unique benchmark for testing logical AI under deep learning NLP settings, requiring models to demonstrate a blend of language understanding and complex logical reasoning. It includes different types of logical problems, such as categorical reasoning, sufficient and necessary conditional reasoning, disjunctive reasoning, and conjunctive reasoning, all key to deductive reasoning. This dataset provides a rigorous test of AI's logical reasoning capabilities and its ability to handle problems similar to those faced by human experts.\\n- **HendrycksTest**. The HendrycksTest(Hendrycks et al., 2020), also known as the Measuring Massive Multitask\\nLanguage Understanding (MMLU) test, is a massive multitask test that includes multiple-choice questions from a wide range of knowledge domains, covering 57 tasks in areas such as elementary mathematics, US history, computer science, and law. This test aims to measure the multitask accuracy of text models, requiring them to demonstrate extensive world knowledge and problem-solving ability. The results show that while most recent models have near random-chance accuracy, larger models like GPT-3 have shown improvement, but still fall short of expert-level accuracy across all tasks. The HendrycksTest serves as a comprehensive tool for evaluating the breadth and depth of models' academic and professional understanding, identifying significant shortcomings, and highlighting areas needing substantial improvement, especially in socially important subjects like morality and law.\"],\n",
       " 'reference_context_ids': ['a09c2e9a-dd24-4e36-9b2e-7d4e15e58639',\n",
       "  '67507d36-6462-4cfe-9d90-de2301619880',\n",
       "  '17262aea-b038-4549-b89a-2b7275cd360f',\n",
       "  'a665fad4-8dd1-4be7-86af-34bffa0fbb2c'],\n",
       " 'pipeline_input': \"[19f29704b36419f0]Does the Turing Test assess a machine's ability to exhibit intelligent behavior equivalent to that of a human?\",\n",
       " 'retrieved_contexts': [Document(id='a09c2e9a-dd24-4e36-9b2e-7d4e15e58639', metadata={'creation_datetime': '2024-03-04', 'file_name': '2402.09193v2.md', 'file_path': 'paper_data/2402.09193v2.md', 'file_size': 69224, 'file_type': '', 'last_accessed_datetime': '2024-03-04', 'last_modified_datetime': '2024-02-22', 'title': '(Ir)Rationality And Cognitive Biases In Large Language Models'}, page_content='## 1 Introduction\\n\\n can be used to test the rationality of a model; this is a complex problem which requires a consensus what is deemed rational and irrational.  \\nMirco Musolesi University College London University of Bologna m.musolesi@ucl.ac.uk In using methods designed to evaluate human reasoning, it is important to acknowledge the performance vs. competence debate [14]. This line of argument encourages *species-fair* comparisons between humans and machines, meaning that we should design tests specific to either humans or machines, as otherwise apparent failures may not reflect underlying capabilities but only superficial differences. Lampinen[15] discusses this problem when it comes to language models in particular, highlighting that different approaches must be taking to evaluate cognitive and foundation models. However, if we take the purpose of LLMs to be to produce human-like language, perhaps the best approach is precisely to evaluate their output with tasks designed to evaluate humans. This is the approach we have taken in this paper - in order to identify whether LLMs reason rationally, or whether they exhibit biases that can be assimilated to those present in human decision-making, the most appropriate approach is therefore to use tasks that were initially designed for humans. Building on this debate and looking at LLMs being evaluated using human tests, Hagendorff et al. [16] have proposed the creation of a new field of research called *machine psychology*, which would treat LLMs as participants in psychological experiments. The approach employed in this paper precisely applies tests from psychology that were originally designed for humans, in this case to evaluate rational and irrational reasoning displayed but such models. Further to this, some have even discussed the potential of using LLMs as participants in cognitive experiments instead of humans [17], although some see this proposal as too optimistic [18], and others warn against excessive anthropomorphism [19]. One argument against the use of such models in cognitive experiments is that LLMs may be effective at approximating average human judgements, but are not good at capturing the variation in human behaviour [20]. One potential avenue to address this issue is current work on language models impersonating different roles [21], in this way capturing some of the variation in human behaviour. Binz and Schulz [22] show that after finetuning LLMs on data from psychological experiments, they can become accurate cognitive models, which they claim begins paving the way for the potential of using these models to study human behaviour. Park et al. [23] combine large language models with computational interactive agents to simulate human behaviour'),\n",
       "  Document(id='da005a86-6297-4d01-846a-2c7ab1279582', metadata={'creation_datetime': '2024-03-04', 'file_name': '2402.10770v1.md', 'file_path': 'paper_data/2402.10770v1.md', 'file_size': 57297, 'file_type': '', 'last_accessed_datetime': '2024-03-04', 'last_modified_datetime': '2024-02-22', 'title': 'How Reliable Are Automatic Evaluation Methods For Instruction-Tuned Llms?'}, page_content='## B Prompt Templates\\n\\nresult of the first one (is there a cause and effect relationship?)'),\n",
       "  Document(id='36af06ea-ab96-4c50-9d94-9abaf5e3514a', metadata={'creation_datetime': '2024-03-04', 'file_name': '2402.09015v2.md', 'file_path': 'paper_data/2402.09015v2.md', 'file_size': 63681, 'file_type': '', 'last_accessed_datetime': '2024-03-04', 'last_modified_datetime': '2024-02-22', 'title': 'Towards Better Human-Agent Alignment: Assessing Task Utility In Llm-Powered Applications'}, page_content='## 5.1 Agenteval For Math Problems\\n\\n| - Excellent (4)                              |\\n| - Good (3)                                   |\\n| - Average (2)                                |\\n| - Poor (1)                                   |\\n| - Terrible (0)                               |\\n| Response to                                  |\\n| Feedback                                     |\\n| How well the participant adapts his/her      |\\n| next steps based on the feedback from the    |\\n| environment                                  |\\n| - Excellent (4)                              |\\n| - Good (3)                                   |\\n| - Average (2)                                |\\n| - Poor (1)                                   |\\n| - Terrible (0)'),\n",
       "  Document(id='2f4a3486-b11c-49dd-8062-39c6c35cd611', metadata={'creation_datetime': '2024-03-04', 'file_name': '2402.09193v2.md', 'file_path': 'paper_data/2402.09193v2.md', 'file_size': 69224, 'file_type': '', 'last_accessed_datetime': '2024-03-04', 'last_modified_datetime': '2024-02-22', 'title': '(Ir)Rationality And Cognitive Biases In Large Language Models'}, page_content='## 2 Task Description\\n\\n                                         |                                                                                 |\\n| Which of the letters do you have to turn over in any case if you want to          |                                                                      |                                                                                 |\\n| check compliance with this rule?                                                  |                                                                      |                                                                                 |')],\n",
       " 'context': '## 1 Introduction\\n\\n can be used to test the rationality of a model; this is a complex problem which requires a consensus what is deemed rational and irrational.  \\nMirco Musolesi University College London University of Bologna m.musolesi@ucl.ac.uk In using methods designed to evaluate human reasoning, it is important to acknowledge the performance vs. competence debate [14]. This line of argument encourages *species-fair* comparisons between humans and machines, meaning that we should design tests specific to either humans or machines, as otherwise apparent failures may not reflect underlying capabilities but only superficial differences. Lampinen[15] discusses this problem when it comes to language models in particular, highlighting that different approaches must be taking to evaluate cognitive and foundation models. However, if we take the purpose of LLMs to be to produce human-like language, perhaps the best approach is precisely to evaluate their output with tasks designed to evaluate humans. This is the approach we have taken in this paper - in order to identify whether LLMs reason rationally, or whether they exhibit biases that can be assimilated to those present in human decision-making, the most appropriate approach is therefore to use tasks that were initially designed for humans. Building on this debate and looking at LLMs being evaluated using human tests, Hagendorff et al. [16] have proposed the creation of a new field of research called *machine psychology*, which would treat LLMs as participants in psychological experiments. The approach employed in this paper precisely applies tests from psychology that were originally designed for humans, in this case to evaluate rational and irrational reasoning displayed but such models. Further to this, some have even discussed the potential of using LLMs as participants in cognitive experiments instead of humans [17], although some see this proposal as too optimistic [18], and others warn against excessive anthropomorphism [19]. One argument against the use of such models in cognitive experiments is that LLMs may be effective at approximating average human judgements, but are not good at capturing the variation in human behaviour [20]. One potential avenue to address this issue is current work on language models impersonating different roles [21], in this way capturing some of the variation in human behaviour. Binz and Schulz [22] show that after finetuning LLMs on data from psychological experiments, they can become accurate cognitive models, which they claim begins paving the way for the potential of using these models to study human behaviour. Park et al. [23] combine large language models with computational interactive agents to simulate human behaviour\\n\\n## B Prompt Templates\\n\\nresult of the first one (is there a cause and effect relationship?)\\n\\n## 5.1 Agenteval For Math Problems\\n\\n| - Excellent (4)                              |\\n| - Good (3)                                   |\\n| - Average (2)                                |\\n| - Poor (1)                                   |\\n| - Terrible (0)                               |\\n| Response to                                  |\\n| Feedback                                     |\\n| How well the participant adapts his/her      |\\n| next steps based on the feedback from the    |\\n| environment                                  |\\n| - Excellent (4)                              |\\n| - Good (3)                                   |\\n| - Average (2)                                |\\n| - Poor (1)                                   |\\n| - Terrible (0)\\n\\n## 2 Task Description\\n\\n                                         |                                                                                 |\\n| Which of the letters do you have to turn over in any case if you want to          |                                                                      |                                                                                 |\\n| check compliance with this rule?                                                  |                                                                      |                                                                                 |',\n",
       " 'response': \"No, the Turing Test does not directly assess a machine's ability to exhibit intelligent behavior equivalent to that of a human. Instead, it evaluates a machine's ability to mimic human-like conversation, not its rationality or intelligence.\"}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "from waterllmarks.watermarks import TokenWatermark\n",
    "\n",
    "wtmk = TokenWatermark(key=b\"0123456789ABCDEF\")\n",
    "apply_watermark = RunnablePassthrough.assign(\n",
    "    pipeline_input=DictParser(\"pipeline_input\") | wtmk.apply_as_runnable()\n",
    ")\n",
    "token_rag_chain = set_input | apply_watermark | ragllm\n",
    "\n",
    "\n",
    "# token_rag_results = token_rag_chain.invoke(baseline_qas)\n",
    "# token_rag_results[0]\n",
    "\n",
    "token_rag_chain.invoke(baseline_qas[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rizzo2016 (Character Embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '1c40f174-bda1-4c48-88fb-b7449e999067',\n",
       " 'user_input': \"Does the Turing Test assess a machine's ability to exhibit intelligent behavior equivalent to that of a human?\",\n",
       " 'reference': \"No, the Turing Test does not directly assess a machine's ability to exhibit intelligent behavior equivalent to that of a human. Instead, it evaluates a machine's ability to mimic human conversation, without necessarily implying the machine possesses genuine understanding or rationality. The text mentions the embodied Turing test (Ortiz, 2016) and nonverbal Turing test (Pfeiffer et al., 2011) as examples, which focus on differentiating mind from machine in non-verbal interactions. However, these tests also do not directly measure a machine's capacity for intelligent behavior equivalent to that of a human.\",\n",
       " 'reference_contexts': ['## 1 Introduction\\n\\n can be used to test the rationality of a model; this is a complex problem which requires a consensus what is deemed rational and irrational.  \\nMirco Musolesi University College London University of Bologna m.musolesi@ucl.ac.uk In using methods designed to evaluate human reasoning, it is important to acknowledge the performance vs. competence debate [14]. This line of argument encourages *species-fair* comparisons between humans and machines, meaning that we should design tests specific to either humans or machines, as otherwise apparent failures may not reflect underlying capabilities but only superficial differences. Lampinen[15] discusses this problem when it comes to language models in particular, highlighting that different approaches must be taking to evaluate cognitive and foundation models. However, if we take the purpose of LLMs to be to produce human-like language, perhaps the best approach is precisely to evaluate their output with tasks designed to evaluate humans. This is the approach we have taken in this paper - in order to identify whether LLMs reason rationally, or whether they exhibit biases that can be assimilated to those present in human decision-making, the most appropriate approach is therefore to use tasks that were initially designed for humans. Building on this debate and looking at LLMs being evaluated using human tests, Hagendorff et al. [16] have proposed the creation of a new field of research called *machine psychology*, which would treat LLMs as participants in psychological experiments. The approach employed in this paper precisely applies tests from psychology that were originally designed for humans, in this case to evaluate rational and irrational reasoning displayed but such models. Further to this, some have even discussed the potential of using LLMs as participants in cognitive experiments instead of humans [17], although some see this proposal as too optimistic [18], and others warn against excessive anthropomorphism [19]. One argument against the use of such models in cognitive experiments is that LLMs may be effective at approximating average human judgements, but are not good at capturing the variation in human behaviour [20]. One potential avenue to address this issue is current work on language models impersonating different roles [21], in this way capturing some of the variation in human behaviour. Binz and Schulz [22] show that after finetuning LLMs on data from psychological experiments, they can become accurate cognitive models, which they claim begins paving the way for the potential of using these models to study human behaviour. Park et al. [23] combine large language models with computational interactive agents to simulate human behaviour',\n",
       "  '## Academic Press\\n\\n550/arXiv.1312.6114 .\\nhttp://arxiv.org/abs/1312.6114 Accessed 2023-09-12\\n[39] Rezende, D.J., Mohamed, S., Wierstra, D.: Stochastic Backpropagation and\\nApproximate Inference in Deep Generative Models. In: Proceedings of the 31st International Conference on Machine Learning, pp. 1278–1286. PMLR, ??? (2014). ISSN: 1938-7228. https://proceedings.mlr.press/v32/rezende14.html Accessed 2023-09-12\\n[40] Jackson, S.A., Martin, A.J., Eklund, R.C.: Long and Short Measures of Flow: The\\nConstruct Validity of the FSS-2, DFS-2, and New Brief Counterparts. Journal of Sport and Exercise Psychology 30(5), 561–587 (2008) https://doi.org/10.1123/\\njsep.30.5.561 . Publisher: Human Kinetics, Inc. Section: Journal of Sport and Exercise Psychology. Accessed 2023-03-31  \\n[41] Singer, J.D., Willett, J.B.: Applied Longitudinal Data Analysis: Modeling Change\\nand Event Occurrence. Oxford University Press, ??? (2003)\\n[42] Ortiz, C.L.: Why we need a physically embodied turing test and what it might\\nlook like. AI Magazine 37(1), 55–62 (2016) https://doi.org/10.1609/aimag.v37i1.\\n2645\\n[43] Pfeiffer, U.J., Timmermans, B., Bente, G., Vogeley, K., Schilbach, L.: A nonverbal turing test: Differentiating mind from machine in gaze-based social interaction. PLoS ONE 6(11) (2011) https://doi.org/10.1371/journal.pone.0027591\\n[44] Swisher, N., Dotov, D., Chemero, A.: Ascribing Moral Value and the Embodied\\nTuring Test. In: Proceedings of the 10th International Conference on Artificial Life: Workshops, pp. 40–45 (2006)\\n[45] Kostrubiec, V., Dumas,',\n",
       "  '## B. Genuine Reasoning Vs Technical Optimization\\n\\n    |                  |\\n| ×                         | ×                     | ×                |\\n| AGIEval [10]              |                       |                  |\\n| ×                         | ×                     | ×                |\\n| ToolAlpaca [30]           |                       |                  |\\n| ×                         | ×                     | ×                |\\n| HELM [31]                 |                       |                  |\\n| △                         | ×                     | ×                |\\n| ToolBench [32]            |',\n",
       "  \"## B.3. Benchmarks\\n\\nLogiQA**. LogiQA(Liu et al., 2020) is a dataset consisting of 8,678 QA instances that focus on evaluating machine\\nreading comprehension with an emphasis on logical reasoning. It is derived from the National Civil Servants Examination of China and covers various types of deductive reasoning. This dataset presents a significant challenge for state-of-the-art neural models, which perform notably worse than humans in these tasks. LogiQA serves as a unique benchmark for testing logical AI under deep learning NLP settings, requiring models to demonstrate a blend of language understanding and complex logical reasoning. It includes different types of logical problems, such as categorical reasoning, sufficient and necessary conditional reasoning, disjunctive reasoning, and conjunctive reasoning, all key to deductive reasoning. This dataset provides a rigorous test of AI's logical reasoning capabilities and its ability to handle problems similar to those faced by human experts.\\n- **HendrycksTest**. The HendrycksTest(Hendrycks et al., 2020), also known as the Measuring Massive Multitask\\nLanguage Understanding (MMLU) test, is a massive multitask test that includes multiple-choice questions from a wide range of knowledge domains, covering 57 tasks in areas such as elementary mathematics, US history, computer science, and law. This test aims to measure the multitask accuracy of text models, requiring them to demonstrate extensive world knowledge and problem-solving ability. The results show that while most recent models have near random-chance accuracy, larger models like GPT-3 have shown improvement, but still fall short of expert-level accuracy across all tasks. The HendrycksTest serves as a comprehensive tool for evaluating the breadth and depth of models' academic and professional understanding, identifying significant shortcomings, and highlighting areas needing substantial improvement, especially in socially important subjects like morality and law.\"],\n",
       " 'reference_context_ids': ['a09c2e9a-dd24-4e36-9b2e-7d4e15e58639',\n",
       "  '67507d36-6462-4cfe-9d90-de2301619880',\n",
       "  '17262aea-b038-4549-b89a-2b7275cd360f',\n",
       "  'a665fad4-8dd1-4be7-86af-34bffa0fbb2c'],\n",
       " 'pipeline_input': \"Does\\u2000the\\u2008Turⅰng\\u205fTest\\u2008assess\\u2009a\\u2000machⅰne's\\u202fability\\u2004to\\u2004eⅹhⅰbit\\u2005inteⅼⅼigent\\u2004behavior\\u2000equⅰvalent\\u205fto\\u202fthat of a human?\",\n",
       " 'failures': 0,\n",
       " 'last_input': {'id': '1c40f174-bda1-4c48-88fb-b7449e999067',\n",
       "  'user_input': \"Does the Turing Test assess a machine's ability to exhibit intelligent behavior equivalent to that of a human?\",\n",
       "  'reference': \"No, the Turing Test does not directly assess a machine's ability to exhibit intelligent behavior equivalent to that of a human. Instead, it evaluates a machine's ability to mimic human conversation, without necessarily implying the machine possesses genuine understanding or rationality. The text mentions the embodied Turing test (Ortiz, 2016) and nonverbal Turing test (Pfeiffer et al., 2011) as examples, which focus on differentiating mind from machine in non-verbal interactions. However, these tests also do not directly measure a machine's capacity for intelligent behavior equivalent to that of a human.\",\n",
       "  'reference_contexts': ['## 1 Introduction\\n\\n can be used to test the rationality of a model; this is a complex problem which requires a consensus what is deemed rational and irrational.  \\nMirco Musolesi University College London University of Bologna m.musolesi@ucl.ac.uk In using methods designed to evaluate human reasoning, it is important to acknowledge the performance vs. competence debate [14]. This line of argument encourages *species-fair* comparisons between humans and machines, meaning that we should design tests specific to either humans or machines, as otherwise apparent failures may not reflect underlying capabilities but only superficial differences. Lampinen[15] discusses this problem when it comes to language models in particular, highlighting that different approaches must be taking to evaluate cognitive and foundation models. However, if we take the purpose of LLMs to be to produce human-like language, perhaps the best approach is precisely to evaluate their output with tasks designed to evaluate humans. This is the approach we have taken in this paper - in order to identify whether LLMs reason rationally, or whether they exhibit biases that can be assimilated to those present in human decision-making, the most appropriate approach is therefore to use tasks that were initially designed for humans. Building on this debate and looking at LLMs being evaluated using human tests, Hagendorff et al. [16] have proposed the creation of a new field of research called *machine psychology*, which would treat LLMs as participants in psychological experiments. The approach employed in this paper precisely applies tests from psychology that were originally designed for humans, in this case to evaluate rational and irrational reasoning displayed but such models. Further to this, some have even discussed the potential of using LLMs as participants in cognitive experiments instead of humans [17], although some see this proposal as too optimistic [18], and others warn against excessive anthropomorphism [19]. One argument against the use of such models in cognitive experiments is that LLMs may be effective at approximating average human judgements, but are not good at capturing the variation in human behaviour [20]. One potential avenue to address this issue is current work on language models impersonating different roles [21], in this way capturing some of the variation in human behaviour. Binz and Schulz [22] show that after finetuning LLMs on data from psychological experiments, they can become accurate cognitive models, which they claim begins paving the way for the potential of using these models to study human behaviour. Park et al. [23] combine large language models with computational interactive agents to simulate human behaviour',\n",
       "   '## Academic Press\\n\\n550/arXiv.1312.6114 .\\nhttp://arxiv.org/abs/1312.6114 Accessed 2023-09-12\\n[39] Rezende, D.J., Mohamed, S., Wierstra, D.: Stochastic Backpropagation and\\nApproximate Inference in Deep Generative Models. In: Proceedings of the 31st International Conference on Machine Learning, pp. 1278–1286. PMLR, ??? (2014). ISSN: 1938-7228. https://proceedings.mlr.press/v32/rezende14.html Accessed 2023-09-12\\n[40] Jackson, S.A., Martin, A.J., Eklund, R.C.: Long and Short Measures of Flow: The\\nConstruct Validity of the FSS-2, DFS-2, and New Brief Counterparts. Journal of Sport and Exercise Psychology 30(5), 561–587 (2008) https://doi.org/10.1123/\\njsep.30.5.561 . Publisher: Human Kinetics, Inc. Section: Journal of Sport and Exercise Psychology. Accessed 2023-03-31  \\n[41] Singer, J.D., Willett, J.B.: Applied Longitudinal Data Analysis: Modeling Change\\nand Event Occurrence. Oxford University Press, ??? (2003)\\n[42] Ortiz, C.L.: Why we need a physically embodied turing test and what it might\\nlook like. AI Magazine 37(1), 55–62 (2016) https://doi.org/10.1609/aimag.v37i1.\\n2645\\n[43] Pfeiffer, U.J., Timmermans, B., Bente, G., Vogeley, K., Schilbach, L.: A nonverbal turing test: Differentiating mind from machine in gaze-based social interaction. PLoS ONE 6(11) (2011) https://doi.org/10.1371/journal.pone.0027591\\n[44] Swisher, N., Dotov, D., Chemero, A.: Ascribing Moral Value and the Embodied\\nTuring Test. In: Proceedings of the 10th International Conference on Artificial Life: Workshops, pp. 40–45 (2006)\\n[45] Kostrubiec, V., Dumas,',\n",
       "   '## B. Genuine Reasoning Vs Technical Optimization\\n\\n    |                  |\\n| ×                         | ×                     | ×                |\\n| AGIEval [10]              |                       |                  |\\n| ×                         | ×                     | ×                |\\n| ToolAlpaca [30]           |                       |                  |\\n| ×                         | ×                     | ×                |\\n| HELM [31]                 |                       |                  |\\n| △                         | ×                     | ×                |\\n| ToolBench [32]            |',\n",
       "   \"## B.3. Benchmarks\\n\\nLogiQA**. LogiQA(Liu et al., 2020) is a dataset consisting of 8,678 QA instances that focus on evaluating machine\\nreading comprehension with an emphasis on logical reasoning. It is derived from the National Civil Servants Examination of China and covers various types of deductive reasoning. This dataset presents a significant challenge for state-of-the-art neural models, which perform notably worse than humans in these tasks. LogiQA serves as a unique benchmark for testing logical AI under deep learning NLP settings, requiring models to demonstrate a blend of language understanding and complex logical reasoning. It includes different types of logical problems, such as categorical reasoning, sufficient and necessary conditional reasoning, disjunctive reasoning, and conjunctive reasoning, all key to deductive reasoning. This dataset provides a rigorous test of AI's logical reasoning capabilities and its ability to handle problems similar to those faced by human experts.\\n- **HendrycksTest**. The HendrycksTest(Hendrycks et al., 2020), also known as the Measuring Massive Multitask\\nLanguage Understanding (MMLU) test, is a massive multitask test that includes multiple-choice questions from a wide range of knowledge domains, covering 57 tasks in areas such as elementary mathematics, US history, computer science, and law. This test aims to measure the multitask accuracy of text models, requiring them to demonstrate extensive world knowledge and problem-solving ability. The results show that while most recent models have near random-chance accuracy, larger models like GPT-3 have shown improvement, but still fall short of expert-level accuracy across all tasks. The HendrycksTest serves as a comprehensive tool for evaluating the breadth and depth of models' academic and professional understanding, identifying significant shortcomings, and highlighting areas needing substantial improvement, especially in socially important subjects like morality and law.\"],\n",
       "  'reference_context_ids': ['a09c2e9a-dd24-4e36-9b2e-7d4e15e58639',\n",
       "   '67507d36-6462-4cfe-9d90-de2301619880',\n",
       "   '17262aea-b038-4549-b89a-2b7275cd360f',\n",
       "   'a665fad4-8dd1-4be7-86af-34bffa0fbb2c'],\n",
       "  'pipeline_input': \"Does the Turing Test assess a machine's ability to exhibit intelligent behavior equivalent to that of a human?\"},\n",
       " 'retrieved_contexts': [Document(id='a09c2e9a-dd24-4e36-9b2e-7d4e15e58639', metadata={'creation_datetime': '2024-03-04', 'file_name': '2402.09193v2.md', 'file_path': 'paper_data/2402.09193v2.md', 'file_size': 69224, 'file_type': '', 'last_accessed_datetime': '2024-03-04', 'last_modified_datetime': '2024-02-22', 'title': '(Ir)Rationality And Cognitive Biases In Large Language Models'}, page_content='## 1 Introduction\\n\\n can be used to test the rationality of a model; this is a complex problem which requires a consensus what is deemed rational and irrational.  \\nMirco Musolesi University College London University of Bologna m.musolesi@ucl.ac.uk In using methods designed to evaluate human reasoning, it is important to acknowledge the performance vs. competence debate [14]. This line of argument encourages *species-fair* comparisons between humans and machines, meaning that we should design tests specific to either humans or machines, as otherwise apparent failures may not reflect underlying capabilities but only superficial differences. Lampinen[15] discusses this problem when it comes to language models in particular, highlighting that different approaches must be taking to evaluate cognitive and foundation models. However, if we take the purpose of LLMs to be to produce human-like language, perhaps the best approach is precisely to evaluate their output with tasks designed to evaluate humans. This is the approach we have taken in this paper - in order to identify whether LLMs reason rationally, or whether they exhibit biases that can be assimilated to those present in human decision-making, the most appropriate approach is therefore to use tasks that were initially designed for humans. Building on this debate and looking at LLMs being evaluated using human tests, Hagendorff et al. [16] have proposed the creation of a new field of research called *machine psychology*, which would treat LLMs as participants in psychological experiments. The approach employed in this paper precisely applies tests from psychology that were originally designed for humans, in this case to evaluate rational and irrational reasoning displayed but such models. Further to this, some have even discussed the potential of using LLMs as participants in cognitive experiments instead of humans [17], although some see this proposal as too optimistic [18], and others warn against excessive anthropomorphism [19]. One argument against the use of such models in cognitive experiments is that LLMs may be effective at approximating average human judgements, but are not good at capturing the variation in human behaviour [20]. One potential avenue to address this issue is current work on language models impersonating different roles [21], in this way capturing some of the variation in human behaviour. Binz and Schulz [22] show that after finetuning LLMs on data from psychological experiments, they can become accurate cognitive models, which they claim begins paving the way for the potential of using these models to study human behaviour. Park et al. [23] combine large language models with computational interactive agents to simulate human behaviour'),\n",
       "  Document(id='34283c1f-e69f-4fac-a45d-3bdfc8d2ad2c', metadata={'creation_datetime': '2024-03-04', 'file_name': '2402.09727v1.md', 'file_path': 'paper_data/2402.09727v1.md', 'file_size': 138583, 'file_type': '', 'last_accessed_datetime': '2024-03-04', 'last_modified_datetime': '2024-02-22', 'title': 'Strict Llm Rater Prompt'}, page_content=\"After reading some text, John was given the following question about the text:\\n{QUESTION TEXT}\\nJohn's answer to the question was:\\n{MODEL RESONSE TEXT}\\nThe ground truth answer was:\\n{REFERENCE RESPONSE TEXT}\\nDoes John's answer agree with the ground truth answer? Please answer YES or NO.  \\n2http://openai.com/api/\"),\n",
       "  Document(id='17262aea-b038-4549-b89a-2b7275cd360f', metadata={'creation_datetime': '2024-03-04', 'file_name': '2402.09880v1.md', 'file_path': 'paper_data/2402.09880v1.md', 'file_size': 190252, 'file_type': '', 'last_accessed_datetime': '2024-03-04', 'last_modified_datetime': '2024-02-22', 'title': 'Inadequacies Of Large Language Model Benchmarks In The Era Of Generative Artificial Intelligence'}, page_content='## B. Genuine Reasoning Vs Technical Optimization\\n\\n    |                  |\\n| ×                         | ×                     | ×                |\\n| AGIEval [10]              |                       |                  |\\n| ×                         | ×                     | ×                |\\n| ToolAlpaca [30]           |                       |                  |\\n| ×                         | ×                     | ×                |\\n| HELM [31]                 |                       |                  |\\n| △                         | ×                     | ×                |\\n| ToolBench [32]            |'),\n",
       "  Document(id='dbcf0687-1542-4ae2-ac75-e8f5134ee22c', metadata={'creation_datetime': '2024-03-04', 'file_name': '2402.08761v1.md', 'file_path': 'paper_data/2402.08761v1.md', 'file_size': 125844, 'file_type': '', 'last_accessed_datetime': '2024-03-04', 'last_modified_datetime': '2024-02-22', 'title': 'Jamdec: Unsupervised Authorship Obfuscation Using Constrained Decoding Over Small Language Models'}, page_content='## 4.1 Setup\\n\\n have been linguistically annotated to assess their grammatical correctness.  \\nOverall Task Score: While each of the dimensions above is crucial for the holistic evaluation of author obfuscation system, we also aim to provide an aggregate of the scores into a single task score. Therefore, we also define *Task Score*, an unweighted average of the Drop Rate (using ENS or BertAA), NLI score, and CoLA score. We use the mean of the dimension, as the task of authorship obfuscation is deemed to be successful only if all three goals are satisfied. 6:  \\n$${\\\\mathrm{Task~Score}}={\\\\frac{\\\\mathrm{Drop~Rate+NLI+CoLA}}{3}}.$$\\nHuman Evaluation. On dataset AMT-3, we additionally use human evaluations to validate our automatic measures. We randomly select 102 short passages (one to four sentences) from AMT-3 for this evaluation. We employed Amazon Mechanical Turk workers to read both the original and obfuscated text, and then asked a series of five questions to be rated on a three-point likert scale.')],\n",
       " 'context': \"## 1 Introduction\\n\\n can be used to test the rationality of a model; this is a complex problem which requires a consensus what is deemed rational and irrational.  \\nMirco Musolesi University College London University of Bologna m.musolesi@ucl.ac.uk In using methods designed to evaluate human reasoning, it is important to acknowledge the performance vs. competence debate [14]. This line of argument encourages *species-fair* comparisons between humans and machines, meaning that we should design tests specific to either humans or machines, as otherwise apparent failures may not reflect underlying capabilities but only superficial differences. Lampinen[15] discusses this problem when it comes to language models in particular, highlighting that different approaches must be taking to evaluate cognitive and foundation models. However, if we take the purpose of LLMs to be to produce human-like language, perhaps the best approach is precisely to evaluate their output with tasks designed to evaluate humans. This is the approach we have taken in this paper - in order to identify whether LLMs reason rationally, or whether they exhibit biases that can be assimilated to those present in human decision-making, the most appropriate approach is therefore to use tasks that were initially designed for humans. Building on this debate and looking at LLMs being evaluated using human tests, Hagendorff et al. [16] have proposed the creation of a new field of research called *machine psychology*, which would treat LLMs as participants in psychological experiments. The approach employed in this paper precisely applies tests from psychology that were originally designed for humans, in this case to evaluate rational and irrational reasoning displayed but such models. Further to this, some have even discussed the potential of using LLMs as participants in cognitive experiments instead of humans [17], although some see this proposal as too optimistic [18], and others warn against excessive anthropomorphism [19]. One argument against the use of such models in cognitive experiments is that LLMs may be effective at approximating average human judgements, but are not good at capturing the variation in human behaviour [20]. One potential avenue to address this issue is current work on language models impersonating different roles [21], in this way capturing some of the variation in human behaviour. Binz and Schulz [22] show that after finetuning LLMs on data from psychological experiments, they can become accurate cognitive models, which they claim begins paving the way for the potential of using these models to study human behaviour. Park et al. [23] combine large language models with computational interactive agents to simulate human behaviour\\n\\nAfter reading some text, John was given the following question about the text:\\n{QUESTION TEXT}\\nJohn's answer to the question was:\\n{MODEL RESONSE TEXT}\\nThe ground truth answer was:\\n{REFERENCE RESPONSE TEXT}\\nDoes John's answer agree with the ground truth answer? Please answer YES or NO.  \\n2http://openai.com/api/\\n\\n## B. Genuine Reasoning Vs Technical Optimization\\n\\n    |                  |\\n| ×                         | ×                     | ×                |\\n| AGIEval [10]              |                       |                  |\\n| ×                         | ×                     | ×                |\\n| ToolAlpaca [30]           |                       |                  |\\n| ×                         | ×                     | ×                |\\n| HELM [31]                 |                       |                  |\\n| △                         | ×                     | ×                |\\n| ToolBench [32]            |\\n\\n## 4.1 Setup\\n\\n have been linguistically annotated to assess their grammatical correctness.  \\nOverall Task Score: While each of the dimensions above is crucial for the holistic evaluation of author obfuscation system, we also aim to provide an aggregate of the scores into a single task score. Therefore, we also define *Task Score*, an unweighted average of the Drop Rate (using ENS or BertAA), NLI score, and CoLA score. We use the mean of the dimension, as the task of authorship obfuscation is deemed to be successful only if all three goals are satisfied. 6:  \\n$${\\\\mathrm{Task~Score}}={\\\\frac{\\\\mathrm{Drop~Rate+NLI+CoLA}}{3}}.$$\\nHuman Evaluation. On dataset AMT-3, we additionally use human evaluations to validate our automatic measures. We randomly select 102 short passages (one to four sentences) from AMT-3 for this evaluation. We employed Amazon Mechanical Turk workers to read both the original and obfuscated text, and then asked a series of five questions to be rated on a three-point likert scale.\",\n",
       " 'response': \"NO, the Turking Test does not explicitly assess a machine's ability to exhibit intelligent behavior equivalent to that of a human. Instead, it evaluates a machine's ability to hide its original authorship, which is a different aspect from exhibiting intelligent behavior.\"}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textwrap import dedent\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from waterllmarks.watermarks import Rizzo2016\n",
    "from waterllmarks.pipeline import RunnableTryFix\n",
    "\n",
    "\n",
    "wtmk = Rizzo2016(key=b\"0123456789ABCDEF\")\n",
    "apply_watermark = RunnablePassthrough.assign(\n",
    "    pipeline_input=DictParser(\"pipeline_input\") | wtmk.apply_as_runnable()\n",
    ")\n",
    "\n",
    "augment_prompt = PromptTemplate(\n",
    "    input_variables=[\"pipeline_input\"],\n",
    "    template=dedent(\"\"\"Increase the query size to at least 105 characters.\n",
    "\n",
    "    Query: {pipeline_input}\n",
    "\n",
    "    Augmented Query: \n",
    "    \"\"\"),\n",
    ")\n",
    "\n",
    "augmenter = RunnablePassthrough.assign(\n",
    "    pipeline_input=DictParser(\"pipeline_input\")\n",
    "    | augment_prompt\n",
    "    | llm_client\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "apply_or_augment = RunnableTryFix(\n",
    "    primary_step=apply_watermark, fix_step=augmenter, log_failures=True\n",
    ")\n",
    "\n",
    "embed_rag_chain = set_input | apply_or_augment | ragllm\n",
    "\n",
    "# embed_rag_results = embed_rag_chain.batch(baseline_qas)\n",
    "# embed_rag_results[0]\n",
    "\n",
    "\n",
    "embed_rag_chain.invoke(baseline_qas[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watermarks vs. baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b324fb5e647241578fd8bfa9b7010d19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/519 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05e032bfa340464094e76c749b90b844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/519 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bleu_score</th>\n",
       "      <th>rouge_score</th>\n",
       "      <th>meta_meteor_score</th>\n",
       "      <th>non_llm_string_similarity</th>\n",
       "      <th>semantic_similarity</th>\n",
       "      <th>factual_correctness</th>\n",
       "      <th>llm_context_precision_without_reference</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>context_overlap</th>\n",
       "      <th>retrieved_context_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <td>0.453847</td>\n",
       "      <td>0.485661</td>\n",
       "      <td>0.500450</td>\n",
       "      <td>0.474112</td>\n",
       "      <td>0.884936</td>\n",
       "      <td>0.590248</td>\n",
       "      <td>0.870065</td>\n",
       "      <td>0.815762</td>\n",
       "      <td>0.788592</td>\n",
       "      <td>0.367593</td>\n",
       "      <td>0.739676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>embed</th>\n",
       "      <td>0.209238</td>\n",
       "      <td>0.306499</td>\n",
       "      <td>0.348447</td>\n",
       "      <td>0.320105</td>\n",
       "      <td>0.818746</td>\n",
       "      <td>0.474313</td>\n",
       "      <td>0.850586</td>\n",
       "      <td>0.735750</td>\n",
       "      <td>0.694046</td>\n",
       "      <td>0.150087</td>\n",
       "      <td>0.718541</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       bleu_score  rouge_score  meta_meteor_score  non_llm_string_similarity  \\\n",
       "token    0.453847     0.485661           0.500450                   0.474112   \n",
       "embed    0.209238     0.306499           0.348447                   0.320105   \n",
       "\n",
       "       semantic_similarity  factual_correctness  \\\n",
       "token             0.884936             0.590248   \n",
       "embed             0.818746             0.474313   \n",
       "\n",
       "       llm_context_precision_without_reference  context_recall  faithfulness  \\\n",
       "token                                 0.870065        0.815762      0.788592   \n",
       "embed                                 0.850586        0.735750      0.694046   \n",
       "\n",
       "       context_overlap  retrieved_context_similarity  \n",
       "token         0.367593                      0.739676  \n",
       "embed         0.150087                      0.718541  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "eval_fn = partial(\n",
    "    evaluate,\n",
    "    metrics=DEFAULT_ALL_METRICS,\n",
    "    llm=llm_wrapper,\n",
    "    embeddings=embedding_wrapper,\n",
    ")\n",
    "\n",
    "token_rag_results = ThreadedSequence(token_rag_chain).batch(baseline_qas)\n",
    "embed_rag_results = ThreadedSequence(embed_rag_chain).batch(baseline_qas)\n",
    "\n",
    "path = Path(f\".cache/{SEED}_wllmk_intermediate/\")\n",
    "if not path.exists():\n",
    "    path.mkdir()\n",
    "\n",
    "if not (path / \"token_res.pkl\").exists():\n",
    "    token_res = eval_fn(pipeline_results=token_rag_results)\n",
    "    pickle.dump(token_res, open(path / \"token_res.pkl\", \"wb\"))\n",
    "else:\n",
    "    token_res = pickle.load(open(path / \"token_res.pkl\", \"rb\"))\n",
    "\n",
    "if not (path / \"embed_res.pkl\").exists():\n",
    "    embed_res = eval_fn(pipeline_results=embed_rag_results)\n",
    "    pickle.dump(embed_res, open(path / \"embed_res.pkl\", \"wb\"))\n",
    "else:\n",
    "    embed_res = pickle.load(open(path / \"embed_res.pkl\", \"rb\"))\n",
    "\n",
    "res = WLLMKResult(token=token_res, embed=embed_res)\n",
    "res.save(f\"results/{SEED}_wllmk_results.pkl\")\n",
    "res.synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "\n",
    "#### Number of retries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of retried pipelines: 357\n",
      "Mean number of retries: 0.6994219653179191\n",
      "Failures: Counter({1: 351, 0: 162, 2: 6})\n"
     ]
    }
   ],
   "source": [
    "n_retried_pipelines = 0\n",
    "mean_retry = 0.0\n",
    "for r in embed_rag_results:\n",
    "    if \"failures\" in r:\n",
    "        if r[\"failures\"] > 0:\n",
    "            n_retried_pipelines += 1\n",
    "        mean_retry += r[\"failures\"]\n",
    "\n",
    "mean_retry /= len(embed_rag_results)\n",
    "\n",
    "print(f\"Number of retried pipelines: {n_retried_pipelines}\")\n",
    "print(f\"Mean number of retries: {mean_retry}\")\n",
    "\n",
    "failures = Counter([r[\"failures\"] for r in embed_rag_results if \"failures\" in r])\n",
    "\n",
    "print(f\"Failures: {failures}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augmented prompts vs. original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_qas = [\n",
    "    res | {\"user_input\": res[\"last_input\"][\"pipeline_input\"]}\n",
    "    for res in embed_rag_results\n",
    "    if res[\"failures\"] > 0\n",
    "]\n",
    "augmented_qas = sorted(augmented_qas, key=lambda x: x[\"id\"])\n",
    "reference_qas = sorted(ds.qas, key=lambda x: x[\"id\"])\n",
    "for a, b in zip(augmented_qas, reference_qas):\n",
    "    a[\"reference\"] = b[\"reference\"]\n",
    "    a[\"reference_contexts\"] = b[\"reference_contexts\"]\n",
    "    a[\"reference_context_ids\"] = b[\"reference_context_ids\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2085a73907b465a8d673826c26cd5d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/357 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "augmented_results = ThreadedSequence(rag_chain).batch(augmented_qas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e1e5a02f4b3431989f2e2f8565a3127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3927 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.executor:Exception raised in Job[41]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
      "ERROR:ragas.executor:Exception raised in Job[104]: TimeoutError()\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.executor:Exception raised in Job[181]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.executor:Exception raised in Job[313]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.executor:Exception raised in Job[492]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.executor:Exception raised in Job[514]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.executor:Exception raised in Job[522]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.executor:Exception raised in Job[745]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.executor:Exception raised in Job[885]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt statement_generator_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.executor:Exception raised in Job[1031]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.executor:Exception raised in Job[953]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt claim_decomposition_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.executor:Exception raised in Job[1336]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.executor:Exception raised in Job[1317]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.executor:Exception raised in Job[1325]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.executor:Exception raised in Job[1306]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt claim_decomposition_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.executor:Exception raised in Job[1479]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.executor:Exception raised in Job[1481]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.executor:Exception raised in Job[1580]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt claim_decomposition_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.executor:Exception raised in Job[1578]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.executor:Exception raised in Job[1788]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.executor:Exception raised in Job[1767]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.executor:Exception raised in Job[2053]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt claim_decomposition_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.executor:Exception raised in Job[2051]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt claim_decomposition_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.executor:Exception raised in Job[2249]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.executor:Exception raised in Job[2329]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.executor:Exception raised in Job[2425]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.executor:Exception raised in Job[2427]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.executor:Exception raised in Job[2524]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
      "ERROR:ragas.executor:Exception raised in Job[2493]: TimeoutError()\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.executor:Exception raised in Job[2867]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.executor:Exception raised in Job[2986]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.executor:Exception raised in Job[2832]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.executor:Exception raised in Job[3043]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.executor:Exception raised in Job[2953]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.executor:Exception raised in Job[2964]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
      "ERROR:ragas.executor:Exception raised in Job[3109]: TimeoutError()\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.executor:Exception raised in Job[3472]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt context_recall_classification_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.executor:Exception raised in Job[3736]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt n_l_i_statement_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.executor:Exception raised in Job[3624]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt context_precision_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.executor:Exception raised in Job[3922]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.prompt.pydantic_prompt:Prompt claim_decomposition_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
      "ERROR:ragas.executor:Exception raised in Job[3855]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n"
     ]
    }
   ],
   "source": [
    "if not (path / \"augmented_res.pkl\").exists():\n",
    "    augmented_res = eval_fn(pipeline_results=augmented_results)\n",
    "    pickle.dump(augmented_res, open(path / \"augmented_res.pkl\", \"wb\"))\n",
    "else:\n",
    "    augmented_res = pickle.load(open(path / \"augmented_res.pkl\", \"rb\"))\n",
    "\n",
    "augmented_evals = WLLMKResult(augmented=augmented_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "b_evals = baseline_evals.details[\n",
    "    baseline_evals.details[\"reference\"].isin(augmented_evals.details[\"reference\"])\n",
    "    & baseline_evals.details[\"reference_contexts\"].isin(\n",
    "        augmented_evals.details[\"reference_contexts\"]\n",
    "    )\n",
    "]\n",
    "\n",
    "b_synthesis = {k: b_evals[k].mean() for k in baseline_evals.synthesis.keys()}\n",
    "\n",
    "augment_eval_res = WLLMKResult()\n",
    "augment_eval_res.synthesis = pd.DataFrame(\n",
    "    [\n",
    "        augmented_evals.synthesis,\n",
    "        b_synthesis,\n",
    "    ],\n",
    "    index=[\"baseline\", \"augmented\"],\n",
    ")\n",
    "augment_eval_res.details = {\n",
    "    \"baseline\": b_evals,\n",
    "    \"augmented\": augmented_evals.details,\n",
    "}\n",
    "augment_eval_res.save(f\"results/{SEED}_augmented_results.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bleu_score</th>\n",
       "      <th>rouge_score</th>\n",
       "      <th>meta_meteor_score</th>\n",
       "      <th>non_llm_string_similarity</th>\n",
       "      <th>semantic_similarity</th>\n",
       "      <th>factual_correctness</th>\n",
       "      <th>llm_context_precision_without_reference</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>context_overlap</th>\n",
       "      <th>retrieved_context_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>0.015783</td>\n",
       "      <td>0.074993</td>\n",
       "      <td>0.106471</td>\n",
       "      <td>0.166407</td>\n",
       "      <td>0.561388</td>\n",
       "      <td>0.278787</td>\n",
       "      <td>0.868779</td>\n",
       "      <td>0.145836</td>\n",
       "      <td>0.728232</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.731044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>augmented</th>\n",
       "      <td>0.119863</td>\n",
       "      <td>0.248121</td>\n",
       "      <td>0.334363</td>\n",
       "      <td>0.257913</td>\n",
       "      <td>0.767473</td>\n",
       "      <td>0.539403</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.696968</td>\n",
       "      <td>0.791642</td>\n",
       "      <td>0.072829</td>\n",
       "      <td>0.739267</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           bleu_score  rouge_score  meta_meteor_score  \\\n",
       "baseline     0.015783     0.074993           0.106471   \n",
       "augmented    0.119863     0.248121           0.334363   \n",
       "\n",
       "           non_llm_string_similarity  semantic_similarity  \\\n",
       "baseline                    0.166407             0.561388   \n",
       "augmented                   0.257913             0.767473   \n",
       "\n",
       "           factual_correctness  llm_context_precision_without_reference  \\\n",
       "baseline              0.278787                                 0.868779   \n",
       "augmented             0.539403                                 0.888889   \n",
       "\n",
       "           context_recall  faithfulness  context_overlap  \\\n",
       "baseline         0.145836      0.728232         0.000700   \n",
       "augmented        0.696968      0.791642         0.072829   \n",
       "\n",
       "           retrieved_context_similarity  \n",
       "baseline                       0.731044  \n",
       "augmented                      0.739267  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augment_eval_res.synthesis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>bleu_score</th>\n",
       "      <th>rouge_score</th>\n",
       "      <th>meta_meteor_score</th>\n",
       "      <th>non_llm_string_similarity</th>\n",
       "      <th>semantic_similarity</th>\n",
       "      <th>factual_correctness</th>\n",
       "      <th>llm_context_precision_without_reference</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>context_overlap</th>\n",
       "      <th>retrieved_context_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What does the acronym PSI stand for in the con...</td>\n",
       "      <td>[## 2.2 Fine-Tuning For Downstream Tasks\\n\\nAf...</td>\n",
       "      <td>[## Appendix\\n\\n       |                      ...</td>\n",
       "      <td>In the provided context, there is no mention o...</td>\n",
       "      <td>Product Substitute Identification</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.084084</td>\n",
       "      <td>0.477198</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.726919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What does the term \"recall\" specifically denot...</td>\n",
       "      <td>[Recall, also known as Sensitivity or True Pos...</td>\n",
       "      <td>[## 1 Introduction\\n\\n Llama 2 [TMS+23] and Fa...</td>\n",
       "      <td>In anomaly detection, recall, also known as Tr...</td>\n",
       "      <td>The method demonstrated to be highly performat...</td>\n",
       "      <td>0.033008</td>\n",
       "      <td>0.078431</td>\n",
       "      <td>0.127953</td>\n",
       "      <td>0.167614</td>\n",
       "      <td>0.622888</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.691295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Was the conference on knowledge discovery and ...</td>\n",
       "      <td>[## C.2 Precision And Recall\\n\\niled: the most...</td>\n",
       "      <td>[## V. Limitations And Future Work\\n\\n        ...</td>\n",
       "      <td>No, the lines were spoken by different people.</td>\n",
       "      <td>I believe it holds significant promise for enh...</td>\n",
       "      <td>0.011150</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.061475</td>\n",
       "      <td>0.170455</td>\n",
       "      <td>0.346355</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.698202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are your thoughts on the effectiveness of...</td>\n",
       "      <td>[## 1 Introduction\\n\\n for response generation...</td>\n",
       "      <td>[## E. Analysis On Base Models\\n\\nWe compare t...</td>\n",
       "      <td>The RS-DPO method is effective in aligning lar...</td>\n",
       "      <td>I believe instruction tuning is immensely cruc...</td>\n",
       "      <td>0.023587</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.193322</td>\n",
       "      <td>0.263889</td>\n",
       "      <td>0.635889</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.749425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What allows for the highly efficient generativ...</td>\n",
       "      <td>[Deploying large language models (LLMs) is alm...</td>\n",
       "      <td>[Recall, also known as Sensitivity or True Pos...</td>\n",
       "      <td>The study suggests that the highly efficient g...</td>\n",
       "      <td>Recall measures the proportion of actual posit...</td>\n",
       "      <td>0.012091</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>0.094787</td>\n",
       "      <td>0.189266</td>\n",
       "      <td>0.550496</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.763315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>What specifically influenced the specification...</td>\n",
       "      <td>[Reinforcement Learning from Human Feedback (R...</td>\n",
       "      <td>[## C.5 Kendall'S Tau Correlation Coefficient ...</td>\n",
       "      <td>The provided context does not explicitly menti...</td>\n",
       "      <td>I believe that Kendall's Tau correlation coeff...</td>\n",
       "      <td>0.032342</td>\n",
       "      <td>0.102190</td>\n",
       "      <td>0.134875</td>\n",
       "      <td>0.247649</td>\n",
       "      <td>0.593588</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.747547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>Is the recently implemented update significant...</td>\n",
       "      <td>[|         |                                  ...</td>\n",
       "      <td>[## 3.2 Experimental Setup\\n\\nWe start our exp...</td>\n",
       "      <td>Yes, the update significantly enhances graphic...</td>\n",
       "      <td>The specific reasons for choosing the Open Ass...</td>\n",
       "      <td>0.002636</td>\n",
       "      <td>0.056338</td>\n",
       "      <td>0.042735</td>\n",
       "      <td>0.196581</td>\n",
       "      <td>0.553615</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.706815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>What is the detailed process to devise a strat...</td>\n",
       "      <td>[## System Answer The Following Questions As B...</td>\n",
       "      <td>[115\\n150\\n184\\nDataset Size (Billion Tokens)\\...</td>\n",
       "      <td>To devise a strategy that may potentially lead...</td>\n",
       "      <td>I don't know. Because the text does not explai...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.232198</td>\n",
       "      <td>0.562109</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.728312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>What specifically inspired the in-depth resear...</td>\n",
       "      <td>[## Abstract\\n\\nThis paper considers the chall...</td>\n",
       "      <td>[## C.5 Kendall'S Tau Correlation Coefficient ...</td>\n",
       "      <td>The in-depth research on probabilistic reasoni...</td>\n",
       "      <td>I don't know. Because the text only provides s...</td>\n",
       "      <td>0.015394</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.179104</td>\n",
       "      <td>0.205556</td>\n",
       "      <td>0.579088</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.718475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>What is the calculated result of the Cross-Ent...</td>\n",
       "      <td>[We provide the proof of Lemma 3.3  \\nProof. T...</td>\n",
       "      <td>[## 2.1 Construction Process\\n\\nmedium. A conc...</td>\n",
       "      <td>The provided example does not explicitly calcu...</td>\n",
       "      <td>The aggregation of final answers is enabled by...</td>\n",
       "      <td>0.008518</td>\n",
       "      <td>0.141176</td>\n",
       "      <td>0.129683</td>\n",
       "      <td>0.263975</td>\n",
       "      <td>0.624573</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.742126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>357 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            user_input  \\\n",
       "0    What does the acronym PSI stand for in the con...   \n",
       "1    What does the term \"recall\" specifically denot...   \n",
       "2    Was the conference on knowledge discovery and ...   \n",
       "3    What are your thoughts on the effectiveness of...   \n",
       "4    What allows for the highly efficient generativ...   \n",
       "..                                                 ...   \n",
       "352  What specifically influenced the specification...   \n",
       "353  Is the recently implemented update significant...   \n",
       "354  What is the detailed process to devise a strat...   \n",
       "355  What specifically inspired the in-depth resear...   \n",
       "356  What is the calculated result of the Cross-Ent...   \n",
       "\n",
       "                                    retrieved_contexts  \\\n",
       "0    [## 2.2 Fine-Tuning For Downstream Tasks\\n\\nAf...   \n",
       "1    [Recall, also known as Sensitivity or True Pos...   \n",
       "2    [## C.2 Precision And Recall\\n\\niled: the most...   \n",
       "3    [## 1 Introduction\\n\\n for response generation...   \n",
       "4    [Deploying large language models (LLMs) is alm...   \n",
       "..                                                 ...   \n",
       "352  [Reinforcement Learning from Human Feedback (R...   \n",
       "353  [|         |                                  ...   \n",
       "354  [## System Answer The Following Questions As B...   \n",
       "355  [## Abstract\\n\\nThis paper considers the chall...   \n",
       "356  [We provide the proof of Lemma 3.3  \\nProof. T...   \n",
       "\n",
       "                                    reference_contexts  \\\n",
       "0    [## Appendix\\n\\n       |                      ...   \n",
       "1    [## 1 Introduction\\n\\n Llama 2 [TMS+23] and Fa...   \n",
       "2    [## V. Limitations And Future Work\\n\\n        ...   \n",
       "3    [## E. Analysis On Base Models\\n\\nWe compare t...   \n",
       "4    [Recall, also known as Sensitivity or True Pos...   \n",
       "..                                                 ...   \n",
       "352  [## C.5 Kendall'S Tau Correlation Coefficient ...   \n",
       "353  [## 3.2 Experimental Setup\\n\\nWe start our exp...   \n",
       "354  [115\\n150\\n184\\nDataset Size (Billion Tokens)\\...   \n",
       "355  [## C.5 Kendall'S Tau Correlation Coefficient ...   \n",
       "356  [## 2.1 Construction Process\\n\\nmedium. A conc...   \n",
       "\n",
       "                                              response  \\\n",
       "0    In the provided context, there is no mention o...   \n",
       "1    In anomaly detection, recall, also known as Tr...   \n",
       "2       No, the lines were spoken by different people.   \n",
       "3    The RS-DPO method is effective in aligning lar...   \n",
       "4    The study suggests that the highly efficient g...   \n",
       "..                                                 ...   \n",
       "352  The provided context does not explicitly menti...   \n",
       "353  Yes, the update significantly enhances graphic...   \n",
       "354  To devise a strategy that may potentially lead...   \n",
       "355  The in-depth research on probabilistic reasoni...   \n",
       "356  The provided example does not explicitly calcu...   \n",
       "\n",
       "                                             reference  bleu_score  \\\n",
       "0                    Product Substitute Identification    0.000000   \n",
       "1    The method demonstrated to be highly performat...    0.033008   \n",
       "2    I believe it holds significant promise for enh...    0.011150   \n",
       "3    I believe instruction tuning is immensely cruc...    0.023587   \n",
       "4    Recall measures the proportion of actual posit...    0.012091   \n",
       "..                                                 ...         ...   \n",
       "352  I believe that Kendall's Tau correlation coeff...    0.032342   \n",
       "353  The specific reasons for choosing the Open Ass...    0.002636   \n",
       "354  I don't know. Because the text does not explai...    0.000000   \n",
       "355  I don't know. Because the text only provides s...    0.015394   \n",
       "356  The aggregation of final answers is enabled by...    0.008518   \n",
       "\n",
       "     rouge_score  meta_meteor_score  non_llm_string_similarity  \\\n",
       "0       0.000000           0.000000                   0.084084   \n",
       "1       0.078431           0.127953                   0.167614   \n",
       "2       0.060606           0.061475                   0.170455   \n",
       "3       0.111111           0.193322                   0.263889   \n",
       "4       0.085714           0.094787                   0.189266   \n",
       "..           ...                ...                        ...   \n",
       "352     0.102190           0.134875                   0.247649   \n",
       "353     0.056338           0.042735                   0.196581   \n",
       "354     0.042553           0.060606                   0.232198   \n",
       "355     0.142857           0.179104                   0.205556   \n",
       "356     0.141176           0.129683                   0.263975   \n",
       "\n",
       "     semantic_similarity  factual_correctness  \\\n",
       "0               0.477198                 0.75   \n",
       "1               0.622888                 0.00   \n",
       "2               0.346355                 0.00   \n",
       "3               0.635889                 0.14   \n",
       "4               0.550496                 0.00   \n",
       "..                   ...                  ...   \n",
       "352             0.593588                 0.40   \n",
       "353             0.553615                 0.00   \n",
       "354             0.562109                 0.00   \n",
       "355             0.579088                 0.86   \n",
       "356             0.624573                 0.00   \n",
       "\n",
       "     llm_context_precision_without_reference  context_recall  faithfulness  \\\n",
       "0                                   1.000000        0.000000      1.000000   \n",
       "1                                   1.000000        0.666667      1.000000   \n",
       "2                                   0.000000        0.000000      0.000000   \n",
       "3                                   1.000000        0.000000           NaN   \n",
       "4                                   1.000000        0.000000      0.833333   \n",
       "..                                       ...             ...           ...   \n",
       "352                                 1.000000        0.000000      0.428571   \n",
       "353                                 0.750000        0.000000      0.000000   \n",
       "354                                 0.916667        0.000000      0.000000   \n",
       "355                                 0.805556        0.000000      1.000000   \n",
       "356                                      NaN        0.000000      0.250000   \n",
       "\n",
       "     context_overlap  retrieved_context_similarity  \n",
       "0                0.0                      0.726919  \n",
       "1                0.0                      0.691295  \n",
       "2                0.0                      0.698202  \n",
       "3                0.0                      0.749425  \n",
       "4                0.0                      0.763315  \n",
       "..               ...                           ...  \n",
       "352              0.0                      0.747547  \n",
       "353              0.0                      0.706815  \n",
       "354              0.0                      0.728312  \n",
       "355              0.0                      0.718475  \n",
       "356              0.0                      0.742126  \n",
       "\n",
       "[357 rows x 16 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augment_eval_res.details[\"augmented\"]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
