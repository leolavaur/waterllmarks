{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM and RAG testing with Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The team was granted access for the photoshoot at Blenheim Palace by the event coordinator.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "\n",
    "# client = OpenAI()\n",
    "client = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "message = \"Who granted the team access for the photoshoot at Blenheim Palace?\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"message\"],\n",
    "    template=\"\"\"You are a helpful assistant.\n",
    "    \n",
    "    Question: {message}\n",
    "    \n",
    "    Answer:\"\"\",\n",
    ")\n",
    "\n",
    "chain = prompt | client\n",
    "response = chain.invoke({\"message\": message, \"toto\": 1})\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8576 520\n",
      "{'doc_id': '6f86094c-47fe-43de-a77a-e8c34c69c997', 'contents': \"# Rag-Driver: Generalisable Driving Explanations With Retrieval-Augmented In-Context Learning In Multi-Modal Large Language Model\\n\\nJianhao Yuan1, Shuyang Sun1, Daniel Omeiza1, Bo Zhao2, Paul Newman1, Lars Kunze1, Matthew Gadd1\\n1 University of Oxford 2 Beijing Academy of Artificial Intelligence\\n{jianhaoyuan,kevinsun,daniel,pnewman,lars,mattgadd}@robots.ox.ac.uk  \\nAbstract—Robots powered by 'blackbox' models need to provide\\nhuman-understandable explanations which we can trust. Hence,\\nexplainability plays a critical role in trustworthy autonomous\\ndecision-making to foster transparency and acceptance among\\nend users, especially in complex autonomous driving. Recent\\nadvancements in Multi-Modal Large Language models (MLLMs)\\nhave shown promising potential in enhancing the explainability\\nas a driving agent by producing control predictions along with\\nnatural language explanations. However, severe data scarcity\\ndue to expensive annotation costs and significant domain gaps\\nbetween different datasets makes the development of a robust and\\ngeneralisable system an extremely challenging task. Moreover, the\\nprohibitively expensive training requirements of MLLM and the\\nunsolved problem of catastrophic forgetting further limit their\\ngeneralisability post-deployment. To address these challenges, we\\npresent RAG-Driver, a novel retrieval-augmented multi-modal\\nlarge language model that leverages in-context learning for high-\\nperformance, explainable, and generalisable autonomous driving.\\nBy grounding in retrieved expert demonstration, we empirically\\nvalidate that RAG-Driver achieves state-of-the-art performance in\\nproducing driving action explanations, justifications, and control\\nsignal prediction. More importantly, it exhibits exceptional zero-\\nshot generalisation capabilities to unseen environments without  \\nfurther training endeavours1.\\nIndex Terms—Autonomous driving, multi-modal language\\nmodel, end-to-end driving, domain generalisation\", 'metadata': {'creation_datetime': '2024-03-04', 'file_name': '2402.10828v1.md', 'file_path': 'paper_data/2402.10828v1.md', 'file_size': 64885, 'file_type': None, 'last_accessed_datetime': '2024-03-04', 'last_modified_datetime': '2024-02-22'}}\n",
      "# Rag-Driver: Generalisable Driving Explanations With Retrieval-Augmented In-Context Learning In Multi-Modal Large Language Model\n",
      "\n",
      "Jianhao Yuan1, Shuyang Sun1, Daniel Omeiza1, Bo Zhao2, Paul Newman1, Lars Kunze1, Matthew Gadd1\n",
      "1 University of Oxford 2 Beijing Academy of Artificial Intelligence\n",
      "{jianhaoyuan,kevinsun,daniel,pnewman,lars,mattgadd}@robots.ox.ac.uk  \n",
      "Abstract—Robots powered by 'blackbox' models need to provide\n",
      "human-understandable explanations which we can trust. Hence,\n",
      "explainability plays a critical role in trustworthy autonomous\n",
      "decision-making to foster transparency and acceptance among\n",
      "end users, especially in complex autonomous driving. Recent\n",
      "advancements in Multi-Modal Large Language models (MLLMs)\n",
      "have shown promising potential in enhancing the explainability\n",
      "as a driving agent by producing control predictions along with\n",
      "natural language explanations. However, severe data scarcity\n",
      "due to expensive annotation costs and significant domain gaps\n",
      "between different datasets makes the development of a robust and\n",
      "generalisable system an extremely challenging task. Moreover, the\n",
      "prohibitively expensive training requirements of MLLM and the\n",
      "unsolved problem of catastrophic forgetting further limit their\n",
      "generalisability post-deployment. To address these challenges, we\n",
      "present RAG-Driver, a novel retrieval-augmented multi-modal\n",
      "large language model that leverages in-context learning for high-\n",
      "performance, explainable, and generalisable autonomous driving.\n",
      "By grounding in retrieved expert demonstration, we empirically\n",
      "validate that RAG-Driver achieves state-of-the-art performance in\n",
      "producing driving action explanations, justifications, and control\n",
      "signal prediction. More importantly, it exhibits exceptional zero-\n",
      "shot generalisation capabilities to unseen environments without  \n",
      "further training endeavours1.\n",
      "Index Terms—Autonomous driving, multi-modal language\n",
      "model, end-to-end driving, domain generalisation\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "corpus = load_dataset(\"MarkrAI/AutoRAG-evaluation-2024-LLM-paper-v1\", \"corpus\")\n",
    "qa = load_dataset(\"MarkrAI/AutoRAG-evaluation-2024-LLM-paper-v1\", \"qa\")\n",
    "\n",
    "print(len(corpus[\"train\"]), len(qa[\"train\"]))\n",
    "print(corpus[\"train\"][0])\n",
    "print(corpus[\"train\"][0][\"contents\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embedding_fn = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "query = \"Hello, how are you today?\"\n",
    "\n",
    "embedded_query = embedding_fn.embed_query(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "\n",
    "docs = corpus[\"train\"][\"contents\"]\n",
    "ids = corpus[\"train\"][\"doc_id\"]\n",
    "\n",
    "chroma_client = Chroma(embedding_function=embedding_fn)\n",
    "for i in range(0, len(docs), 1000):\n",
    "    chroma_client.add_texts(texts=docs[i : i + 1000], ids=ids[i : i + 1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='f06e8b7c-92db-4a33-a5f2-99d440aa0760', metadata={}, page_content=\"# How Secure Are Large Language Models (Llms) For Navigation In Urban Environments?\\n## B. Navigational Prompt Suffix (Nps) Attack\\n\\naMa and VELMA-LLaMa2, specifically examining their performance before and after the attack. On the Touchdown dataset, VELMA-LLaMa and VELMA-LLaMa2 both experienced an increase in their SPD metrics, by 9.09% and 4.65% over their baseline values, respectively. At the same time, both models demonstrated marked declines in KPA from their baseline results, with VELMA-LLaMa experiencing a decrease of 69.13% and VELMA-LLaMa2 a decrease of 57.72%. On the Map2seq dataset, both VELMA-LLaMa and VELMA- LLaMa2 exhibited significant increases in SPD, with VELMA- LLaMa showing a 13.35% increase and VELMA-LLaMa2 experiencing a 14.44% increase. Furthermore, the KPA of VELMA-LLaMa decreased by 43.67% from its baseline, while VELMA-LLaMa2 showed a reduction of 64.35%. These outcomes underscore the significant vulnerability of VELMA- LLaMa and VELMA-LLaMa2 to NPS attacks, as evidenced by their considerable deviations from baseline performance.  \\nIn the fine-tuning experiments, the models VELMA-FT and VELMA-RBL, after fine-tuning, showed improvements over their baseline across multiple metrics. Yet, their performance significantly deteriorated under attack conditions. On the touchdown dataset, VELMA-FT's performance was notably affected, with its SPD rising by 56.91% from the original baseline, while VELMA-RBL's SPD showed an even more substantial increase of 85.26%. For KPA, VELMA-FT experienced a decrease of 43.80%, and VELMA-RBL faced a more drastic reduction of 71.14%. Additionally, after the attack, VELMA-FT and VELMA-RBL each experienced significant decreases in TC compared to their baseline performances, with decreases of 62.24% and 84.08%, respectively. On the Map2seq dataset, both models also experienced significant performance declines from their original baselines, similar to their earlier results on the Touchdown dataset. The SPD of VELMA-FT increased significantly, showing a performance degradation of 259.38% from its baseline, whereas VELMA- RBL's SPD worsened by 132.97%. Additionally, the\"),\n",
       " Document(id='2a9bd807-7fcd-4d60-adbd-463bebc3d40a', metadata={}, page_content=\"# How Secure Are Large Language Models (Llms) For Navigation In Urban Environments?\\n## B. Navigational Prompt Suffix (Nps) Attack\\n\\nBL faced a more drastic reduction of 71.14%. Additionally, after the attack, VELMA-FT and VELMA-RBL each experienced significant decreases in TC compared to their baseline performances, with decreases of 62.24% and 84.08%, respectively. On the Map2seq dataset, both models also experienced significant performance declines from their original baselines, similar to their earlier results on the Touchdown dataset. The SPD of VELMA-FT increased significantly, showing a performance degradation of 259.38% from its baseline, whereas VELMA- RBL's SPD worsened by 132.97%. Additionally, the KPA of VELMA-FT dropped by 64.67%, and VELMA-RBL's KPA decreased by 58.00%. Similarly, the TC for VELMA-FT and VELMA-RBL fell drastically, by 90.13% and 73.18% from their baselines, respectively. These results underscore the significant decrease in navigational performance due to NPS attacks, confirming the susceptibility of LLM-based navigation models to such threats, which can be exploited by simply appending attack-generated suffixes to the input prompts.  \\nb) *Black-Box Attacks*: In the above section, we analyzed how suffixes generated by attacking the source LLM can significantly reduce the performance of navigation models that use the same LLM for reasoning. However, in practical application scenarios, we cannot know which LLM a navigation model is based on. Therefore, this section discusses whether suffixes generated by attacking other LLMs can impact navigation models that use different LLMs from the attacked ones.  \\nFirstly, by analyzing the navigational outcomes of VELMA-\\nGPT3 with suffixes generated from attacking LLaMa and Vicuna added to the original navigation instructions, it is observed that suffixes generated by attacking two different large language models (LLMs) can decrease the navigational performance of VELMA-GPT3 on both datasets. Specifically, on the Touchdown dataset, these two suffixes led to reductions in VELMA-GPT3's SPD, KPA, and TC by 23.87%, 49.49%, and 55.88%, as well as 19.82%, 48.47%, and 33.82% respectively. On the Map2Seq dataset, the impact was similarly detrimental, with VELMA-GPT3 experiencing decreases of 11.27% and 12.21% in SPD, 38\"),\n",
       " Document(id='5ffb59d5-54da-4aa6-a561-680ebe8c3560', metadata={}, page_content=\"# How Secure Are Large Language Models (Llms) For Navigation In Urban Environments?\\n## B. Navigational Prompt Suffix (Nps) Attack\\n\\n two different large language models (LLMs) can decrease the navigational performance of VELMA-GPT3 on both datasets. Specifically, on the Touchdown dataset, these two suffixes led to reductions in VELMA-GPT3's SPD, KPA, and TC by 23.87%, 49.49%, and 55.88%, as well as 19.82%, 48.47%, and 33.82% respectively. On the Map2Seq dataset, the impact was similarly detrimental, with VELMA-GPT3 experiencing decreases of 11.27% and 12.21% in SPD, 38.31% and 18.83% in KPA, and 53.33% and 40.00% in TC. Notably, suffixes from LLaMa attacks had a more pronounced effect than those from Vicuna.  \\nSubsequently, we assessed the performance of GPT-4 as a navigation reasoner, which exhibits significant improvements in language understanding, generation, and reasoning over its predecessor, GPT-3. Considering the substantial costs associated with invoking the GPT-4 API for comparison, we only verified the navigational outcomes of VELMA-GPT4 on the Touchdown dataset after being subjected to attacks. Our investigation, as shown in Table I, indicates that even GPT-4, despite its state-of-the-art performance, is susceptible to declines in navigation model performance when faced with attacks. Specifically, compared to the original model, the VELMA- GPT4 model after attacking experienced a slight increase in the SPD metric by 1.83% but saw significant decreases in KPA by 54.01% and TC by 62.00% on the Touchdown dataset. Similarly, in the context of 2-shot learning, under black-box attacks, VELMA-LLaMa and VELMA-LLaMa2 experienced declines in KPA by 63.76% and 55.83% respectively on the Touchdown dataset, and by 37.87% and 58.10% respectively on the Map2Seq dataset.  \\nFor the experiments on finetuning, it was observed that although black-box attacks do not perform as effectively as white-box attacks, they nevertheless resulted in substantial declines in navigational accuracy. Specifically, VELMA-FT on the Touchdown dataset experienced a 35.11% increase in SPD, with reductions of 43.23% in KPA and 51.42% in TC.\"),\n",
       " Document(id='2fea1183-5f86-44f3-aa0d-c41a67294f64', metadata={}, page_content='# How Secure Are Large Language Models (Llms) For Navigation In Urban Environments?\\n## B. Navigational Prompt Suffix (Nps) Attack\\n\\n                   |           |\\n| 53.70%                            | )         |\\n| 11.5                              |           |\\n| (                                 |           |\\n| ↓                                 |           |\\n| 72.94%                            | )         |  \\noriginal values. For SPD, where lower values are better, our attack caused an increase. For KPA and TC, where higher values are preferable, our attack led to a decrease.  \\na) *White-Box Attacks*: In Table I, the baseline performances of VELMA-LLaMa and VELMA-LLaMa2, applied directly in 2-shot in-context learning without prior training, showcased low TC accuracy in navigational outcomes, not surpassing 4% accuracy. Therefore, our analysis focuses on comparing the SPD and KPA metrics of VELMA-LLaMa and VELMA-LLaMa2, specifically examining their performance before and after the attack. On the Touchdown dataset, VELMA-LLaMa and VELMA-LLaMa2 both experienced an increase in their SPD metrics, by 9.09% and 4.65% over their baseline values, respectively. At the same time, both models demonstrated marked declines in KPA from their baseline results, with VELMA-LLaMa experiencing a decrease of 69.13% and VELMA-LLaMa2 a decrease of 57.72%. On the Map2seq dataset, both'),\n",
       " Document(id='a432390e-feab-4cd8-ad58-596f8d35d299', metadata={}, page_content='# How Secure Are Large Language Models (Llms) For Navigation In Urban Environments?\\n## B. Navigational Prompt Suffix (Nps) Attack\\n\\n, both based on LLaMa-7b, using all training instances of their respective datasets. In our experiments, we primarily generated suffixes based on two LLMs, LLaMa [35] and Vicuna [4], and applied these generated suffixes to the input prompt of aforementioned models. Depending on whether the suffix generated for the targeted LLM model is applied to the original LLM model or other LLM models, we categorized the attacks into white box attacks and black box attacks. That is, applying a suffix generated by LLaMa to a model using LLaMa for reasoning is considered a white box attack, while applying a suffix generated by LLaMa to models using other non-LLaMa LLMs is considered a black box attack. The same applies to suffixes generated by Vicuna.  \\nTable I shows the results of VELMA models with GPT-\\n3, LLaMa, and LLaMa2 acting as reasoners in 2-shot incontext learning and the results of VELMA-FT and VELMA- RBL in finetuning. Unmarked results represent the original outcomes, † represents results with suffixes generated by the NPS attack applied to LLaMa in the input prompt, and ‡\\nrepresents the results with suffixes generated by the NPS attack applied to Vicuna in the input prompt. ◦ indicates whitebox attacks, and - indicates black-box attacks. It is observed that adversarial prompt suffixes derived from different LLMs reduced navigational accuracy to some extent. To more clearly discern the effects before and after the attack, we calculated the percentage change of the post-attack values relative to the  \\n| Touchdown                         | Map2seq   |\\n|-----------------------------------|-----------|\\n| LLM Models                        | SPD       |\\n| ↓                                 |           |\\n| KPA                               |  ')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = chroma_client.as_retriever(search_kwargs={\"k\": 5})\n",
    "retriever.invoke(\n",
    "    \"What is the performance percentage increase observed in the navigational prompt suffix attack scenario when using VELMA-FT?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Does the Turing Test assess a machine's ability to exhibit intelligent behavior equivalent to that of a human?\n",
      "1 Is the oa_temp or the zone_occ the most impactful feature according to the Shapley values?\n",
      "2 What is the performance percentage increase observed in the navigational prompt suffix attack scenario when using VELMA-FT?\n",
      "3 What are essential components of evaluating large language models (LLMs)?\n",
      "4 How many errors were there in the Inference phase as per the document?\n",
      "5 What is the meaning of PLP-former in the context of this text?\n",
      "6 How does the Qmsum value vary between the first and the third entries?\n",
      "7 What is the purpose of using the torch.einsum function in the provided text?\n",
      "8 What is the result of applying the PAL attack against GPT-3.5-Turbo?\n",
      "9 What is the process to deceive the test function as described?\n",
      "10 What leads to a greater decrease in performance in deploying LLMs/VLMs in robotics according to the experiment?\n",
      "11 Why has the accuracy of the models not reached 100% after adjusting and reordering steps?\n",
      "12 How significant do you believe the challenge of proper prompt engineering is in accurately evaluating artificial intelligence models?\n",
      "13 Is P7 a beginner who slightly disagrees with the effectiveness of LLM-powered editing functions for presentations?\n",
      "14 Is the Kendall's Tau correlation coefficient for Figure 5 .18 or .20?\n",
      "15 In the HotPotQA dataset, which model variation of HGOT achieved the highest EM score in the \"Medium\" category, and how did it compare to the second-best performers?\n",
      "16 What types of attacks are Large Language Models (LLMs) vulnerable to in urban navigation?\n",
      "17 What mathematical formula is used to formalize the jointable attack?\n",
      "18 What does \"Perception Attack\" mean in the context of robotics?\n",
      "19 How similar are ProbLog in-context examples and ProbLog queries in teaching the syntax and structure necessary for writing ProbLog code?\n",
      "20 What method boosts the performance of MLLM in explainable driving without further training effort?\n",
      "21 What is the result of having all sides of a quadrilateral equal to 5 units?\n",
      "22 What is the procedure to underscore possible causes of agent behaviors in LLM-Based Autonomous Systems?\n",
      "23 What enables the improvement of diagnostic accuracy by combining insights from multiple large language models?\n",
      "24 Why has Syntaxshap not been widely adopted in all text generation applications?\n",
      "25 How important do you consider the identification and optimization of a steering vector in unifying causal representation learning with foundation models?\n",
      "26 Is the text providing detailed results of an experiment related to LLMs-based navigation security?\n",
      "27 Is the evaluation based on cosine similarity or exact data replication?\n",
      "28 What are the main contributions of the research on the butterfly effect of model editing as mentioned in the introduction?\n",
      "29 What are the components of Agentlens's visual analysis for studying agent behaviors in LLM-based autonomous systems?\n",
      "30 What is the highest percentage result reported?\n",
      "31 What does \"MMs\" stand for in the context of evaluating models on geometry problem-solving?\n",
      "32 How does the FBank acoustic feature compare to Whisper-tiny in terms of model size?\n",
      "33 What does the term \"Longheads\" refer to in the context of this document?\n",
      "34 What is the result of the Example Generation on Forget Set?\n",
      "35 What is the process to interpret Kendall's Tau Correlation Coefficient for Figure 5 in relation to cross-lingual vocabulary adaptation efficiency?\n",
      "36 What makes Hgot useful for retrieval-augmented in-context learning in factuality evaluation?\n",
      "37 Why has the model editing not led to the permanent solution for the collapse of large language models?\n",
      "38 How do you assess the significance of Kendall's Tau Correlation Coefficient for evaluating cross-lingual vocabulary adaptation in generative models?\n",
      "39 Does the model primarily use the graph to generate responses?\n",
      "40 Is the focal point Turing patterns or fractal geometry?\n",
      "41 What is the source of the prompts for the HPSv2 dataset used in the ablation study?\n",
      "42 What are the datasets mentioned in the detailed results?\n",
      "43 What metrics are used to benchmark the TOPGUN approach in tool planning?\n",
      "44 What is the meaning of AgentEval in the context of LLM-powered applications?\n",
      "45 How does the effectiveness of the new LLM-driven discrete prompt optimization framework compare to human-engineered prompts in multi-step tasks?\n",
      "46 What is the purpose of mixing sample and token level loss in the described ablation study?\n",
      "47 What is the result of analyzing the given Qmsum data?\n",
      "48 What is the procedure to minimize academic procrastination according to the study?\n",
      "49 What allows the indexing and categorization of these digital media artifacts?\n",
      "50 Why is there no explanation on how the backward and forward passes affect the model's understanding of embeddings in few-shot prompts in the text?\n",
      "51 How important do you think is genuine reasoning in AI development compared to technical optimization?\n",
      "52 Is the subject related to the acceleration of lightweight models on the edge?\n",
      "53 Is the Transformer model initially used for machine translation or facial recognition?\n",
      "54 What are the two steps to be followed in responding to a prompt after receiving potentially applicable feedback according to the instructions?\n",
      "55 What are the datasets used for natural language inference, commonsense, and sentiment analysis in the training and evaluation processes?\n",
      "56 How many countries do the Andes cover?\n",
      "57 What is the meaning of \"AmazonCounterfactualClassification\"?\n",
      "58 How similar is the process of data processing in the Sequential Recommendation (SR) task to traditional sequential recommendation tasks?\n",
      "59 How does Biomistral ensure relevance and prevent irrelevant tokens or hallucinations during inference?\n",
      "60 What is the consequence of using HGOT for fact retrieval and reasoning?\n",
      "61 What is the procedure to integrate Triple into end-to-end pipelines for prompt learning under a limited budget?\n",
      "62 What enables the constraint of raw material usage not to exceed available amounts in the optimization model?\n",
      "63 Why isn't there any data provided for hours of Data (h) besides Jukebox in the table?\n",
      "64 How do you rate the effectiveness of Vicuna as an LLM and fine-tuned HuBERT as a speech encoder for ASR tasks compared to other models?\n",
      "65 Is the METEOR score used for evaluating the semantic similarity in MC tasks?\n",
      "66 Is the NPE defense experiment showing a higher success rate for upward or downward navigation in LLMs-based navigation?\n",
      "67 What is the count of training data for AVE used in the Task section and what year is associated with the PM data for the OOD Test?\n",
      "68 What are the data sources mentioned in improving driving explanations?\n",
      "69 What is the value of Kendall's Tau Correlation Coefficient for Figure 5 in the study?\n",
      "70 What is Kendall's Tau correlation coefficient?\n",
      "71 How similar are the performance metrics of diminutive language models in clinical tasks?\n",
      "72 What contributed to AgentLens providing a significant improvement in exploring the cause of agent behaviors over the baseline?\n",
      "73 What is the consequence of a Navigational Prompt Suffix (NPS) attack on Large Language Models (LLMs) for navigation in urban environments?\n",
      "74 What is the process to reduce the computational cost of finetuning in model distillation?\n",
      "75 What enables the efficient routing of tokens in the V-MoE model?\n",
      "76 Why don't LLMs provide accurate explanations for incorrect predictions?\n",
      "77 What do you think about the use of \"Forget Set\" in data generation?\n",
      "78 Is Linqi Song affiliated with City University of Hong Kong?\n",
      "79 Is the feature about 'generating text to address an in-line prompt' or 'creating a comprehensive database for text analysis'?\n",
      "80 What is the purpose of the LAMBADA metric developed by OpenAI, and how does it differ in its approach to assessing Large Language Models (LLMs)?\n",
      "81 What are the names of the models mentioned in the main results section?\n",
      "82 How many languages were primarily focused on in the study?\n",
      "83 What is Kendall's Tau correlation coefficient?\n",
      "84 How similar are the metrics used to evaluate Llama2-13B-chat and Vicuna-13B-v1.5-16K models?\n",
      "85 What has led to the significant attention towards multimodal variants based on Large Language Models?\n",
      "86 What is the significance of Kendall's Tau correlation coefficient in assessing the performance of the vocabulary adaptation?\n",
      "87 What is the procedure to integrate Triple into end-to-end pipelines for prompt learning under a limited budget?\n",
      "88 What makes the GRACE framework able to perform generative cross-modal retrieval?\n",
      "89 Why doesn't the text provide a clear explanation on how self-alignment mitigates hallucinations in LLMs?\n",
      "90 In the context of machine learning models, how important do you think precision is in computations, such as using FP32 versus BF16?\n",
      "91 Is the angle BAC equal to 30 degrees?\n",
      "92 Is the text about a musical performance or a scientific experiment?\n",
      "93 What is the purpose of the evaluation framework mentioned in the document?\n",
      "94 What are some components of the legal evaluation framework described in the document?\n",
      "95 What percentage improvement in model performance was observed when comparing GPT-4 and GPT-4 plus RAG?\n",
      "96 What does the term \"black-box\" mean in the context of AI-generated text detection?\n",
      "97 How similar are the studies focusing on AI in video editing and AI in text compression?\n",
      "98 What are the limitations observed in the study on LLMs and MMs in solving geometry problems?\n",
      "99 What is the result of Bryan Wang's internship at Meta Reality Labs - Research?\n"
     ]
    }
   ],
   "source": [
    "for i, q in enumerate(qa[\"train\"][:100][\"query\"]):\n",
    "    print(i, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What makes the GRACE framework able to perform generative cross-modal retrieval?\n",
      "Dataset answer: GRACE assigns unique identifiers to images and comprises two training steps: learning to memorize the associations between visual content and their identifiers, and learning to retrieve the identifier of a relevant image given a textual query.\n",
      "Model answer with context: \n",
      "Answer: The use of unique identifiers for images and a training scheme focused on \"learning to memorize\" and \"learning to retrieve\" enable the GRACE framework to effectively perform generative cross-modal retrieval.\n",
      "Model answer without context: \n",
      "The GRACE framework uses a unified graph-based representation and joint cross-modal learning to effectively perform generative cross-modal retrieval.\n"
     ]
    }
   ],
   "source": [
    "question = qa[\"train\"][88][\"query\"]\n",
    "answer = qa[\"train\"][88][\"generation_gt\"][0]\n",
    "\n",
    "# Use the existing retriever instead of papers_collection\n",
    "docs = retriever.invoke(question)\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Dataset answer: {answer}\")\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"context\"],\n",
    "    template=\"\"\"You are an helpful AI assistant, answer in one sentence.\n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "response_rag = (prompt | client).invoke({\"question\": question, \"context\": context})\n",
    "\n",
    "print(f\"Model answer with context: {response_rag}\")\n",
    "\n",
    "promt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an helpful AI assistant, answer in one sentence.\n",
    "    Question: {question}\n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "response_norag = (promt | client).invoke({\"question\": question})\n",
    "print(f\"Model answer without context: {response_norag}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Generative Cross-Modal Retrieval: Memorizing Images In Multimodal Language Models For Retrieval And Beyond\n",
      "## 4 Experiments 4.1 Datasets And Baselines\n",
      "\n",
      "We evaluated our proposed generative cross-modal retrieval framework, GRACE, on two commonlyused datasets: Flickr30K (Young et al., 2014) and MS-COCO (Lin et al., 2014). Flickr30K contains 31,783 images sourced from Flickr. Each image is associated with five human-annotated sentences.  \n",
      "We adopted the data split used by Li et al., comprising 29,783 images for training, 1,000 for validation, and 1,000 for testing. MS-COCO comprises\n",
      "123,287 images, and each MS-COCO image comes with five sentences of annotations. We followed the dataset split proposed in (Lee et al., 2018), utilizing 113,287 images for training, 5,000 for validation, and 5,000 for testing. Consistent with prior studies (Young et al., 2014; Chen et al., 2021),  \n",
      "| Paradigm              | Methods        |\n",
      "|-----------------------|----------------|\n",
      "| Flickr30K             | MS-COCO (5K)   |\n",
      "| R@1                   | R@5            |\n",
      "| Two-tower             |                |\n",
      "| VSE++ (               | Faghri et al.  |\n",
      "| Dual-path (           | Zheng et al.   |\n",
      "| CAMERA (              | Qu et al.      |\n",
      "| 70.5                  | 81.5           |\n",
      "| CLIP (                | Radford et al. |\n",
      "| GRACE                 |                |\n",
      "| Numeric Identifier\n",
      "\n",
      "# Generative Cross-Modal Retrieval: Memorizing Images In Multimodal Language Models For Retrieval And Beyond\n",
      "## 3.2 Overview\n",
      "\n",
      "In this work, we present GRACE, a novel generative cross-modal retrieval framework, as illustrated in Figure 2. As previously discussed, addressing the challenges of visual memory and visual recall is essential for generative cross-modal retrieval. Towards this objective, GRACE assigns **unique** identifiers to images in the dataset DI. This strategy allows the model to learn mappings from images to their respective identifiers, facilitating visual memory. Moreover, the model could generate identifiers as retrieval results rather than generate real images. Representing images as identifiers underpins our training scheme, which is divided into two core steps: \"learning to memorize\" and \"learning to retrieve\". The two training steps are designed to enable the model to effectively memorize images in parameters and subsequently learn to recall them in response to textual queries.\n",
      "\n",
      "# Generative Cross-Modal Retrieval: Memorizing Images In Multimodal Language Models For Retrieval And Beyond\n",
      "## Limitations\n",
      "\n",
      "This work introduces a new paradigm in text-image retrieval, but it also has some limitations to be addressed. 1) The evaluation of GRACE's image retrieval ability on Flickr30K and MS-COCO was compared with two-tower baselines. However, it is important to note that Flickr30K and MS-COCO are also used as benchmarks for text-image ranking approaches, where one-tower frameworks have dominated. This may confuse newcomers to the field, as they may perceive GRACE and two-tower approaches as lagging behind the one-tower framework. However, it should be noted that GRACE\n",
      "and one-tower approaches focus on image retrieval, placing high demands on retrieval efficiency, while two-tower approaches are primarily suitable for the ranking stage, allowing for more time-consuming calculations to improve performance. 2) The identifiers currently used by GRACE are not as satisfactory as expected, only yielding results comparable to previous methods. However, as a pioneering work, the main significance of this work lies in validating the feasibility of generative cross-model retrieval. Further research is expected to enhance this paradigm.\n",
      "\n",
      "# Generative Cross-Modal Retrieval: Memorizing Images In Multimodal Language Models For Retrieval And Beyond\n",
      "## 2 Related Work 2.1 Cross-Modal Retrieval\n",
      "\n",
      "The current cross-modal retrieval (text-image matching) approaches can be categorized into the two frameworks and the one-tower framework based on how modality interaction is handled. Onetower framework (Chen et al., 2020; Diao et al.,\n",
      "2021; Lee et al., 2018; Qu et al., 2021) embraces fine-grained cross-modal interactions to achieve matching between fragments (e.g., objects and words). As for the two-tower framework (Chen et al., 2021; Faghri et al., 2017; Zheng et al., 2020;\n",
      "Qu et al., 2020), images and texts are independently mapped into a joint feature space in which the semantic similarities are calculated via cosine function or Euclidean distance. Both the one-tower framework and the two-tower framework formulate the cross-modal retrieval as a discriminative problem, which relies on discriminative loss and negative samples to learn an embedding space. In this work, we explore a new generative paradigm for cross-modal retrieval.  \n",
      "2.2\n",
      "Generative Retrieval Generative retrieval is an emerging new retrieval paradigm in text retrieval, which generates identifier strings of passages as the retrieval target. Instead of generating entire passages, this approach uses identifiers to reduce the amount of useless information and make it easier for the model to memorize and learn (Li et al., 2023d). Different types of identifiers have been explored in various search scenarios, including passage titles (Web URLs), numeric IDs, and substrings of passages, as shown in previous studies (De Cao et al., 2020; Tay et al., 2022; Bevilacqua et al., 2022; Li et al., 2023c; Zhang et al., 2023; Li et al., 2023b). Generative retrieval gains a lot of attention in text retrieval, as it could take advantage of the powerful generative language models. However, how to facilitate cross-modal retrieval in a generative way is still an untapped problem.\n",
      "\n",
      "# Generative Cross-Modal Retrieval: Memorizing Images In Multimodal Language Models For Retrieval And Beyond\n",
      "## 4.4 Efficiency Analysis\n",
      "\n",
      "In large-scale cross-modal retrieval, efficiency emerges as a crucial factor. This is why the onetower framework, effective for small-scale ranking, falls short in the retrieval stage. To address this, we conducted experiments comparing the efficiency of CLIP and GRACE. CLIP can pre-encode all images into vectors, incurring most of its inference cost from text encoding and calculating the similarity between text embeddings and image embeddings. In contrast, the generative framework necessitates generating identifiers. We assessed the query latency of both CLIP and GRACE to varying image sizes, with detailed results presented in Figure 4.  \n",
      "Our findings are insightful. Firstly, CLIP's inference speed decreases progressively as image size increases, owing to the escalating number of similarity calculations required. Secondly, the inference speed of our generative framework remains nearly constant, a result of encoding all images directly into its parameters. Thirdly, when image sizes exceed a certain threshold (about 150,000 images), our generative framework surpasses CLIP in terms of inference speed, and this advantage grows as image sizes continue to increase. Lastly, these findings underscore that the generative framework is not only capable of large-scale image retrieval but can also perform comparably to two-tower approaches.\n"
     ]
    }
   ],
   "source": [
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With RAG:\n",
      "Bleu: 0.08724449615067745\n",
      "Rouge: 0.38235294117647056\n",
      "Without RAG:\n",
      "Bleu: 0.018597924362574465\n",
      "Rouge: 0.17543859649122806\n"
     ]
    }
   ],
   "source": [
    "from ragas import SingleTurnSample\n",
    "from ragas.metrics import BleuScore, RougeScore\n",
    "\n",
    "bleu = BleuScore()\n",
    "rouge = RougeScore()\n",
    "print(\"With RAG:\")\n",
    "test_data = SingleTurnSample(\n",
    "    **{\n",
    "        \"user_input\": question,\n",
    "        \"response\": response_rag,\n",
    "        \"reference\": answer,\n",
    "    }\n",
    ")\n",
    "print(\"Bleu:\", bleu.single_turn_score(test_data))\n",
    "print(\"Rouge:\", rouge.single_turn_score(test_data))\n",
    "\n",
    "print(\"Without RAG:\")\n",
    "test_data = SingleTurnSample(\n",
    "    **{\n",
    "        \"user_input\": question,\n",
    "        \"response\": response_norag,\n",
    "        \"reference\": answer,\n",
    "    }\n",
    ")\n",
    "print(\"Bleu:\", bleu.single_turn_score(test_data))\n",
    "print(\"Rouge:\", rouge.single_turn_score(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
