{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WaterLLLMarks Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Papers dataset\n",
    "\n",
    "Paper got from AutoRAG's evaluation pipeline. The dataset is composed of text chunks automatically extracted from the papers' PDFs using OCR. This makes it relatively low quality, as there is a lot of noise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "corpus = load_dataset(\"MarkrAI/AutoRAG-evaluation-2024-LLM-paper-v1\", \"corpus\")\n",
    "qa = load_dataset(\"MarkrAI/AutoRAG-evaluation-2024-LLM-paper-v1\", \"qa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "client = ChatOpenAI(model=\"gpt-3.5-turbo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "docs = corpus[\"train\"][\"contents\"]\n",
    "ids = corpus[\"train\"][\"doc_id\"]\n",
    "\n",
    "embedding_fn = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "chroma_client = Chroma(embedding_function=embedding_fn)\n",
    "for i in range(0, len(docs), 1000):\n",
    "    chroma_client.add_texts(texts=docs[i : i + 1000], ids=ids[i : i + 1000])\n",
    "\n",
    "retriever = chroma_client.as_retriever(search_kwargs={\"k\": 3})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|â–‹         | 2/30 [00:25<05:59, 12.83s/it]\n",
      "  + Exception Group Traceback (most recent call last):\n",
      "  |   File \"/home/leo/Codespace/leolavaur.git/llm-text-watermarking/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "  |     exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  |   File \"/tmp/ipykernel_165361/2906848695.py\", line 13, in <module>\n",
      "  |     res = pipeline.apply(qaset[20:50], max_workers=10)\n",
      "  |           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"/home/leo/Codespace/leolavaur.git/llm-text-watermarking/waterllmarks/pipeline.py\", line 173, in apply\n",
      "  |     return [f.result() for f in iterator]\n",
      "  |             ^^^^^^^^^^\n",
      "  |   File \"/home/leo/.local/share/uv/python/cpython-3.12.6-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n",
      "  |     return self.__get_result()\n",
      "  |            ^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"/home/leo/.local/share/uv/python/cpython-3.12.6-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n",
      "  |     raise self._exception\n",
      "  |   File \"/home/leo/.local/share/uv/python/cpython-3.12.6-linux-x86_64-gnu/lib/python3.12/concurrent/futures/thread.py\", line 58, in run\n",
      "  |     result = self.fn(*self.args, **self.kwargs)\n",
      "  |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"/home/leo/Codespace/leolavaur.git/llm-text-watermarking/waterllmarks/pipeline.py\", line 113, in __call__\n",
      "  |     raise e\n",
      "  |   File \"/home/leo/Codespace/leolavaur.git/llm-text-watermarking/waterllmarks/pipeline.py\", line 110, in __call__\n",
      "  |     artifacts = step(artifacts)\n",
      "  |                 ^^^^^^^^^^^^^^^\n",
      "  |   File \"/home/leo/Codespace/leolavaur.git/llm-text-watermarking/waterllmarks/pipeline.py\", line 215, in wrapper\n",
      "  |     return call_func(self, artifacts)\n",
      "  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"/home/leo/Codespace/leolavaur.git/llm-text-watermarking/waterllmarks/pipeline.py\", line 237, in wrapper\n",
      "  |     return call_func(self, artifacts)\n",
      "  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"/home/leo/Codespace/leolavaur.git/llm-text-watermarking/waterllmarks/pipeline.py\", line 422, in __call__\n",
      "  |     artifacts[\"metrics\"] = asyncio.run(compute_metrics())\n",
      "  |                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"/home/leo/Codespace/leolavaur.git/llm-text-watermarking/.venv/lib/python3.12/site-packages/nest_asyncio.py\", line 30, in run\n",
      "  |     return loop.run_until_complete(task)\n",
      "  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"/home/leo/Codespace/leolavaur.git/llm-text-watermarking/.venv/lib/python3.12/site-packages/nest_asyncio.py\", line 98, in run_until_complete\n",
      "  |     return f.result()\n",
      "  |            ^^^^^^^^^^\n",
      "  |   File \"/home/leo/.local/share/uv/python/cpython-3.12.6-linux-x86_64-gnu/lib/python3.12/asyncio/futures.py\", line 203, in result\n",
      "  |     raise self._exception.with_traceback(self._exception_tb)\n",
      "  |   File \"/home/leo/.local/share/uv/python/cpython-3.12.6-linux-x86_64-gnu/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
      "  |     result = coro.send(None)\n",
      "  |              ^^^^^^^^^^^^^^^\n",
      "  |   File \"/home/leo/Codespace/leolavaur.git/llm-text-watermarking/waterllmarks/pipeline.py\", line 402, in compute_metrics\n",
      "  |     async with asyncio.TaskGroup() as tg:\n",
      "  |                ^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"/home/leo/.local/share/uv/python/cpython-3.12.6-linux-x86_64-gnu/lib/python3.12/asyncio/taskgroups.py\", line 145, in __aexit__\n",
      "  |     raise me from None\n",
      "  | ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n",
      "  +-+---------------- 1 ----------------\n",
      "    | Traceback (most recent call last):\n",
      "    |   File \"/home/leo/Codespace/leolavaur.git/llm-text-watermarking/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1582, in _request\n",
      "    |     response = await self._client.send(\n",
      "    |                ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/leo/Codespace/leolavaur.git/llm-text-watermarking/.venv/lib/python3.12/site-packages/httpx/_client.py\", line 1629, in send\n",
      "    |     response = await self._send_handling_auth(\n",
      "    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/leo/Codespace/leolavaur.git/llm-text-watermarking/.venv/lib/python3.12/site-packages/httpx/_client.py\", line 1657, in _send_handling_auth\n",
      "    |     response = await self._send_handling_redirects(\n",
      "    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/leo/Codespace/leolavaur.git/llm-text-watermarking/.venv/lib/python3.12/site-packages/httpx/_client.py\", line 1694, in _send_handling_redirects\n",
      "    |     response = await self._send_single_request(request)\n",
      "    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/leo/Codespace/leolavaur.git/llm-text-watermarking/.venv/lib/python3.12/site-packages/httpx/_client.py\", line 1730, in _send_single_request\n",
      "    |     response = await transport.handle_async_request(request)\n",
      "    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/leo/Codespace/leolavaur.git/llm-text-watermarking/.venv/lib/python3.12/site-packages/httpx/_transports/default.py\", line 394, in handle_async_request\n",
      "    |     resp = await self._pool.handle_async_request(req)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/leo/Codespace/leolavaur.git/llm-text-watermarking/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py\", line 256, in handle_async_request\n",
      "    |     raise exc from None\n",
      "    |   File \"/home/leo/Codespace/leolavaur.git/llm-text-watermarking/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py\", line 236, in handle_async_request\n",
      "    |     response = await connection.handle_async_request(\n",
      "    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/leo/Codespace/leolavaur.git/llm-text-watermarking/.venv/lib/python3.12/site-packages/httpcore/_async/connection.py\", line 103, in handle_async_request\n",
      "    |     return await self._connection.handle_async_request(request)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/leo/Codespace/leolavaur.git/llm-text-watermarking/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py\", line 136, in handle_async_request\n",
      "    |     raise exc\n",
      "    |   File \"/home/leo/Codespace/leolavaur.git/llm-text-watermarking/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py\", line 106, in handle_async_request\n",
      "    |     ) = await self._receive_response_headers(**kwargs)\n",
      "    |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/leo/Codespace/leolavaur.git/llm-text-watermarking/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py\", line 177, in _receive_response_headers\n",
      "    |     event = await self._receive_event(timeout=timeout)\n",
      "    |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/leo/Codespace/leolavaur.git/llm-text-watermarking/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py\", line 217, in _receive_event\n",
      "    |     data = await self._network_stream.read(\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/leo/Codespace/leolavaur.git/llm-text-watermarking/.venv/lib/python3.12/site-packages/httpcore/_backends/anyio.py\", line 35, in read\n",
      "    |     return await self._stream.receive(max_bytes=max_bytes)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/leo/Codespace/leolavaur.git/llm-text-watermarking/.venv/lib/python3.12/site-packages/anyio/streams/tls.py\", line 204, in receive\n",
      "    |     data = await self._call_sslobject_method(self._ssl_object.read, max_bytes)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/leo/Codespace/leolavaur.git/llm-text-watermarking/.venv/lib/python3.12/site-packages/anyio/streams/tls.py\", line 147, in _call_sslobject_method\n",
      "    |     data = await self.transport_stream.receive()\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/leo/Codespace/leolavaur.git/llm-text-watermarking/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 1246, in receive\n",
      "    |     await self._protocol.read_event.wait()\n",
      "    |   File \"/home/leo/.local/share/uv/python/cpython-3.12.6-linux-x86_64-gnu/lib/python3.12/asyncio/locks.py\", line 209, in wait\n",
      "    |     fut = self._get_loop().create_future()\n",
      "    |           ^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/leo/.local/share/uv/python/cpython-3.12.6-linux-x86_64-gnu/lib/python3.12/asyncio/mixins.py\", line 20, in _get_loop\n",
      "    |     raise RuntimeError(f'{self!r} is bound to a different event loop')\n",
      "    | RuntimeError: <asyncio.locks.Event object at 0x715848fedd30 [unset]> is bound to a different event loop\n",
      "    | \n",
      "    | The above exception was the direct cause of the following exception:\n",
      "    | \n",
      "    | Traceback (most recent call last):\n",
      "    |   File \"/home/leo/.local/share/uv/python/cpython-3.12.6-linux-x86_64-gnu/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    |     result = coro.send(None)\n",
      "    |              ^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/leo/Codespace/leolavaur.git/llm-text-watermarking/.venv/lib/python3.12/site-packages/ragas/metrics/base.py\", line 502, in single_turn_ascore\n",
      "    |     raise e\n",
      "    |   File \"/home/leo/Codespace/leolavaur.git/llm-text-watermarking/.venv/lib/python3.12/site-packages/ragas/metrics/base.py\", line 495, in single_turn_ascore\n",
      "    |     score = await asyncio.wait_for(\n",
      "    |             ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/leo/.local/share/uv/python/cpython-3.12.6-linux-x86_64-gnu/lib/python3.12/asyncio/tasks.py\", line 520, in wait_for\n",
      "    |     return await fut\n",
      "    |            ^^^^^^^^^\n",
      "    |   File \"/home/leo/Codespace/leolavaur.git/llm-text-watermarking/.venv/lib/python3.12/site-packages/ragas/metrics/_noise_sensitivity.py\", line 136, in _single_turn_ascore\n",
      "    |     return await self._ascore(row, callbacks)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/leo/Codespace/leolavaur.git/llm-text-watermarking/.venv/lib/python3.12/site-packages/ragas/metrics/_noise_sensitivity.py\", line 147, in _ascore\n",
      "    |     ans_statements = await self._decompose_answer_into_statements(\n",
      "    |                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/leo/Codespace/leolavaur.git/llm-text-watermarking/.venv/lib/python3.12/site-packages/ragas/metrics/_noise_sensitivity.py\", line 88, in _decompose_answer_into_statements\n",
      "    |     statements_simplified = await self.statement_prompt.generate(\n",
      "    |                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/leo/Codespace/leolavaur.git/llm-text-watermarking/.venv/lib/python3.12/site-packages/ragas/prompt/pydantic_prompt.py\", line 126, in generate\n",
      "    |     output_single = await self.generate_multiple(\n",
      "    |                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/leo/Codespace/leolavaur.git/llm-text-watermarking/.venv/lib/python3.12/site-packages/ragas/prompt/pydantic_prompt.py\", line 187, in generate_multiple\n",
      "    |     resp = await llm.generate(\n",
      "    |            ^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/leo/Codespace/leolavaur.git/llm-text-watermarking/.venv/lib/python3.12/site-packages/ragas/llms/base.py\", line 108, in generate\n",
      "    |     result = await agenerate_text_with_retry(\n",
      "    |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/leo/Codespace/leolavaur.git/llm-text-watermarking/.venv/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n",
      "    |     return await copy(fn, *args, **kwargs)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/leo/Codespace/leolavaur.git/llm-text-watermarking/.venv/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n",
      "    |     do = await self.iter(retry_state=retry_state)\n",
      "    |          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/leo/Codespace/leolavaur.git/llm-text-watermarking/.venv/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n",
      "    |     result = await action(retry_state)\n",
      "    |              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/leo/Codespace/leolavaur.git/llm-text-watermarking/.venv/lib/python3.12/site-packages/tenacity/_utils.py\", line 99, in inner\n",
      "    |     return call(*args, **kwargs)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/leo/Codespace/leolavaur.git/llm-text-watermarking/.venv/lib/python3.12/site-packages/tenacity/__init__.py\", line 398, in <lambda>\n",
      "    |     self._add_action_func(lambda rs: rs.outcome.result())\n",
      "    |                                      ^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/leo/.local/share/uv/python/cpython-3.12.6-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py\", line 449, in result\n",
      "    |     return self.__get_result()\n",
      "    |            ^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/leo/.local/share/uv/python/cpython-3.12.6-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n",
      "    |     raise self._exception\n",
      "    |   File \"/home/leo/Codespace/leolavaur.git/llm-text-watermarking/.venv/lib/python3.12/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n",
      "    |     result = await fn(*args, **kwargs)\n",
      "    |              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/leo/Codespace/leolavaur.git/llm-text-watermarking/.venv/lib/python3.12/site-packages/ragas/llms/base.py\", line 251, in agenerate_text\n",
      "    |     result = await self.langchain_llm.agenerate_prompt(\n",
      "    |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/leo/Codespace/leolavaur.git/llm-text-watermarking/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 796, in agenerate_prompt\n",
      "    |     return await self.agenerate(\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/leo/Codespace/leolavaur.git/llm-text-watermarking/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 756, in agenerate\n",
      "    |     raise exceptions[0]\n",
      "    |   File \"/home/leo/.local/share/uv/python/cpython-3.12.6-linux-x86_64-gnu/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\n",
      "    |     result = coro.send(None)\n",
      "    |              ^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/leo/Codespace/leolavaur.git/llm-text-watermarking/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 924, in _agenerate_with_cache\n",
      "    |     result = await self._agenerate(\n",
      "    |              ^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/leo/Codespace/leolavaur.git/llm-text-watermarking/.venv/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 951, in _agenerate\n",
      "    |     response = await self.async_client.create(**payload)\n",
      "    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/leo/Codespace/leolavaur.git/llm-text-watermarking/.venv/lib/python3.12/site-packages/openai/resources/chat/completions.py\", line 1720, in create\n",
      "    |     return await self._post(\n",
      "    |            ^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/leo/Codespace/leolavaur.git/llm-text-watermarking/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1849, in post\n",
      "    |     return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/leo/Codespace/leolavaur.git/llm-text-watermarking/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1543, in request\n",
      "    |     return await self._request(\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/leo/Codespace/leolavaur.git/llm-text-watermarking/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1606, in _request\n",
      "    |     return await self._retry_request(\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/leo/Codespace/leolavaur.git/llm-text-watermarking/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1676, in _retry_request\n",
      "    |     return await self._request(\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/leo/Codespace/leolavaur.git/llm-text-watermarking/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1606, in _request\n",
      "    |     return await self._retry_request(\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/leo/Codespace/leolavaur.git/llm-text-watermarking/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1676, in _retry_request\n",
      "    |     return await self._request(\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/home/leo/Codespace/leolavaur.git/llm-text-watermarking/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1616, in _request\n",
      "    |     raise APIConnectionError(request=request) from err\n",
      "    | openai.APIConnectionError: Connection error.\n",
      "    +------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from waterllmarks.pipeline import ExecConfig, RAGLLM, VectorDB, Evaluate\n",
    "\n",
    "pipeline = VectorDB() | RAGLLM() | Evaluate()\n",
    "pipeline.configure(ExecConfig(vectordb=retriever, llm=client, embedding=embedding_fn))\n",
    "\n",
    "qaset = [\n",
    "    {\n",
    "        \"query\": entry[\"query\"],\n",
    "        \"reference\": entry[\"generation_gt\"][0],\n",
    "    }\n",
    "    for entry in qa[\"train\"]  # .select(range(10))\n",
    "]\n",
    "res = pipeline.apply(qaset[20:50], max_workers=10)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': \"Does the Turing Test assess a machine's ability to exhibit intelligent behavior equivalent to that of a human?\",\n",
       " 'reference': 'Yes',\n",
       " 'chunks': [\"# (Ir)Rationality And Cognitive Biases In Large Language Models\\n## Rationality Task Results\\n\\nResults across all tasks are aggregated in Table 3 and Figure 3. The model that displayed the best overall performance was OpenAI's GPT-4, which achieved the highest proportion of answers that were correct and where the results was achieved through correct reasoning (cateogorised as correct and *human-like* in the above categorisation). GPT-4 gave the correct response and correct reasoning in 69.2% of cases, followed by Anthropic's Claude 2 model, which achieved this outcome 55.0% of the time. Conversely, the model with the highest proportion of incorrect responses (both human-like and non-human-like) was Meta's Llama 2 model with 7 billion parameters, which gave incorrect responses in 77.5% of cases. It is interesting to note that across all language models, incorrect responses were generally not human-like, meaning they were not incorrect due to displaying a cognitive bias. Instead, these responses generally displayed illogical reasoning, and even on occasion provided correct reasoning but then gave an incorrect final answer. An example of the latter is illustrated in Figure 4: this example shows Bard's response to the facilitated version of the Wason task, where the correct response is that both Letter 3 and Letter 4 should be turned over. The model correctly reaches this conclusion in the explanation, but both at the start and end of the response only states that Letter 4 needs to be turned over. This type of response, where the reasoning is correct but the final answer is not, was observed across all model families to varying degrees.  \\nThe result that most incorrect responses were not incorrect due to having fallen for a cognitive bias highlight that these models do not fail at these tasks in the same way that humans do. As we have seen, many studies have shown that LLMs simulate human biases and societal norms [24â€“26]. However, when it comes to reasoning, the effect is less clear. The model that displayed the highest proportion of human-like biases in its responses was GPT-3.5, where this only occurred in 21.7% of cases. If we include human-like correct responses for GPT-3.5, this brings the proportion to 50.8% of cases. Again, the model that displayed the most human-like responses (both correct and incorrect) was GPT-4 (73.3%); the lowest was Llama 2 with 13 billion parameters, only giving human-like responses in 8.3% of cases. The comparison between correct and\",\n",
       "  '# (Ir)Rationality And Cognitive Biases In Large Language Models\\n## 1 Introduction\\n\\n can be used to test the rationality of a model; this is a complex problem which requires a consensus what is deemed rational and irrational.  \\nMirco Musolesi University College London University of Bologna m.musolesi@ucl.ac.uk In using methods designed to evaluate human reasoning, it is important to acknowledge the performance vs. competence debate [14]. This line of argument encourages *species-fair* comparisons between humans and machines, meaning that we should design tests specific to either humans or machines, as otherwise apparent failures may not reflect underlying capabilities but only superficial differences. Lampinen[15] discusses this problem when it comes to language models in particular, highlighting that different approaches must be taking to evaluate cognitive and foundation models. However, if we take the purpose of LLMs to be to produce human-like language, perhaps the best approach is precisely to evaluate their output with tasks designed to evaluate humans. This is the approach we have taken in this paper - in order to identify whether LLMs reason rationally, or whether they exhibit biases that can be assimilated to those present in human decision-making, the most appropriate approach is therefore to use tasks that were initially designed for humans. Building on this debate and looking at LLMs being evaluated using human tests, Hagendorff et al. [16] have proposed the creation of a new field of research called *machine psychology*, which would treat LLMs as participants in psychological experiments. The approach employed in this paper precisely applies tests from psychology that were originally designed for humans, in this case to evaluate rational and irrational reasoning displayed but such models. Further to this, some have even discussed the potential of using LLMs as participants in cognitive experiments instead of humans [17], although some see this proposal as too optimistic [18], and others warn against excessive anthropomorphism [19]. One argument against the use of such models in cognitive experiments is that LLMs may be effective at approximating average human judgements, but are not good at capturing the variation in human behaviour [20]. One potential avenue to address this issue is current work on language models impersonating different roles [21], in this way capturing some of the variation in human behaviour. Binz and Schulz [22] show that after finetuning LLMs on data from psychological experiments, they can become accurate cognitive models, which they claim begins paving the way for the potential of using these models to study human behaviour. Park et al. [23] combine large language models with computational interactive agents to simulate human behaviour',\n",
       "  '## 1. Introduction\\n\\nIn the recent years, Large Language Models (LLMs) have been among the most impressive achievements in Artificial intelligence (AI) and Machine Learning (ML), raising questions about whether we are close to an Artificial General Intelligence (AGI) system (Bubeck et al., 2023; Fei et al.,\\n2022). Foundation models like ChatGPT (OpenAI, 2023b) have shown remarkable performance across a wide range of novel and complex tasks, including coding, vision, healthcare, law, education, and psychology (Bubeck et al., 2023).  \\nThese models perform close to human experts, without needing additional re-training or fine-tuning, apparently mimicking human abilities and reasoning (Guo et al., 2023; Min et al., 2021; Bubeck et al., 2023). In particular, recent work has demonstrated how LLMs can emulate human reasoning in solving complex problems: LLMs can simulate chain of thoughts, generating a reasoning path to decompose complex problems into multiple easier steps to solve (Kojima et al., 2022; Wei et al., 2022b; Ho et al., 2022). We could conclude that LLMs are able to replicate high-level cognitive and reasoning capabilities of humans (Huang & Chang, 2022; Hagendorff et al., 2022), especially with bigger foundation models like GPT-4 (Wei et al., 2022a). If we assume this is true, a natural question arises: \"Can we leverage LLMs to simulate aspects of human behavior that deviate from perfect rationality so as to eventually improve our understanding of diverse human conduct?\"\\nModeling human behavior has seen significant interest in various domains, ranging from robotics for human-robot collaboration (Dragan et al., 2015), to finance and economics for modeling human investors and consumers (Liu et al., 2022; Akerlof & Shiller, 2010). In studies of human-robot interactions, it is commonly embraced that humans must not be modeled as optimal agents (Carroll et al., 2019), and an irrational human when correctly modeled, can communicate more information about their objectives than a perfectly rational human can (Chan et al., 2021). For instance, humans exhibit bounded rationality (Simon, 1997), often making satisfactory but not optimal decisions due to their limited knowledge and processing power. They are also known to be myopic in their decision making as they care more about short-term rewards than long-term rewards. While the discount factor in reinforcement learning (RL)',\n",
       "  \"# (Ir)Rationality And Cognitive Biases In Large Language Models\\n## 1 Introduction\\n\\n we use tasks from the cognitive psychology literature designed to test human cognitive biases, and apply these to a series of LLMs to evaluate whether they display rational or irrational reasoning. The capabilities of these models are quickly advancing, therefore the aim of this paper is to provide a methodological contribution showing how we can assess and compare LLMs. A number of studies have taken a similar approach, however they do not generally compare across different model types [12, 16, 29â€“35], or those that do are not evaluating rational reasoning [36]. Some find that LLMs outperform humans on reasoning tasks [16, 37], others find that these models replicate human biases [30, 38], and finally some studies have shown that LLMs perform much worse than humans on certain tasks [36]. Binz and Schulz [12] take a similar approach to that presented in this paper, where they treat GPT-3 as a participant in a psychological experiment to assess its decision-making, information search, deliberation and causal reasoning abilities. They assess the responses across two dimensions, looking at whether GPT-3's output is correct and/or human-like; we follow this approach in this paper as it allows us to distinguish between answers that are incorrect due to a human-like bias or are incorrect in a different way. While they find that GPT-3 performs as well or even better than human subjects, they also find that small changes to the wording of tasks can dramatically decrease the performance, likely due to GPT-3 having encountered these tasks in training. Hagendorff et al. [16] similarly use the Cognitive Reflection Test (CRT) and semantic illusions on a series of OpenAI's Generative Pre-trained Transformer (GPT) models. They classify the responses as *correct, intuitive* (but incorrect), and *atypical* - as models increase in size, the majority of responses go from being atypical, to intuitive, to overwhelmingly correct for GPT-4, which no longer displays human cognitive errors. Other studies that find the reasoning of LLMs to outperform that of humans includes Chen et al.'s [33] assessment of the economic rationality of GPT, and Webb et al.'s [34] comparison of GPT-3 and human performance on analogical tasks.  \\nAs mentioned, some studies have found that LLMs replicate cognitive biases present in human reasoning, and so in some instances display irrational thinking in the same way that humans do. Itzhak et al\",\n",
       "  '# How Reliable Are Automatic Evaluation Methods For Instruction-Tuned Llms?\\n## 4.1 Human Evaluation\\n\\n        |\\n| ENSV_SV    |           |           |           |\\n| 35         |           |           |           |\\n| (36)       |           |           |           |\\n| 68         |           |           |           |\\n| (63)       |           |           |           |\\n| 99         |           |           |           |\\n| (85)       |           |           |           |\\n| Avg. diff. |           |           |           |\\n| 1          | .         |         3 |         4 |  \\ncorrectness since it is the most important criterion for instruction tuning and inherently encompasses relatedness. Moreover, the higher degree of alignment between GPT-4 and humans on correctness compared to the other criteria prompts the need for a deeper analysis.'],\n",
       " 'chunk_ids': ['3e92c39c-6707-4cf9-aca5-c6c84f72309b',\n",
       "  'a09c2e9a-dd24-4e36-9b2e-7d4e15e58639',\n",
       "  'ed69de92-b214-4de5-ac6a-6c2e204e6416',\n",
       "  'a7145295-1155-4d88-baaf-48ec36095ae4',\n",
       "  '9122b070-3595-4f52-9367-f26553930c70'],\n",
       " 'context': '# (Ir)Rationality And Cognitive Biases In Large Language Models\\n## Rationality Task Results\\n\\nResults across all tasks are aggregated in Table 3 and Figure 3. The model that displayed the best overall performance was OpenAI\\'s GPT-4, which achieved the highest proportion of answers that were correct and where the results was achieved through correct reasoning (cateogorised as correct and *human-like* in the above categorisation). GPT-4 gave the correct response and correct reasoning in 69.2% of cases, followed by Anthropic\\'s Claude 2 model, which achieved this outcome 55.0% of the time. Conversely, the model with the highest proportion of incorrect responses (both human-like and non-human-like) was Meta\\'s Llama 2 model with 7 billion parameters, which gave incorrect responses in 77.5% of cases. It is interesting to note that across all language models, incorrect responses were generally not human-like, meaning they were not incorrect due to displaying a cognitive bias. Instead, these responses generally displayed illogical reasoning, and even on occasion provided correct reasoning but then gave an incorrect final answer. An example of the latter is illustrated in Figure 4: this example shows Bard\\'s response to the facilitated version of the Wason task, where the correct response is that both Letter 3 and Letter 4 should be turned over. The model correctly reaches this conclusion in the explanation, but both at the start and end of the response only states that Letter 4 needs to be turned over. This type of response, where the reasoning is correct but the final answer is not, was observed across all model families to varying degrees.  \\nThe result that most incorrect responses were not incorrect due to having fallen for a cognitive bias highlight that these models do not fail at these tasks in the same way that humans do. As we have seen, many studies have shown that LLMs simulate human biases and societal norms [24â€“26]. However, when it comes to reasoning, the effect is less clear. The model that displayed the highest proportion of human-like biases in its responses was GPT-3.5, where this only occurred in 21.7% of cases. If we include human-like correct responses for GPT-3.5, this brings the proportion to 50.8% of cases. Again, the model that displayed the most human-like responses (both correct and incorrect) was GPT-4 (73.3%); the lowest was Llama 2 with 13 billion parameters, only giving human-like responses in 8.3% of cases. The comparison between correct and\\n\\n# (Ir)Rationality And Cognitive Biases In Large Language Models\\n## 1 Introduction\\n\\n can be used to test the rationality of a model; this is a complex problem which requires a consensus what is deemed rational and irrational.  \\nMirco Musolesi University College London University of Bologna m.musolesi@ucl.ac.uk In using methods designed to evaluate human reasoning, it is important to acknowledge the performance vs. competence debate [14]. This line of argument encourages *species-fair* comparisons between humans and machines, meaning that we should design tests specific to either humans or machines, as otherwise apparent failures may not reflect underlying capabilities but only superficial differences. Lampinen[15] discusses this problem when it comes to language models in particular, highlighting that different approaches must be taking to evaluate cognitive and foundation models. However, if we take the purpose of LLMs to be to produce human-like language, perhaps the best approach is precisely to evaluate their output with tasks designed to evaluate humans. This is the approach we have taken in this paper - in order to identify whether LLMs reason rationally, or whether they exhibit biases that can be assimilated to those present in human decision-making, the most appropriate approach is therefore to use tasks that were initially designed for humans. Building on this debate and looking at LLMs being evaluated using human tests, Hagendorff et al. [16] have proposed the creation of a new field of research called *machine psychology*, which would treat LLMs as participants in psychological experiments. The approach employed in this paper precisely applies tests from psychology that were originally designed for humans, in this case to evaluate rational and irrational reasoning displayed but such models. Further to this, some have even discussed the potential of using LLMs as participants in cognitive experiments instead of humans [17], although some see this proposal as too optimistic [18], and others warn against excessive anthropomorphism [19]. One argument against the use of such models in cognitive experiments is that LLMs may be effective at approximating average human judgements, but are not good at capturing the variation in human behaviour [20]. One potential avenue to address this issue is current work on language models impersonating different roles [21], in this way capturing some of the variation in human behaviour. Binz and Schulz [22] show that after finetuning LLMs on data from psychological experiments, they can become accurate cognitive models, which they claim begins paving the way for the potential of using these models to study human behaviour. Park et al. [23] combine large language models with computational interactive agents to simulate human behaviour\\n\\n## 1. Introduction\\n\\nIn the recent years, Large Language Models (LLMs) have been among the most impressive achievements in Artificial intelligence (AI) and Machine Learning (ML), raising questions about whether we are close to an Artificial General Intelligence (AGI) system (Bubeck et al., 2023; Fei et al.,\\n2022). Foundation models like ChatGPT (OpenAI, 2023b) have shown remarkable performance across a wide range of novel and complex tasks, including coding, vision, healthcare, law, education, and psychology (Bubeck et al., 2023).  \\nThese models perform close to human experts, without needing additional re-training or fine-tuning, apparently mimicking human abilities and reasoning (Guo et al., 2023; Min et al., 2021; Bubeck et al., 2023). In particular, recent work has demonstrated how LLMs can emulate human reasoning in solving complex problems: LLMs can simulate chain of thoughts, generating a reasoning path to decompose complex problems into multiple easier steps to solve (Kojima et al., 2022; Wei et al., 2022b; Ho et al., 2022). We could conclude that LLMs are able to replicate high-level cognitive and reasoning capabilities of humans (Huang & Chang, 2022; Hagendorff et al., 2022), especially with bigger foundation models like GPT-4 (Wei et al., 2022a). If we assume this is true, a natural question arises: \"Can we leverage LLMs to simulate aspects of human behavior that deviate from perfect rationality so as to eventually improve our understanding of diverse human conduct?\"\\nModeling human behavior has seen significant interest in various domains, ranging from robotics for human-robot collaboration (Dragan et al., 2015), to finance and economics for modeling human investors and consumers (Liu et al., 2022; Akerlof & Shiller, 2010). In studies of human-robot interactions, it is commonly embraced that humans must not be modeled as optimal agents (Carroll et al., 2019), and an irrational human when correctly modeled, can communicate more information about their objectives than a perfectly rational human can (Chan et al., 2021). For instance, humans exhibit bounded rationality (Simon, 1997), often making satisfactory but not optimal decisions due to their limited knowledge and processing power. They are also known to be myopic in their decision making as they care more about short-term rewards than long-term rewards. While the discount factor in reinforcement learning (RL)\\n\\n# (Ir)Rationality And Cognitive Biases In Large Language Models\\n## 1 Introduction\\n\\n we use tasks from the cognitive psychology literature designed to test human cognitive biases, and apply these to a series of LLMs to evaluate whether they display rational or irrational reasoning. The capabilities of these models are quickly advancing, therefore the aim of this paper is to provide a methodological contribution showing how we can assess and compare LLMs. A number of studies have taken a similar approach, however they do not generally compare across different model types [12, 16, 29â€“35], or those that do are not evaluating rational reasoning [36]. Some find that LLMs outperform humans on reasoning tasks [16, 37], others find that these models replicate human biases [30, 38], and finally some studies have shown that LLMs perform much worse than humans on certain tasks [36]. Binz and Schulz [12] take a similar approach to that presented in this paper, where they treat GPT-3 as a participant in a psychological experiment to assess its decision-making, information search, deliberation and causal reasoning abilities. They assess the responses across two dimensions, looking at whether GPT-3\\'s output is correct and/or human-like; we follow this approach in this paper as it allows us to distinguish between answers that are incorrect due to a human-like bias or are incorrect in a different way. While they find that GPT-3 performs as well or even better than human subjects, they also find that small changes to the wording of tasks can dramatically decrease the performance, likely due to GPT-3 having encountered these tasks in training. Hagendorff et al. [16] similarly use the Cognitive Reflection Test (CRT) and semantic illusions on a series of OpenAI\\'s Generative Pre-trained Transformer (GPT) models. They classify the responses as *correct, intuitive* (but incorrect), and *atypical* - as models increase in size, the majority of responses go from being atypical, to intuitive, to overwhelmingly correct for GPT-4, which no longer displays human cognitive errors. Other studies that find the reasoning of LLMs to outperform that of humans includes Chen et al.\\'s [33] assessment of the economic rationality of GPT, and Webb et al.\\'s [34] comparison of GPT-3 and human performance on analogical tasks.  \\nAs mentioned, some studies have found that LLMs replicate cognitive biases present in human reasoning, and so in some instances display irrational thinking in the same way that humans do. Itzhak et al\\n\\n# How Reliable Are Automatic Evaluation Methods For Instruction-Tuned Llms?\\n## 4.1 Human Evaluation\\n\\n        |\\n| ENSV_SV    |           |           |           |\\n| 35         |           |           |           |\\n| (36)       |           |           |           |\\n| 68         |           |           |           |\\n| (63)       |           |           |           |\\n| 99         |           |           |           |\\n| (85)       |           |           |           |\\n| Avg. diff. |           |           |           |\\n| 1          | .         |         3 |         4 |  \\ncorrectness since it is the most important criterion for instruction tuning and inherently encompasses relatedness. Moreover, the higher degree of alignment between GPT-4 and humans on correctness compared to the other criteria prompts the need for a deeper analysis.',\n",
       " 'answer': \"Yes, the Turing Test is a test of a machine's ability to exhibit intelligent behavior indistinguishable from that of a human. It was proposed by Alan Turing in 1950 as a way to measure a machine's ability to think and reason like a human.\"}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from waterllmarks.pipeline import LLM\n",
    "\n",
    "test_query = {\n",
    "    \"query\": \"Does the Turing Test assess a machine's ability to exhibit intelligent behavior equivalent to that of a human?\",\n",
    "    \"reference\": \"Yes\",\n",
    "    \"chunks\": [\n",
    "        \"# (Ir)Rationality And Cognitive Biases In Large Language Models\\n## Rationality Task Results\\n\\nResults across all tasks are aggregated in Table 3 and Figure 3. The model that displayed the best overall performance was OpenAI's GPT-4, which achieved the highest proportion of answers that were correct and where the results was achieved through correct reasoning (cateogorised as correct and *human-like* in the above categorisation). GPT-4 gave the correct response and correct reasoning in 69.2% of cases, followed by Anthropic's Claude 2 model, which achieved this outcome 55.0% of the time. Conversely, the model with the highest proportion of incorrect responses (both human-like and non-human-like) was Meta's Llama 2 model with 7 billion parameters, which gave incorrect responses in 77.5% of cases. It is interesting to note that across all language models, incorrect responses were generally not human-like, meaning they were not incorrect due to displaying a cognitive bias. Instead, these responses generally displayed illogical reasoning, and even on occasion provided correct reasoning but then gave an incorrect final answer. An example of the latter is illustrated in Figure 4: this example shows Bard's response to the facilitated version of the Wason task, where the correct response is that both Letter 3 and Letter 4 should be turned over. The model correctly reaches this conclusion in the explanation, but both at the start and end of the response only states that Letter 4 needs to be turned over. This type of response, where the reasoning is correct but the final answer is not, was observed across all model families to varying degrees.  \\nThe result that most incorrect responses were not incorrect due to having fallen for a cognitive bias highlight that these models do not fail at these tasks in the same way that humans do. As we have seen, many studies have shown that LLMs simulate human biases and societal norms [24â€“26]. However, when it comes to reasoning, the effect is less clear. The model that displayed the highest proportion of human-like biases in its responses was GPT-3.5, where this only occurred in 21.7% of cases. If we include human-like correct responses for GPT-3.5, this brings the proportion to 50.8% of cases. Again, the model that displayed the most human-like responses (both correct and incorrect) was GPT-4 (73.3%); the lowest was Llama 2 with 13 billion parameters, only giving human-like responses in 8.3% of cases. The comparison between correct and\",\n",
    "        \"# (Ir)Rationality And Cognitive Biases In Large Language Models\\n## 1 Introduction\\n\\n can be used to test the rationality of a model; this is a complex problem which requires a consensus what is deemed rational and irrational.  \\nMirco Musolesi University College London University of Bologna m.musolesi@ucl.ac.uk In using methods designed to evaluate human reasoning, it is important to acknowledge the performance vs. competence debate [14]. This line of argument encourages *species-fair* comparisons between humans and machines, meaning that we should design tests specific to either humans or machines, as otherwise apparent failures may not reflect underlying capabilities but only superficial differences. Lampinen[15] discusses this problem when it comes to language models in particular, highlighting that different approaches must be taking to evaluate cognitive and foundation models. However, if we take the purpose of LLMs to be to produce human-like language, perhaps the best approach is precisely to evaluate their output with tasks designed to evaluate humans. This is the approach we have taken in this paper - in order to identify whether LLMs reason rationally, or whether they exhibit biases that can be assimilated to those present in human decision-making, the most appropriate approach is therefore to use tasks that were initially designed for humans. Building on this debate and looking at LLMs being evaluated using human tests, Hagendorff et al. [16] have proposed the creation of a new field of research called *machine psychology*, which would treat LLMs as participants in psychological experiments. The approach employed in this paper precisely applies tests from psychology that were originally designed for humans, in this case to evaluate rational and irrational reasoning displayed but such models. Further to this, some have even discussed the potential of using LLMs as participants in cognitive experiments instead of humans [17], although some see this proposal as too optimistic [18], and others warn against excessive anthropomorphism [19]. One argument against the use of such models in cognitive experiments is that LLMs may be effective at approximating average human judgements, but are not good at capturing the variation in human behaviour [20]. One potential avenue to address this issue is current work on language models impersonating different roles [21], in this way capturing some of the variation in human behaviour. Binz and Schulz [22] show that after finetuning LLMs on data from psychological experiments, they can become accurate cognitive models, which they claim begins paving the way for the potential of using these models to study human behaviour. Park et al. [23] combine large language models with computational interactive agents to simulate human behaviour\",\n",
    "        '## 1. Introduction\\n\\nIn the recent years, Large Language Models (LLMs) have been among the most impressive achievements in Artificial intelligence (AI) and Machine Learning (ML), raising questions about whether we are close to an Artificial General Intelligence (AGI) system (Bubeck et al., 2023; Fei et al.,\\n2022). Foundation models like ChatGPT (OpenAI, 2023b) have shown remarkable performance across a wide range of novel and complex tasks, including coding, vision, healthcare, law, education, and psychology (Bubeck et al., 2023).  \\nThese models perform close to human experts, without needing additional re-training or fine-tuning, apparently mimicking human abilities and reasoning (Guo et al., 2023; Min et al., 2021; Bubeck et al., 2023). In particular, recent work has demonstrated how LLMs can emulate human reasoning in solving complex problems: LLMs can simulate chain of thoughts, generating a reasoning path to decompose complex problems into multiple easier steps to solve (Kojima et al., 2022; Wei et al., 2022b; Ho et al., 2022). We could conclude that LLMs are able to replicate high-level cognitive and reasoning capabilities of humans (Huang & Chang, 2022; Hagendorff et al., 2022), especially with bigger foundation models like GPT-4 (Wei et al., 2022a). If we assume this is true, a natural question arises: \"Can we leverage LLMs to simulate aspects of human behavior that deviate from perfect rationality so as to eventually improve our understanding of diverse human conduct?\"\\nModeling human behavior has seen significant interest in various domains, ranging from robotics for human-robot collaboration (Dragan et al., 2015), to finance and economics for modeling human investors and consumers (Liu et al., 2022; Akerlof & Shiller, 2010). In studies of human-robot interactions, it is commonly embraced that humans must not be modeled as optimal agents (Carroll et al., 2019), and an irrational human when correctly modeled, can communicate more information about their objectives than a perfectly rational human can (Chan et al., 2021). For instance, humans exhibit bounded rationality (Simon, 1997), often making satisfactory but not optimal decisions due to their limited knowledge and processing power. They are also known to be myopic in their decision making as they care more about short-term rewards than long-term rewards. While the discount factor in reinforcement learning (RL)',\n",
    "        \"# (Ir)Rationality And Cognitive Biases In Large Language Models\\n## 1 Introduction\\n\\n we use tasks from the cognitive psychology literature designed to test human cognitive biases, and apply these to a series of LLMs to evaluate whether they display rational or irrational reasoning. The capabilities of these models are quickly advancing, therefore the aim of this paper is to provide a methodological contribution showing how we can assess and compare LLMs. A number of studies have taken a similar approach, however they do not generally compare across different model types [12, 16, 29â€“35], or those that do are not evaluating rational reasoning [36]. Some find that LLMs outperform humans on reasoning tasks [16, 37], others find that these models replicate human biases [30, 38], and finally some studies have shown that LLMs perform much worse than humans on certain tasks [36]. Binz and Schulz [12] take a similar approach to that presented in this paper, where they treat GPT-3 as a participant in a psychological experiment to assess its decision-making, information search, deliberation and causal reasoning abilities. They assess the responses across two dimensions, looking at whether GPT-3's output is correct and/or human-like; we follow this approach in this paper as it allows us to distinguish between answers that are incorrect due to a human-like bias or are incorrect in a different way. While they find that GPT-3 performs as well or even better than human subjects, they also find that small changes to the wording of tasks can dramatically decrease the performance, likely due to GPT-3 having encountered these tasks in training. Hagendorff et al. [16] similarly use the Cognitive Reflection Test (CRT) and semantic illusions on a series of OpenAI's Generative Pre-trained Transformer (GPT) models. They classify the responses as *correct, intuitive* (but incorrect), and *atypical* - as models increase in size, the majority of responses go from being atypical, to intuitive, to overwhelmingly correct for GPT-4, which no longer displays human cognitive errors. Other studies that find the reasoning of LLMs to outperform that of humans includes Chen et al.'s [33] assessment of the economic rationality of GPT, and Webb et al.'s [34] comparison of GPT-3 and human performance on analogical tasks.  \\nAs mentioned, some studies have found that LLMs replicate cognitive biases present in human reasoning, and so in some instances display irrational thinking in the same way that humans do. Itzhak et al\",\n",
    "        \"# How Reliable Are Automatic Evaluation Methods For Instruction-Tuned Llms?\\n## 4.1 Human Evaluation\\n\\n        |\\n| ENSV_SV    |           |           |           |\\n| 35         |           |           |           |\\n| (36)       |           |           |           |\\n| 68         |           |           |           |\\n| (63)       |           |           |           |\\n| 99         |           |           |           |\\n| (85)       |           |           |           |\\n| Avg. diff. |           |           |           |\\n| 1          | .         |         3 |         4 |  \\ncorrectness since it is the most important criterion for instruction tuning and inherently encompasses relatedness. Moreover, the higher degree of alignment between GPT-4 and humans on correctness compared to the other criteria prompts the need for a deeper analysis.\",\n",
    "    ],\n",
    "    \"chunk_ids\": [\n",
    "        \"3e92c39c-6707-4cf9-aca5-c6c84f72309b\",\n",
    "        \"a09c2e9a-dd24-4e36-9b2e-7d4e15e58639\",\n",
    "        \"ed69de92-b214-4de5-ac6a-6c2e204e6416\",\n",
    "        \"a7145295-1155-4d88-baaf-48ec36095ae4\",\n",
    "        \"9122b070-3595-4f52-9367-f26553930c70\",\n",
    "    ],\n",
    "    \"context\": \"# (Ir)Rationality And Cognitive Biases In Large Language Models\\n## Rationality Task Results\\n\\nResults across all tasks are aggregated in Table 3 and Figure 3. The model that displayed the best overall performance was OpenAI's GPT-4, which achieved the highest proportion of answers that were correct and where the results was achieved through correct reasoning (cateogorised as correct and *human-like* in the above categorisation). GPT-4 gave the correct response and correct reasoning in 69.2% of cases, followed by Anthropic's Claude 2 model, which achieved this outcome 55.0% of the time. Conversely, the model with the highest proportion of incorrect responses (both human-like and non-human-like) was Meta's Llama 2 model with 7 billion parameters, which gave incorrect responses in 77.5% of cases. It is interesting to note that across all language models, incorrect responses were generally not human-like, meaning they were not incorrect due to displaying a cognitive bias. Instead, these responses generally displayed illogical reasoning, and even on occasion provided correct reasoning but then gave an incorrect final answer. An example of the latter is illustrated in Figure 4: this example shows Bard's response to the facilitated version of the Wason task, where the correct response is that both Letter 3 and Letter 4 should be turned over. The model correctly reaches this conclusion in the explanation, but both at the start and end of the response only states that Letter 4 needs to be turned over. This type of response, where the reasoning is correct but the final answer is not, was observed across all model families to varying degrees.  \\nThe result that most incorrect responses were not incorrect due to having fallen for a cognitive bias highlight that these models do not fail at these tasks in the same way that humans do. As we have seen, many studies have shown that LLMs simulate human biases and societal norms [24â€“26]. However, when it comes to reasoning, the effect is less clear. The model that displayed the highest proportion of human-like biases in its responses was GPT-3.5, where this only occurred in 21.7% of cases. If we include human-like correct responses for GPT-3.5, this brings the proportion to 50.8% of cases. Again, the model that displayed the most human-like responses (both correct and incorrect) was GPT-4 (73.3%); the lowest was Llama 2 with 13 billion parameters, only giving human-like responses in 8.3% of cases. The comparison between correct and\\n\\n# (Ir)Rationality And Cognitive Biases In Large Language Models\\n## 1 Introduction\\n\\n can be used to test the rationality of a model; this is a complex problem which requires a consensus what is deemed rational and irrational.  \\nMirco Musolesi University College London University of Bologna m.musolesi@ucl.ac.uk In using methods designed to evaluate human reasoning, it is important to acknowledge the performance vs. competence debate [14]. This line of argument encourages *species-fair* comparisons between humans and machines, meaning that we should design tests specific to either humans or machines, as otherwise apparent failures may not reflect underlying capabilities but only superficial differences. Lampinen[15] discusses this problem when it comes to language models in particular, highlighting that different approaches must be taking to evaluate cognitive and foundation models. However, if we take the purpose of LLMs to be to produce human-like language, perhaps the best approach is precisely to evaluate their output with tasks designed to evaluate humans. This is the approach we have taken in this paper - in order to identify whether LLMs reason rationally, or whether they exhibit biases that can be assimilated to those present in human decision-making, the most appropriate approach is therefore to use tasks that were initially designed for humans. Building on this debate and looking at LLMs being evaluated using human tests, Hagendorff et al. [16] have proposed the creation of a new field of research called *machine psychology*, which would treat LLMs as participants in psychological experiments. The approach employed in this paper precisely applies tests from psychology that were originally designed for humans, in this case to evaluate rational and irrational reasoning displayed but such models. Further to this, some have even discussed the potential of using LLMs as participants in cognitive experiments instead of humans [17], although some see this proposal as too optimistic [18], and others warn against excessive anthropomorphism [19]. One argument against the use of such models in cognitive experiments is that LLMs may be effective at approximating average human judgements, but are not good at capturing the variation in human behaviour [20]. One potential avenue to address this issue is current work on language models impersonating different roles [21], in this way capturing some of the variation in human behaviour. Binz and Schulz [22] show that after finetuning LLMs on data from psychological experiments, they can become accurate cognitive models, which they claim begins paving the way for the potential of using these models to study human behaviour. Park et al. [23] combine large language models with computational interactive agents to simulate human behaviour\\n\\n## 1. Introduction\\n\\nIn the recent years, Large Language Models (LLMs) have been among the most impressive achievements in Artificial intelligence (AI) and Machine Learning (ML), raising questions about whether we are close to an Artificial General Intelligence (AGI) system (Bubeck et al., 2023; Fei et al.,\\n2022). Foundation models like ChatGPT (OpenAI, 2023b) have shown remarkable performance across a wide range of novel and complex tasks, including coding, vision, healthcare, law, education, and psychology (Bubeck et al., 2023).  \\nThese models perform close to human experts, without needing additional re-training or fine-tuning, apparently mimicking human abilities and reasoning (Guo et al., 2023; Min et al., 2021; Bubeck et al., 2023). In particular, recent work has demonstrated how LLMs can emulate human reasoning in solving complex problems: LLMs can simulate chain of thoughts, generating a reasoning path to decompose complex problems into multiple easier steps to solve (Kojima et al., 2022; Wei et al., 2022b; Ho et al., 2022). We could conclude that LLMs are able to replicate high-level cognitive and reasoning capabilities of humans (Huang & Chang, 2022; Hagendorff et al., 2022), especially with bigger foundation models like GPT-4 (Wei et al., 2022a). If we assume this is true, a natural question arises: \\\"Can we leverage LLMs to simulate aspects of human behavior that deviate from perfect rationality so as to eventually improve our understanding of diverse human conduct?\\\"\\nModeling human behavior has seen significant interest in various domains, ranging from robotics for human-robot collaboration (Dragan et al., 2015), to finance and economics for modeling human investors and consumers (Liu et al., 2022; Akerlof & Shiller, 2010). In studies of human-robot interactions, it is commonly embraced that humans must not be modeled as optimal agents (Carroll et al., 2019), and an irrational human when correctly modeled, can communicate more information about their objectives than a perfectly rational human can (Chan et al., 2021). For instance, humans exhibit bounded rationality (Simon, 1997), often making satisfactory but not optimal decisions due to their limited knowledge and processing power. They are also known to be myopic in their decision making as they care more about short-term rewards than long-term rewards. While the discount factor in reinforcement learning (RL)\\n\\n# (Ir)Rationality And Cognitive Biases In Large Language Models\\n## 1 Introduction\\n\\n we use tasks from the cognitive psychology literature designed to test human cognitive biases, and apply these to a series of LLMs to evaluate whether they display rational or irrational reasoning. The capabilities of these models are quickly advancing, therefore the aim of this paper is to provide a methodological contribution showing how we can assess and compare LLMs. A number of studies have taken a similar approach, however they do not generally compare across different model types [12, 16, 29â€“35], or those that do are not evaluating rational reasoning [36]. Some find that LLMs outperform humans on reasoning tasks [16, 37], others find that these models replicate human biases [30, 38], and finally some studies have shown that LLMs perform much worse than humans on certain tasks [36]. Binz and Schulz [12] take a similar approach to that presented in this paper, where they treat GPT-3 as a participant in a psychological experiment to assess its decision-making, information search, deliberation and causal reasoning abilities. They assess the responses across two dimensions, looking at whether GPT-3's output is correct and/or human-like; we follow this approach in this paper as it allows us to distinguish between answers that are incorrect due to a human-like bias or are incorrect in a different way. While they find that GPT-3 performs as well or even better than human subjects, they also find that small changes to the wording of tasks can dramatically decrease the performance, likely due to GPT-3 having encountered these tasks in training. Hagendorff et al. [16] similarly use the Cognitive Reflection Test (CRT) and semantic illusions on a series of OpenAI's Generative Pre-trained Transformer (GPT) models. They classify the responses as *correct, intuitive* (but incorrect), and *atypical* - as models increase in size, the majority of responses go from being atypical, to intuitive, to overwhelmingly correct for GPT-4, which no longer displays human cognitive errors. Other studies that find the reasoning of LLMs to outperform that of humans includes Chen et al.'s [33] assessment of the economic rationality of GPT, and Webb et al.'s [34] comparison of GPT-3 and human performance on analogical tasks.  \\nAs mentioned, some studies have found that LLMs replicate cognitive biases present in human reasoning, and so in some instances display irrational thinking in the same way that humans do. Itzhak et al\\n\\n# How Reliable Are Automatic Evaluation Methods For Instruction-Tuned Llms?\\n## 4.1 Human Evaluation\\n\\n        |\\n| ENSV_SV    |           |           |           |\\n| 35         |           |           |           |\\n| (36)       |           |           |           |\\n| 68         |           |           |           |\\n| (63)       |           |           |           |\\n| 99         |           |           |           |\\n| (85)       |           |           |           |\\n| Avg. diff. |           |           |           |\\n| 1          | .         |         3 |         4 |  \\ncorrectness since it is the most important criterion for instruction tuning and inherently encompasses relatedness. Moreover, the higher degree of alignment between GPT-4 and humans on correctness compared to the other criteria prompts the need for a deeper analysis.\",\n",
    "    \"answer\": \"Yes, the Turing Test assesses a machine's ability to exhibit intelligent behavior equivalent to that of a human.\",\n",
    "}\n",
    "\n",
    "llm = LLM(config=pipeline.config)\n",
    "llm(test_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG-12000 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1adc8b3ac3be4d5a8e45438e6e027be8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "BuilderConfig 'corpus' not found. Available: ['default']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m----> 3\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mneural-bridge/rag-dataset-12000\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcorpus\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Codespace/leolavaur.git/llm-text-watermarking/.venv/lib/python3.12/site-packages/datasets/load.py:2129\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2124\u001b[0m verification_mode \u001b[38;5;241m=\u001b[39m VerificationMode(\n\u001b[1;32m   2125\u001b[0m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mBASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS\n\u001b[1;32m   2126\u001b[0m )\n\u001b[1;32m   2128\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 2129\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2142\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2143\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2144\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2146\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   2147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m~/Codespace/leolavaur.git/llm-text-watermarking/.venv/lib/python3.12/site-packages/datasets/load.py:1886\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[1;32m   1884\u001b[0m builder_cls \u001b[38;5;241m=\u001b[39m get_dataset_builder_class(dataset_module, dataset_name\u001b[38;5;241m=\u001b[39mdataset_name)\n\u001b[1;32m   1885\u001b[0m \u001b[38;5;66;03m# Instantiate the dataset builder\u001b[39;00m\n\u001b[0;32m-> 1886\u001b[0m builder_instance: DatasetBuilder \u001b[38;5;241m=\u001b[39m \u001b[43mbuilder_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1892\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mhash\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1893\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1894\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1896\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1897\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbuilder_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1898\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1899\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1900\u001b[0m builder_instance\u001b[38;5;241m.\u001b[39m_use_legacy_cache_dir_if_possible(dataset_module)\n\u001b[1;32m   1902\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance\n",
      "File \u001b[0;32m~/Codespace/leolavaur.git/llm-text-watermarking/.venv/lib/python3.12/site-packages/datasets/builder.py:342\u001b[0m, in \u001b[0;36mDatasetBuilder.__init__\u001b[0;34m(self, cache_dir, dataset_name, config_name, hash, base_path, info, features, token, repo_id, data_files, data_dir, storage_options, writer_batch_size, **config_kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m     config_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m data_dir\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_kwargs \u001b[38;5;241m=\u001b[39m config_kwargs\n\u001b[0;32m--> 342\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_builder_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;66;03m# prepare info: DatasetInfo are a standardized dataclass across all datasets\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;66;03m# Prefill datasetinfo\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;66;03m# TODO FOR PACKAGED MODULES IT IMPORTS DATA FROM src/packaged_modules which doesn't make sense\u001b[39;00m\n",
      "File \u001b[0;32m~/Codespace/leolavaur.git/llm-text-watermarking/.venv/lib/python3.12/site-packages/datasets/builder.py:569\u001b[0m, in \u001b[0;36mDatasetBuilder._create_builder_config\u001b[0;34m(self, config_name, custom_features, **config_kwargs)\u001b[0m\n\u001b[1;32m    567\u001b[0m     builder_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder_configs\u001b[38;5;241m.\u001b[39mget(config_name)\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m builder_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBUILDER_CONFIGS:\n\u001b[0;32m--> 569\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    570\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuilderConfig \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found. Available: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder_configs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    571\u001b[0m         )\n\u001b[1;32m    573\u001b[0m \u001b[38;5;66;03m# if not using an existing config, then create a new config on the fly\u001b[39;00m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m builder_config:\n",
      "\u001b[0;31mValueError\u001b[0m: BuilderConfig 'corpus' not found. Available: ['default']"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"neural-bridge/rag-dataset-12000\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
